{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6SO6vMcfHYi1"
   },
   "outputs": [],
   "source": [
    "#USEFUL LINKS\n",
    "# LINK TO REPLACE WORD IN SENTENCE\n",
    "# https://www.programiz.com/python-programming/methods/string/replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rHl7QgWBpGHo"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PPc-MrchQAKs",
    "outputId": "035fddac-8ddf-4913-a9e4-bd11c3489b42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fxR2Xg2g6QFa"
   },
   "outputs": [],
   "source": [
    "train = pickle.load(open('/content/drive/MyDrive/train.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "M4rI6Rdp6QFb"
   },
   "outputs": [],
   "source": [
    "embedding_matrix = pickle.load(open('/content/drive/MyDrive/embedding_matrix.pkl','rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "QTJ9Phqi2fw3"
   },
   "outputs": [],
   "source": [
    "tknizer_ERRONEOUS_SENTENCE = pickle.load(open('/content/drive/MyDrive/tknizer_ERRONEOUS_SENTENCE.pkl','rb'))\n",
    "tknizer_CORRECT_SENTENCE = pickle.load(open('/content/drive/MyDrive/tknizer_CORRECT_SENTENCE.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hBb0yEYlCynz",
    "outputId": "a1b3b24c-1339-4da9-9023-b14dcbddfe39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34411\n",
      "86321\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocab_size_CORRECT_SENTENCE=len(tknizer_CORRECT_SENTENCE.word_index.keys())\n",
    "print(vocab_size_CORRECT_SENTENCE)\n",
    "vocab_size_ERRONEOUS_SENTENCE=len(tknizer_ERRONEOUS_SENTENCE.word_index.keys())\n",
    "print(vocab_size_ERRONEOUS_SENTENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "7cyHikQG31fc"
   },
   "outputs": [],
   "source": [
    "embedding_matrix = pickle.load(open('/content/drive/MyDrive/embedding_matrix.pkl','rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nx_svoCs5IYk",
    "outputId": "5623879c-302a-4197-94b1-e4432db790cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vanilla_model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_1 (Encoder)          multiple                  9004468   \n",
      "_________________________________________________________________\n",
      "decoder_1 (Decoder)          multiple                  3816468   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  8868556   \n",
      "=================================================================\n",
      "Total params: 21,689,492\n",
      "Trainable params: 21,689,492\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vanilla = tf.keras.models.load_model('/content/drive/MyDrive/save_model/enc_dec')\n",
    "vanilla.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "impressive-advancement"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embedding_dim, input_length, enc_units):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.input_length = input_length\n",
    "        self.enc_units= enc_units\n",
    "        self.lstm_output = 0\n",
    "        self.lstm_state_h=0\n",
    "        self.lstm_state_c=0\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim, input_length=self.input_length,\n",
    "                           mask_zero=True, name=\"embedding_layer_encoder\", input_shape=(self.vocab_size,))\n",
    "        self.lstm = LSTM(self.enc_units, return_state=True, return_sequences=True, name=\"Encoder_LSTM\")\n",
    "        \n",
    "    def call(self, input_sentances, training=True):\n",
    "        input_embedd                        = self.embedding(input_sentances)\n",
    "        self.lstm_output, self.lstm_state_h,self.lstm_state_c = self.lstm(input_embedd)\n",
    "        return self.lstm_output, self.lstm_state_h,self.lstm_state_c\n",
    "    def get_states(self):\n",
    "        return self.lstm_state_h,self.lstm_state_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "effective-world"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embedding_dim, input_length, dec_units):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dec_units = dec_units\n",
    "        self.input_length = input_length\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # we are using embedding_matrix weights and not training the embedding layer\n",
    "        self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim, input_length=self.input_length,\n",
    "                           mask_zero=True, name=\"embedding_layer_decoder\", weights=[embedding_matrix],input_shape=(self.vocab_size,))\n",
    "        self.lstm = LSTM(self.dec_units, return_sequences=True, return_state=True, name=\"Encoder_LSTM\")\n",
    "        \n",
    "    def call(self, target_sentances, state_h, state_c):\n",
    "        target_embedd           = self.embedding(target_sentances)\n",
    "        lstm_output, _,_        = self.lstm(target_embedd, initial_state=[state_h, state_c])\n",
    "        return lstm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "horizontal-links"
   },
   "outputs": [],
   "source": [
    "class vanilla_model(Model):\n",
    "    def __init__(self, encoder_inputs_length,decoder_inputs_length, output_vocab_size):\n",
    "        super().__init__() \n",
    "        self.encoder = Encoder(vocab_size=vocab_size_ERRONEOUS_SENTENCE + 1, embedding_dim=100, input_length=encoder_inputs_length, enc_units=256)\n",
    "        self.decoder = Decoder(vocab_size=vocab_size_CORRECT_SENTENCE + 1, embedding_dim=100, input_length=decoder_inputs_length, dec_units=256)\n",
    "        self.dense   = Dense(output_vocab_size, activation='softmax')\n",
    "        \n",
    "        \n",
    "    def call(self, data):\n",
    "        input,output = data[0], data[1]\n",
    "        encoder_output, encoder_h, encoder_c = self.encoder(input)\n",
    "        decoder_output                       = self.decoder(output, encoder_h, encoder_c)\n",
    "        output                               = self.dense(decoder_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "earlier-africa"
   },
   "outputs": [],
   "source": [
    "vanilla = vanilla_model(encoder_inputs_length=16,decoder_inputs_length=16,output_vocab_size=vocab_size_CORRECT_SENTENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "DZ_QeXvxG1Wu",
    "outputId": "f47a67fc-4d7a-4a52-8a87-cb2901f761a3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'wallace '"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inference(enc_inp,dec_inp):\n",
    "        \n",
    "    translation=\"\"\n",
    "\n",
    "    e_input=[]\n",
    "    for i in enc_inp.split():\n",
    "        if tknizer_ERRONEOUS_SENTENCE.word_index.get(i) == None:\n",
    "            e_input.append(0)\n",
    "        else:\n",
    "            e_input.append(tknizer_ERRONEOUS_SENTENCE.word_index.get(i))\n",
    "\n",
    "    #e_input = pad_sequences(e_input, maxlen=16, padding='post')\n",
    "\n",
    "\n",
    "    e_output, e_hidden, e_cell = vanilla.layers[0](np.array([e_input], dtype='int32'))\n",
    "\n",
    "    #there is no onestep decoder in this thing, so I have to use the decoder input to predict output\n",
    "\n",
    "    #decoder input\n",
    "    d_input=[]\n",
    "    for i in dec_inp.split():\n",
    "        if tknizer_CORRECT_SENTENCE.word_index.get(i) == None:\n",
    "            d_input.append(0)\n",
    "        else:\n",
    "            d_input.append(tknizer_CORRECT_SENTENCE.word_index.get(i))\n",
    "\n",
    "    #d_input = pad_sequences(d_input, maxlen=16, padding='post')\n",
    "\n",
    "    prediction = vanilla.layers[2](vanilla.layers[1](np.array([d_input], dtype='int32'),e_hidden,e_cell))\n",
    "\n",
    "    for word in prediction[0]:\n",
    "        word = tknizer_CORRECT_SENTENCE.index_word[tf.argmax(word).numpy()]\n",
    "        if word == \"<end>\":\n",
    "            break\n",
    "    translation += word + \" \"\n",
    "    \n",
    "    return translation\n",
    "a = 'tihs is bad'\n",
    "b =  '<start> tihs is bad'\n",
    "pred = inference(a,b)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PECCvxpqLz3y",
    "outputId": "2aab49e2-f71b-4f99-d116-b20f4f2eff3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('barnsley ',\n",
       " 'Tihs is bad and we are good',\n",
       " '<start> Tihs is bad and we are good')"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'Tihs is bad and we are good'\n",
    "b = '<start> ' + a\n",
    "pred = inference(a,b)\n",
    "pred , a , b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JdCJHXwV--b-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "GRAMMAR_ERROR_HANDLING___final",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
