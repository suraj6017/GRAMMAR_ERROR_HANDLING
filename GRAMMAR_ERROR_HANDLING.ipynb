{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6SO6vMcfHYi1"
   },
   "outputs": [],
   "source": [
    "#USEFUL LINKS\n",
    "# LINK TO REPLACE WORD IN SENTENCE\n",
    "# https://www.programiz.com/python-programming/methods/string/replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "xtYwh6xLP-rR",
    "outputId": "1629d6c9-33d5-4254-ed27-bf2b0bcd9dd9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all imports\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk import pos_tag \n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Conv2D, Flatten , Input , Conv1D , Concatenate , MaxPooling1D , Dropout , Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM\n",
    "import datetime\n",
    "\n",
    "from keras.layers import Concatenate\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.layers import Embedding\n",
    "from sklearn.metrics import  f1_score , roc_auc_score\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import nltk.translate.bleu_score as bleu\n",
    "\n",
    "\n",
    "tf.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PPc-MrchQAKs",
    "outputId": "28cf8a99-68cb-43b0-bd1a-25e7140b6fc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.activity.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fexperimentsandconfigs%20https%3a%2f%2fwww.googleapis.com%2fauth%2fphotos.native&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "4/1AY0e-g7E9DjrGpKJUOzvisDP1ySA5p5cVr6EoXNlyFAYFAwltUpxCgHvDZE\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LSbFUJW3Rq66"
   },
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "Q75mMFTpQGn2",
    "outputId": "f111270e-9186-4f4e-971e-7f0cbd7424b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(202231, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CORRECT_SENTENCE</th>\n",
       "      <th>ERRONEOUS_SENTENCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well Prince so Genoa and Lucca are now just fa...</td>\n",
       "      <td>Well Prince so Genoa and Lucca are now just fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>But I warn you if you don t tell me</td>\n",
       "      <td>But I warn you if you don t tell me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It was in July 1805 and the speaker was the</td>\n",
       "      <td>It was in July 1805 and the speaker was the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>With these words she greeted Prince Vasili Kur...</td>\n",
       "      <td>With theze wordz zhe greeted Prince Vazili Kur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anna Pavlovna had had a cough for some days</td>\n",
       "      <td>Anna Pavlovna had had a cough oor some days</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    CORRECT_SENTENCE                                 ERRONEOUS_SENTENCE\n",
       "0  Well Prince so Genoa and Lucca are now just fa...  Well Prince so Genoa and Lucca are now just fa...\n",
       "1                But I warn you if you don t tell me                But I warn you if you don t tell me\n",
       "2        It was in July 1805 and the speaker was the        It was in July 1805 and the speaker was the\n",
       "3  With these words she greeted Prince Vasili Kur...  With theze wordz zhe greeted Prince Vazili Kur...\n",
       "4        Anna Pavlovna had had a cough for some days        Anna Pavlovna had had a cough oor some days"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/content/drive/MyDrive/Copy of Copy of input_output_LENNN_10.csv')\n",
    "print(data.shape)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YP5rbIrmQpVd",
    "outputId": "3ca18661-f1ed-4c9a-eeb3-050853ab3720"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pandas.core.series.Series, 0)"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data.CORRECT_SENTENCE) , data.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SBzZDw87QmL1",
    "outputId": "0e50bd4e-34dd-48e1-a87f-55e3c947b47d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 202231/202231 [00:00<00:00, 2053116.26it/s]\n",
      "100%|██████████| 202231/202231 [00:00<00:00, 2314431.44it/s]\n",
      "100%|██████████| 202231/202231 [00:00<00:00, 2284972.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(94, 1, 44.3706503948455)"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_max = max([len(i) for i in tqdm(data['CORRECT_SENTENCE'])])\n",
    "length_min = min([len(i) for i in tqdm(data['CORRECT_SENTENCE'])])\n",
    "avg = [len(i) for i in tqdm(data['CORRECT_SENTENCE'])]\n",
    "length_avg = np.array([avg]).mean()\n",
    "\n",
    "length_max , length_min , length_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "255HrENRQvfZ"
   },
   "outputs": [],
   "source": [
    "CORRECT_SENTENCE_LEN = data['CORRECT_SENTENCE'].str.split().apply(len) \n",
    "ERRONEOUS_SENTENCE_LEN = data['ERRONEOUS_SENTENCE'].str.split().apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M_b7WjeERiSJ",
    "outputId": "18f81daf-5d10-40bb-e460-ff9a8c49a37f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0\n",
      "10 4.0\n",
      "20 7.0\n",
      "30 10.0\n",
      "40 10.0\n",
      "50 10.0\n",
      "60 10.0\n",
      "70 10.0\n",
      "80 10.0\n",
      "90 10.0\n",
      "100 10.0\n",
      "90 10.0\n",
      "91 10.0\n",
      "92 10.0\n",
      "93 10.0\n",
      "94 10.0\n",
      "95 10.0\n",
      "96 10.0\n",
      "97 10.0\n",
      "98 10.0\n",
      "99 10.0\n",
      "100 10.0\n",
      "99.1 10.0\n",
      "99.2 10.0\n",
      "99.3 10.0\n",
      "99.4 10.0\n",
      "99.5 10.0\n",
      "99.6 10.0\n",
      "99.7 10.0\n",
      "99.8 10.0\n",
      "99.9 10.0\n",
      "100 10.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,101,10):\n",
    "    print(i,np.percentile(ERRONEOUS_SENTENCE_LEN, i))\n",
    "for i in range(90,101):\n",
    "    print(i,np.percentile(ERRONEOUS_SENTENCE_LEN, i))\n",
    "for i in [99.1,99.2,99.3,99.4,99.5,99.6,99.7,99.8,99.9,100]:\n",
    "    print(i,np.percentile(ERRONEOUS_SENTENCE_LEN, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6E6rKnexSM6o"
   },
   "source": [
    "SINCE 99.9% OF DATA HAS LENGTH LESS THAN 10 , SO SELECTING SENTENCE WITH WORD <10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "ZFwGWKh6SMiy",
    "outputId": "34561849-c4d6-495a-c2e6-755b5cb9be39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(202231, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ERRONEOUS_SENTENCE</th>\n",
       "      <th>english_inp</th>\n",
       "      <th>english_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well Prince so Genoa and Lucca are now just fa...</td>\n",
       "      <td>&lt;start&gt; Well Prince so Genoa and Lucca are now...</td>\n",
       "      <td>Well Prince so Genoa and Lucca are now just fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>But I warn you if you don t tell me</td>\n",
       "      <td>&lt;start&gt; But I warn you if you don t tell me</td>\n",
       "      <td>But I warn you if you don t tell me &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It was in July 1805 and the speaker was the</td>\n",
       "      <td>&lt;start&gt; It was in July 1805 and the speaker wa...</td>\n",
       "      <td>It was in July 1805 and the speaker was the &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>With theze wordz zhe greeted Prince Vazili Kur...</td>\n",
       "      <td>&lt;start&gt; With these words she greeted Prince Va...</td>\n",
       "      <td>With these words she greeted Prince Vasili Kur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anna Pavlovna had had a cough oor some days</td>\n",
       "      <td>&lt;start&gt; Anna Pavlovna had had a cough for some...</td>\n",
       "      <td>Anna Pavlovna had had a cough for some days &lt;end&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  ERRONEOUS_SENTENCE  ...                                        english_out\n",
       "0  Well Prince so Genoa and Lucca are now just fa...  ...  Well Prince so Genoa and Lucca are now just fa...\n",
       "1                But I warn you if you don t tell me  ...          But I warn you if you don t tell me <end>\n",
       "2        It was in July 1805 and the speaker was the  ...  It was in July 1805 and the speaker was the <end>\n",
       "3  With theze wordz zhe greeted Prince Vazili Kur...  ...  With these words she greeted Prince Vasili Kur...\n",
       "4        Anna Pavlovna had had a cough oor some days  ...  Anna Pavlovna had had a cough for some days <end>\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['CORRECT_SENTENCE_LEN'] = data['CORRECT_SENTENCE'].str.split().apply(len)\n",
    "data = data[data['CORRECT_SENTENCE_LEN'] < 20]\n",
    "\n",
    "data['ERRONEOUS_SENTENCE_LEN'] = data['ERRONEOUS_SENTENCE'].str.split().apply(len)\n",
    "data = data[data['ERRONEOUS_SENTENCE_LEN'] < 20]\n",
    "\n",
    "#ADDING start and end IN THE SENTENCES\n",
    "data['english_inp'] = '<start> ' + data['CORRECT_SENTENCE'].astype(str)\n",
    "data['english_out'] = data['CORRECT_SENTENCE'].astype(str) + ' <end>'\n",
    "\n",
    "data = data.drop(['CORRECT_SENTENCE','CORRECT_SENTENCE_LEN','ERRONEOUS_SENTENCE_LEN'], axis=1)\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xen7OFp6T5qr",
    "outputId": "c1acb709-036d-4db7-9ffe-383e3ef65a49"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('With theze wordz zhe greeted Prince Vazili Kuragin a man',\n",
       " '<start> With these words she greeted Prince Vasili Kuragin a man',\n",
       " 'With these words she greeted Prince Vasili Kuragin a man <end>')"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.ERRONEOUS_SENTENCE[3] , data.english_inp[3] , data.english_out[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JM8ciYtJWK6f"
   },
   "source": [
    "### Getting train and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "TfrDw7tzT5Qx"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, validation = train_test_split(data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ySroEsmXWOFW",
    "outputId": "c6f8e3e7-8cbe-45be-a3b5-28a5a81f8e16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(161784, 3) (40447, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, validation.shape)\n",
    "#ADDING TO <end> TO ONE OF THE SENTENCES SO THAT TOKENIZER LEARNS THE WORD <end>\n",
    "train.iloc[0]['english_inp']= str(train.iloc[0]['english_inp'])+' <end>'\n",
    "train.iloc[0]['english_out']= str(train.iloc[0]['english_out'])+' <end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b05db-RnH_NV",
    "outputId": "1d70e73a-efc2-47e5-d813-f4fef037120b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161784/161784 [00:00<00:00, 1695508.38it/s]\n",
      "100%|██████████| 161784/161784 [00:00<00:00, 1617475.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_encoder_length = max([len(i) for i in tqdm(train['ERRONEOUS_SENTENCE'])])\n",
    "max_decoder_length = max([len(i) for i in tqdm(train['english_inp'])])\n",
    "print(max_encoder_length , max_decoder_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "id": "Su69ZPzTsxmn",
    "outputId": "89a8ff4e-3552-4ed8-b554-3174901bb441"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ERRONEOUS_SENTENCE</th>\n",
       "      <th>english_inp</th>\n",
       "      <th>english_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48653</th>\n",
       "      <td>The Bree hobbits were in fact friendly and inq...</td>\n",
       "      <td>&lt;start&gt; The Bree hobbits were in fact friendly...</td>\n",
       "      <td>The Bree hobbits were in fact friendly and inq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96071</th>\n",
       "      <td>He ought to have known</td>\n",
       "      <td>&lt;start&gt; He ought to have known</td>\n",
       "      <td>He ought to have known &lt;end&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      ERRONEOUS_SENTENCE  ...                                        english_out\n",
       "48653  The Bree hobbits were in fact friendly and inq...  ...  The Bree hobbits were in fact friendly and inq...\n",
       "96071                             He ought to have known  ...                       He ought to have known <end>\n",
       "\n",
       "[2 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "id": "CGqQDR8FWV3D",
    "outputId": "12c6849b-6039-4058-b433-ea4083384e47"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ERRONEOUS_SENTENCE</th>\n",
       "      <th>english_inp</th>\n",
       "      <th>english_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145414</th>\n",
       "      <td>Proud Mrs</td>\n",
       "      <td>&lt;start&gt; Proud Mrs</td>\n",
       "      <td>Proud Mrs &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4256</th>\n",
       "      <td>From privates to general they were not expecti...</td>\n",
       "      <td>&lt;start&gt; From privates to general they were not...</td>\n",
       "      <td>From privates to general they were not expecti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       ERRONEOUS_SENTENCE  ...                                        english_out\n",
       "145414                                          Proud Mrs  ...                                    Proud Mrs <end>\n",
       "4256    From privates to general they were not expecti...  ...  From privates to general they were not expecti...\n",
       "\n",
       "[2 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnjtTfVOJLnA"
   },
   "source": [
    "**TOKENIING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "GPa0ldDWkav8"
   },
   "outputs": [],
   "source": [
    "tknizer_ERRONEOUS_SENTENCE = Tokenizer()\n",
    "tknizer_ERRONEOUS_SENTENCE.fit_on_texts(train['ERRONEOUS_SENTENCE'].values)\n",
    "tknizer_CORRECT_SENTENCE = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
    "tknizer_CORRECT_SENTENCE.fit_on_texts(train['english_inp'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hBb0yEYlCynz",
    "outputId": "dbc917e3-648b-4bb3-8447-5059152efa96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34512\n",
      "86363\n"
     ]
    }
   ],
   "source": [
    "vocab_size_CORRECT_SENTENCE=len(tknizer_CORRECT_SENTENCE.word_index.keys())\n",
    "print(vocab_size_CORRECT_SENTENCE)\n",
    "vocab_size_ERRONEOUS_SENTENCE=len(tknizer_ERRONEOUS_SENTENCE.word_index.keys())\n",
    "print(vocab_size_ERRONEOUS_SENTENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nm1uoAC-Nwuk",
    "outputId": "9b4247b5-8c76-4c5f-e2f6-9035d192ef5b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 21920)"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tknizer_CORRECT_SENTENCE.word_index['<start>'], tknizer_CORRECT_SENTENCE.word_index['<end>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jnVtXGEJglN"
   },
   "source": [
    "TOKENIZER WITH ENGLISH WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IKInpM9fI7TA",
    "outputId": "578ae5f4-a2fc-4f01-a784-624a8473162b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34513, 100)"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index = dict()\n",
    "f = open('/content/drive/MyDrive/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size_CORRECT_SENTENCE+1, 100))\n",
    "for word, i in tknizer_CORRECT_SENTENCE.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "48GmIsiBuV-N"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "impressive-advancement"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embedding_dim, input_length, enc_units):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.input_length = input_length\n",
    "        self.enc_units= enc_units\n",
    "        self.lstm_output = 0\n",
    "        self.lstm_state_h=0\n",
    "        self.lstm_state_c=0\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim, input_length=self.input_length,\n",
    "                           mask_zero=True, name=\"embedding_layer_encoder\", input_shape=(self.vocab_size,))\n",
    "        self.lstm = LSTM(self.enc_units, return_state=True, return_sequences=True, name=\"Encoder_LSTM\")\n",
    "        \n",
    "    def call(self, input_sentances, training=True):\n",
    "        input_embedd                        = self.embedding(input_sentances)\n",
    "        self.lstm_output, self.lstm_state_h,self.lstm_state_c = self.lstm(input_embedd)\n",
    "        return self.lstm_output, self.lstm_state_h,self.lstm_state_c\n",
    "    def get_states(self):\n",
    "        return self.lstm_state_h,self.lstm_state_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "effective-world"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embedding_dim, input_length, dec_units):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dec_units = dec_units\n",
    "        self.input_length = input_length\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # we are using embedding_matrix weights and not training the embedding layer\n",
    "        self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim, input_length=self.input_length,\n",
    "                           mask_zero=True, name=\"embedding_layer_decoder\", weights=[embedding_matrix],input_shape=(self.vocab_size,))\n",
    "        self.lstm = LSTM(self.dec_units, return_sequences=True, return_state=True, name=\"Encoder_LSTM\")\n",
    "        \n",
    "    def call(self, target_sentances, state_h, state_c):\n",
    "        target_embedd           = self.embedding(target_sentances)\n",
    "        lstm_output, _,_        = self.lstm(target_embedd, initial_state=[state_h, state_c])\n",
    "        return lstm_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cathedral-graham"
   },
   "source": [
    "## Data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "macro-senator"
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, data, tknizer_ERRONEOUS_SENTENCE, tknizer_CORRECT_SENTENCE, max_len):\n",
    "        self.encoder_inps = data['ERRONEOUS_SENTENCE'].values\n",
    "        self.decoder_inps = data['english_inp'].values\n",
    "        self.decoder_outs = data['english_out'].values\n",
    "        self.tknizer_CORRECT_SENTENCE = tknizer_CORRECT_SENTENCE\n",
    "        self.tknizer_ERRONEOUS_SENTENCE = tknizer_ERRONEOUS_SENTENCE\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        self.encoder_seq = self.tknizer_ERRONEOUS_SENTENCE.texts_to_sequences([self.encoder_inps[i]]) # need to pass list of values\n",
    "        self.decoder_inp_seq = self.tknizer_CORRECT_SENTENCE.texts_to_sequences([self.decoder_inps[i]])\n",
    "        self.decoder_out_seq = self.tknizer_CORRECT_SENTENCE.texts_to_sequences([self.decoder_outs[i]])\n",
    "\n",
    "        self.encoder_seq = pad_sequences(self.encoder_seq, maxlen=self.max_len, dtype='int32', padding='post')\n",
    "        self.decoder_inp_seq = pad_sequences(self.decoder_inp_seq, maxlen=self.max_len, dtype='int32', padding='post')\n",
    "        self.decoder_out_seq = pad_sequences(self.decoder_out_seq, maxlen=self.max_len, dtype='int32', padding='post')\n",
    "        return self.encoder_seq, self.decoder_inp_seq, self.decoder_out_seq\n",
    "\n",
    "    def __len__(self): # your model.fit_gen requires this function\n",
    "        return len(self.encoder_inps)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "featured-offense"
   },
   "outputs": [],
   "source": [
    "class Dataloder(tf.keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, dataset, batch_size=1):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.indexes = np.arange(len(self.dataset.encoder_inps))\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        start = i * self.batch_size\n",
    "        stop = (i + 1) * self.batch_size\n",
    "        data = []\n",
    "        for j in range(start, stop):\n",
    "            data.append(self.dataset[j])\n",
    "\n",
    "        batch = [np.squeeze(np.stack(samples, axis=1), axis=0) for samples in zip(*data)]\n",
    "        \n",
    "        # we are creating data like ([italian, english_inp], english_out) these are already converted into seq\n",
    "        \n",
    "        return [batch[0],batch[1]],batch[2]\n",
    "\n",
    "    def __len__(self):  # your model.fit_gen requires this function\n",
    "        return len(self.indexes) // self.batch_size\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.random.permutation(self.indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "medium-letter"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "horizontal-links"
   },
   "outputs": [],
   "source": [
    "class vanilla_model(Model):\n",
    "    def __init__(self, encoder_inputs_length,decoder_inputs_length, output_vocab_size):\n",
    "        super().__init__() \n",
    "        self.encoder = Encoder(vocab_size=vocab_size_ERRONEOUS_SENTENCE + 1, embedding_dim=100, input_length=encoder_inputs_length, enc_units=256)\n",
    "        self.decoder = Decoder(vocab_size=vocab_size_CORRECT_SENTENCE + 1, embedding_dim=100, input_length=decoder_inputs_length, dec_units=256)\n",
    "        self.dense   = Dense(output_vocab_size, activation='softmax')\n",
    "        \n",
    "        \n",
    "    def call(self, data):\n",
    "        input,output = data[0], data[1]\n",
    "        encoder_output, encoder_h, encoder_c = self.encoder(input)\n",
    "        decoder_output                       = self.decoder(output, encoder_h, encoder_c)\n",
    "        output                               = self.dense(decoder_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "neither-tennis",
    "outputId": "9fc3898e-752e-4d4e-f077-101cea1d380d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 20) (512, 20) (512, 20)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset(train, tknizer_ERRONEOUS_SENTENCE, tknizer_CORRECT_SENTENCE, 16)\n",
    "test_dataset  = Dataset(validation, tknizer_ERRONEOUS_SENTENCE, tknizer_CORRECT_SENTENCE, 16)\n",
    "\n",
    "train_dataset = Dataset(train, tknizer_ERRONEOUS_SENTENCE, tknizer_CORRECT_SENTENCE, 20)\n",
    "test_dataset  = Dataset(validation, tknizer_ERRONEOUS_SENTENCE, tknizer_CORRECT_SENTENCE, 20)\n",
    "\n",
    "train_dataloader = Dataloder(train_dataset, batch_size=512)\n",
    "test_dataloader = Dataloder(test_dataset, batch_size=512)\n",
    "\n",
    "print(train_dataloader[0][0][0].shape, train_dataloader[0][0][1].shape, train_dataloader[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "postal-portrait"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "earlier-africa"
   },
   "outputs": [],
   "source": [
    "vanilla = vanilla_model(encoder_inputs_length=16,decoder_inputs_length=16,output_vocab_size=vocab_size_CORRECT_SENTENCE)\n",
    "optimizer = tf.keras.optimizers.Adam(clipnorm=1.0)\n",
    "vanilla.compile(optimizer= optimizer, loss= loss_function, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fewer-career"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# logfile = \"/content/Project_2/logs/vanilla/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# #logfile = \"C:\\\\Users\\\\rpris\\\\Google Drive\\\\Project_2\\\\logs\\\\vanilla\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# tfboard = tf.keras.callbacks.TensorBoard(log_dir=logfile, histogram_freq=1, write_graph=True)\n",
    "\n",
    "# chkfile = \"/content/wts/vanilla/weights-{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
    "# #chkfile2 = \"C:\\\\Users\\\\rpris\\\\Google Drive\\\\Project_2\\\\wts\\\\vanilla\\\\weights-{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
    "# chkpt = tf.keras.callbacks.ModelCheckpoint(chkfile, monitor='val_loss', save_best_only=True, save_weights_only=True, verbose=0, mode='min')\n",
    "\n",
    "# stp = tf.keras.callbacks.EarlyStopping(patience=7, monitor='val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sUqzPMbcjsc4"
   },
   "outputs": [],
   "source": [
    "# vanilla.load_weights(\"/content/drive/MyDrive/Project_2/wts/vanilla/weights-65-0.5898.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8LNgdV8GlsDy"
   },
   "outputs": [],
   "source": [
    "# vanilla.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rjcMmTHC0S9o",
    "outputId": "1d91a3f9-1524-49ae-d88d-24e27632465a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "157/157 [==============================] - 65s 377ms/step - loss: 3.6994 - val_loss: 3.0375\n",
      "Epoch 2/150\n",
      "157/157 [==============================] - 58s 372ms/step - loss: 3.0206 - val_loss: 2.8943\n",
      "Epoch 3/150\n",
      "157/157 [==============================] - 60s 383ms/step - loss: 2.8586 - val_loss: 2.7258\n",
      "Epoch 4/150\n",
      "157/157 [==============================] - 60s 381ms/step - loss: 2.7114 - val_loss: 2.6221\n",
      "Epoch 5/150\n",
      "157/157 [==============================] - 60s 384ms/step - loss: 2.6069 - val_loss: 2.5303\n",
      "Epoch 6/150\n",
      "157/157 [==============================] - 60s 381ms/step - loss: 2.5131 - val_loss: 2.4368\n",
      "Epoch 7/150\n",
      "157/157 [==============================] - 60s 382ms/step - loss: 2.4173 - val_loss: 2.3289\n",
      "Epoch 8/150\n",
      "157/157 [==============================] - 60s 382ms/step - loss: 2.3108 - val_loss: 2.2299\n",
      "Epoch 9/150\n",
      "157/157 [==============================] - 60s 382ms/step - loss: 2.2240 - val_loss: 2.1273\n",
      "Epoch 10/150\n",
      "157/157 [==============================] - 60s 382ms/step - loss: 2.1086 - val_loss: 2.0327\n",
      "Epoch 11/150\n",
      "157/157 [==============================] - 60s 381ms/step - loss: 2.0192 - val_loss: 1.9337\n",
      "Epoch 12/150\n",
      "157/157 [==============================] - 61s 386ms/step - loss: 1.9217 - val_loss: 1.8334\n",
      "Epoch 13/150\n",
      "157/157 [==============================] - 60s 384ms/step - loss: 1.8254 - val_loss: 1.7493\n",
      "Epoch 14/150\n",
      "157/157 [==============================] - 60s 382ms/step - loss: 1.7420 - val_loss: 1.6907\n",
      "Epoch 15/150\n",
      "157/157 [==============================] - 60s 382ms/step - loss: 1.6695 - val_loss: 1.6019\n",
      "Epoch 16/150\n",
      "157/157 [==============================] - 60s 382ms/step - loss: 1.5801 - val_loss: 1.5361\n",
      "Epoch 17/150\n",
      "157/157 [==============================] - 60s 383ms/step - loss: 1.5198 - val_loss: 1.4725\n",
      "Epoch 18/150\n",
      "157/157 [==============================] - 60s 382ms/step - loss: 1.4440 - val_loss: 1.4083\n",
      "Epoch 19/150\n",
      "157/157 [==============================] - 60s 382ms/step - loss: 1.3912 - val_loss: 1.3485\n",
      "Epoch 20/150\n",
      "157/157 [==============================] - 60s 382ms/step - loss: 1.3323 - val_loss: 1.2958\n",
      "Epoch 21/150\n",
      "157/157 [==============================] - 60s 382ms/step - loss: 1.2569 - val_loss: 1.2307\n",
      "Epoch 22/150\n",
      "157/157 [==============================] - 60s 382ms/step - loss: 1.2305 - val_loss: 1.1813\n",
      "Epoch 23/150\n",
      "157/157 [==============================] - 60s 383ms/step - loss: 1.1654 - val_loss: 1.1320\n",
      "Epoch 24/150\n",
      "157/157 [==============================] - 60s 382ms/step - loss: 1.1138 - val_loss: 1.0682\n",
      "Epoch 25/150\n",
      "157/157 [==============================] - 60s 381ms/step - loss: 1.0648 - val_loss: 1.0196\n",
      "Epoch 26/150\n",
      "157/157 [==============================] - 60s 382ms/step - loss: 1.0149 - val_loss: 0.9697\n",
      "Epoch 27/150\n",
      "157/157 [==============================] - 60s 380ms/step - loss: 0.9621 - val_loss: 0.9334\n",
      "Epoch 28/150\n",
      "157/157 [==============================] - 60s 379ms/step - loss: 0.9235 - val_loss: 0.8930\n",
      "Epoch 29/150\n",
      "157/157 [==============================] - 60s 381ms/step - loss: 0.8762 - val_loss: 0.8430\n",
      "Epoch 30/150\n",
      "157/157 [==============================] - 60s 384ms/step - loss: 0.8413 - val_loss: 0.8111\n",
      "Epoch 31/150\n",
      "157/157 [==============================] - 60s 384ms/step - loss: 0.8133 - val_loss: 0.7844\n",
      "Epoch 32/150\n",
      "157/157 [==============================] - 60s 384ms/step - loss: 0.7578 - val_loss: 0.7447\n",
      "Epoch 33/150\n",
      "157/157 [==============================] - 60s 385ms/step - loss: 0.7383 - val_loss: 0.6993\n",
      "Epoch 34/150\n",
      "157/157 [==============================] - 60s 385ms/step - loss: 0.7153 - val_loss: 0.6681\n",
      "Epoch 35/150\n",
      "157/157 [==============================] - 60s 384ms/step - loss: 0.6830 - val_loss: 0.6395\n",
      "Epoch 36/150\n",
      "157/157 [==============================] - 60s 384ms/step - loss: 0.6398 - val_loss: 0.6064\n",
      "Epoch 37/150\n",
      "157/157 [==============================] - 60s 384ms/step - loss: 0.6110 - val_loss: 0.5844\n",
      "Epoch 38/150\n",
      "157/157 [==============================] - 60s 383ms/step - loss: 0.5780 - val_loss: 0.5485\n",
      "Epoch 39/150\n",
      "157/157 [==============================] - 60s 384ms/step - loss: 0.5607 - val_loss: 0.5239\n",
      "Epoch 40/150\n",
      "157/157 [==============================] - 60s 384ms/step - loss: 0.5379 - val_loss: 0.4975\n",
      "Epoch 41/150\n",
      "157/157 [==============================] - 60s 384ms/step - loss: 0.5033 - val_loss: 0.4811\n",
      "Epoch 42/150\n",
      "157/157 [==============================] - 60s 384ms/step - loss: 0.4814 - val_loss: 0.4539\n",
      "Epoch 43/150\n",
      "157/157 [==============================] - 60s 384ms/step - loss: 0.4639 - val_loss: 0.4445\n",
      "Epoch 44/150\n",
      "157/157 [==============================] - 60s 384ms/step - loss: 0.4434 - val_loss: 0.4257\n",
      "Epoch 45/150\n",
      "157/157 [==============================] - 60s 384ms/step - loss: 0.4176 - val_loss: 0.3952\n",
      "Epoch 46/150\n",
      "157/157 [==============================] - 60s 384ms/step - loss: 0.3933 - val_loss: 0.3706\n",
      "Epoch 47/150\n",
      "157/157 [==============================] - 60s 383ms/step - loss: 0.3679 - val_loss: 0.3555\n",
      "Epoch 48/150\n",
      "157/157 [==============================] - 60s 385ms/step - loss: 0.3483 - val_loss: 0.3470\n",
      "Epoch 49/150\n",
      "157/157 [==============================] - 60s 385ms/step - loss: 0.3377 - val_loss: 0.3280\n",
      "Epoch 50/150\n",
      "157/157 [==============================] - 60s 385ms/step - loss: 0.3331 - val_loss: 0.3139\n",
      "Epoch 51/150\n",
      "157/157 [==============================] - 60s 385ms/step - loss: 0.3057 - val_loss: 0.3033\n",
      "Epoch 52/150\n",
      "157/157 [==============================] - 60s 385ms/step - loss: 0.2901 - val_loss: 0.2780\n",
      "Epoch 53/150\n",
      "157/157 [==============================] - 60s 384ms/step - loss: 0.2839 - val_loss: 0.2754\n",
      "Epoch 54/150\n",
      "157/157 [==============================] - 60s 385ms/step - loss: 0.2512 - val_loss: 0.2665\n",
      "Epoch 55/150\n",
      "157/157 [==============================] - 61s 386ms/step - loss: 0.2545 - val_loss: 0.2466\n",
      "Epoch 56/150\n",
      "157/157 [==============================] - 60s 384ms/step - loss: 0.2444 - val_loss: 0.2356\n",
      "Epoch 57/150\n",
      "157/157 [==============================] - 60s 384ms/step - loss: 0.2361 - val_loss: 0.2294\n",
      "Epoch 58/150\n",
      "157/157 [==============================] - 60s 384ms/step - loss: 0.2148 - val_loss: 0.2135\n",
      "Epoch 59/150\n",
      "157/157 [==============================] - 60s 384ms/step - loss: 0.1982 - val_loss: 0.2087\n",
      "Epoch 60/150\n",
      "157/157 [==============================] - 60s 384ms/step - loss: 0.1940 - val_loss: 0.1951\n",
      "Epoch 61/150\n",
      "157/157 [==============================] - 60s 384ms/step - loss: 0.1810 - val_loss: 0.1837\n",
      "Epoch 62/150\n",
      "157/157 [==============================] - 60s 383ms/step - loss: 0.1794 - val_loss: 0.1738\n",
      "Epoch 63/150\n",
      "157/157 [==============================] - 60s 384ms/step - loss: 0.1670 - val_loss: 0.1649\n",
      "Epoch 64/150\n",
      "157/157 [==============================] - 60s 384ms/step - loss: 0.1701 - val_loss: 0.1605\n",
      "Epoch 65/150\n",
      "157/157 [==============================] - 60s 384ms/step - loss: 0.1515 - val_loss: 0.1557\n",
      "Epoch 66/150\n",
      "157/157 [==============================] - 60s 385ms/step - loss: 0.1441 - val_loss: 0.1530\n",
      "Epoch 67/150\n",
      "157/157 [==============================] - 60s 383ms/step - loss: 0.1448 - val_loss: 0.1384\n",
      "Epoch 68/150\n",
      "157/157 [==============================] - 60s 385ms/step - loss: 0.1362 - val_loss: 0.1406\n",
      "Epoch 69/150\n",
      "157/157 [==============================] - 60s 384ms/step - loss: 0.1223 - val_loss: 0.1260\n",
      "Epoch 70/150\n",
      "157/157 [==============================] - 60s 385ms/step - loss: 0.1197 - val_loss: 0.1272\n",
      "Epoch 71/150\n",
      "157/157 [==============================] - 60s 384ms/step - loss: 0.1208 - val_loss: 0.1150\n",
      "Epoch 72/150\n",
      "157/157 [==============================] - 60s 383ms/step - loss: 0.1138 - val_loss: 0.1074\n",
      "Epoch 73/150\n",
      "157/157 [==============================] - 60s 384ms/step - loss: 0.1120 - val_loss: 0.1044\n",
      "Epoch 74/150\n",
      "157/157 [==============================] - 60s 384ms/step - loss: 0.1004 - val_loss: 0.0938\n",
      "Epoch 75/150\n",
      "157/157 [==============================] - 60s 385ms/step - loss: 0.1029 - val_loss: 0.0961\n",
      "Epoch 76/150\n",
      "157/157 [==============================] - 60s 385ms/step - loss: 0.1028 - val_loss: 0.0847\n",
      "Epoch 77/150\n",
      "157/157 [==============================] - 60s 384ms/step - loss: 0.0896 - val_loss: 0.0828\n",
      "Epoch 78/150\n",
      "157/157 [==============================] - 60s 385ms/step - loss: 0.0918 - val_loss: 0.0811\n",
      "Epoch 79/150\n",
      "157/157 [==============================] - 60s 384ms/step - loss: 0.0829 - val_loss: 0.0803\n",
      "Epoch 80/150\n",
      "157/157 [==============================] - 60s 384ms/step - loss: 0.0839 - val_loss: 0.0757\n",
      "Epoch 81/150\n",
      "157/157 [==============================] - 60s 384ms/step - loss: 0.0790 - val_loss: 0.0738\n",
      "Epoch 82/150\n",
      "157/157 [==============================] - 60s 385ms/step - loss: 0.0777 - val_loss: 0.0751\n",
      "Epoch 83/150\n",
      "157/157 [==============================] - 61s 386ms/step - loss: 0.0715 - val_loss: 0.0693\n",
      "Epoch 84/150\n",
      "157/157 [==============================] - 60s 384ms/step - loss: 0.0724 - val_loss: 0.0695\n",
      "Epoch 85/150\n",
      "157/157 [==============================] - 61s 386ms/step - loss: 0.0667 - val_loss: 0.0677\n",
      "Epoch 86/150\n",
      "157/157 [==============================] - 61s 386ms/step - loss: 0.0644 - val_loss: 0.0661\n",
      "Epoch 87/150\n",
      "157/157 [==============================] - 60s 385ms/step - loss: 0.0676 - val_loss: 0.0591\n",
      "Epoch 88/150\n",
      "157/157 [==============================] - 61s 386ms/step - loss: 0.0605 - val_loss: 0.0595\n",
      "Epoch 89/150\n",
      "157/157 [==============================] - 61s 387ms/step - loss: 0.0572 - val_loss: 0.0607\n",
      "Epoch 90/150\n",
      "157/157 [==============================] - 61s 388ms/step - loss: 0.0570 - val_loss: 0.0574\n",
      "Epoch 91/150\n",
      "157/157 [==============================] - 61s 386ms/step - loss: 0.0545 - val_loss: 0.0546\n",
      "Epoch 92/150\n",
      "157/157 [==============================] - 61s 387ms/step - loss: 0.0540 - val_loss: 0.0530\n",
      "Epoch 93/150\n",
      "157/157 [==============================] - 61s 387ms/step - loss: 0.0525 - val_loss: 0.0491\n",
      "Epoch 94/150\n",
      "157/157 [==============================] - 61s 386ms/step - loss: 0.0498 - val_loss: 0.0471\n",
      "Epoch 95/150\n",
      "157/157 [==============================] - 61s 386ms/step - loss: 0.0470 - val_loss: 0.0471\n",
      "Epoch 96/150\n",
      "157/157 [==============================] - 61s 386ms/step - loss: 0.0451 - val_loss: 0.0463\n",
      "Epoch 97/150\n",
      "157/157 [==============================] - 61s 385ms/step - loss: 0.0435 - val_loss: 0.0463\n",
      "Epoch 98/150\n",
      "157/157 [==============================] - 61s 386ms/step - loss: 0.0418 - val_loss: 0.0438\n",
      "Epoch 99/150\n",
      "157/157 [==============================] - 61s 386ms/step - loss: 0.0405 - val_loss: 0.0426\n",
      "Epoch 100/150\n",
      "157/157 [==============================] - 61s 386ms/step - loss: 0.0382 - val_loss: 0.0382\n",
      "Epoch 101/150\n",
      "157/157 [==============================] - 61s 385ms/step - loss: 0.0405 - val_loss: 0.0376\n",
      "Epoch 102/150\n",
      "157/157 [==============================] - 61s 387ms/step - loss: 0.0392 - val_loss: 0.0366\n",
      "Epoch 103/150\n",
      "157/157 [==============================] - 61s 387ms/step - loss: 0.0365 - val_loss: 0.0363\n",
      "Epoch 104/150\n",
      "157/157 [==============================] - 61s 387ms/step - loss: 0.0357 - val_loss: 0.0337\n",
      "Epoch 105/150\n",
      "157/157 [==============================] - 61s 387ms/step - loss: 0.0330 - val_loss: 0.0317\n",
      "Epoch 106/150\n",
      "157/157 [==============================] - 61s 386ms/step - loss: 0.0316 - val_loss: 0.0320\n",
      "Epoch 107/150\n",
      "157/157 [==============================] - 61s 386ms/step - loss: 0.0340 - val_loss: 0.0305\n",
      "Epoch 108/150\n",
      "157/157 [==============================] - 60s 385ms/step - loss: 0.0337 - val_loss: 0.0326\n",
      "Epoch 109/150\n",
      "157/157 [==============================] - 61s 387ms/step - loss: 0.0322 - val_loss: 0.0326\n",
      "Epoch 110/150\n",
      "157/157 [==============================] - 61s 386ms/step - loss: 0.0295 - val_loss: 0.0270\n",
      "Epoch 111/150\n",
      "157/157 [==============================] - 61s 386ms/step - loss: 0.0300 - val_loss: 0.0273\n",
      "Epoch 112/150\n",
      "157/157 [==============================] - 61s 386ms/step - loss: 0.0288 - val_loss: 0.0274\n",
      "Epoch 113/150\n",
      "157/157 [==============================] - 60s 385ms/step - loss: 0.0272 - val_loss: 0.0276\n",
      "Epoch 114/150\n",
      "157/157 [==============================] - 61s 386ms/step - loss: 0.0270 - val_loss: 0.0252\n",
      "Epoch 115/150\n",
      "157/157 [==============================] - 60s 384ms/step - loss: 0.0256 - val_loss: 0.0219\n",
      "Epoch 116/150\n",
      "157/157 [==============================] - 60s 384ms/step - loss: 0.0231 - val_loss: 0.0261\n",
      "Epoch 117/150\n",
      "157/157 [==============================] - 60s 383ms/step - loss: 0.0228 - val_loss: 0.0206\n",
      "Epoch 118/150\n",
      "157/157 [==============================] - 60s 384ms/step - loss: 0.0236 - val_loss: 0.0251\n",
      "Epoch 119/150\n",
      "157/157 [==============================] - 60s 385ms/step - loss: 0.0249 - val_loss: 0.0215\n",
      "Epoch 120/150\n",
      "157/157 [==============================] - 61s 386ms/step - loss: 0.0241 - val_loss: 0.0195\n",
      "Epoch 121/150\n",
      "157/157 [==============================] - 61s 386ms/step - loss: 0.0211 - val_loss: 0.0206\n",
      "Epoch 122/150\n",
      "157/157 [==============================] - 61s 385ms/step - loss: 0.0217 - val_loss: 0.0198\n",
      "Epoch 123/150\n",
      "157/157 [==============================] - 61s 386ms/step - loss: 0.0212 - val_loss: 0.0201\n",
      "Epoch 124/150\n",
      "157/157 [==============================] - 61s 386ms/step - loss: 0.0196 - val_loss: 0.0180\n",
      "Epoch 125/150\n",
      "157/157 [==============================] - 61s 386ms/step - loss: 0.0184 - val_loss: 0.0171\n",
      "Epoch 126/150\n",
      "157/157 [==============================] - 61s 385ms/step - loss: 0.0185 - val_loss: 0.0170\n",
      "Epoch 127/150\n",
      "157/157 [==============================] - 60s 384ms/step - loss: 0.0179 - val_loss: 0.0154\n",
      "Epoch 128/150\n",
      "157/157 [==============================] - 60s 384ms/step - loss: 0.0183 - val_loss: 0.0155\n",
      "Epoch 129/150\n",
      "157/157 [==============================] - 60s 385ms/step - loss: 0.0184 - val_loss: 0.0146\n",
      "Epoch 130/150\n",
      "157/157 [==============================] - 61s 385ms/step - loss: 0.0166 - val_loss: 0.0154\n",
      "Epoch 131/150\n",
      "157/157 [==============================] - 60s 385ms/step - loss: 0.0172 - val_loss: 0.0154\n",
      "Epoch 132/150\n",
      "157/157 [==============================] - 61s 387ms/step - loss: 0.0165 - val_loss: 0.0143\n",
      "Epoch 133/150\n",
      "157/157 [==============================] - 61s 385ms/step - loss: 0.0162 - val_loss: 0.0154\n",
      "Epoch 134/150\n",
      "157/157 [==============================] - 61s 386ms/step - loss: 0.0156 - val_loss: 0.0143\n",
      "Epoch 135/150\n",
      "157/157 [==============================] - 61s 385ms/step - loss: 0.0158 - val_loss: 0.0142\n",
      "Epoch 136/150\n",
      "157/157 [==============================] - 61s 387ms/step - loss: 0.0168 - val_loss: 0.0146\n",
      "Epoch 137/150\n",
      "157/157 [==============================] - 61s 386ms/step - loss: 0.0150 - val_loss: 0.0132\n",
      "Epoch 138/150\n",
      "157/157 [==============================] - 61s 387ms/step - loss: 0.0150 - val_loss: 0.0125\n",
      "Epoch 139/150\n",
      "157/157 [==============================] - 61s 387ms/step - loss: 0.0155 - val_loss: 0.0145\n",
      "Epoch 140/150\n",
      "157/157 [==============================] - 61s 386ms/step - loss: 0.0160 - val_loss: 0.0124\n",
      "Epoch 141/150\n",
      "157/157 [==============================] - 60s 385ms/step - loss: 0.0140 - val_loss: 0.0111\n",
      "Epoch 142/150\n",
      "157/157 [==============================] - 61s 385ms/step - loss: 0.0121 - val_loss: 0.0116\n",
      "Epoch 143/150\n",
      "157/157 [==============================] - 61s 386ms/step - loss: 0.0125 - val_loss: 0.0127\n",
      "Epoch 144/150\n",
      "157/157 [==============================] - 61s 386ms/step - loss: 0.0124 - val_loss: 0.0127\n",
      "Epoch 145/150\n",
      "157/157 [==============================] - 61s 387ms/step - loss: 0.0134 - val_loss: 0.0121\n",
      "Epoch 146/150\n",
      "157/157 [==============================] - 61s 386ms/step - loss: 0.0130 - val_loss: 0.0106\n",
      "Epoch 147/150\n",
      "157/157 [==============================] - 60s 385ms/step - loss: 0.0118 - val_loss: 0.0121\n",
      "Epoch 148/150\n",
      "157/157 [==============================] - 60s 385ms/step - loss: 0.0126 - val_loss: 0.0108\n",
      "Epoch 149/150\n",
      "157/157 [==============================] - 61s 386ms/step - loss: 0.0112 - val_loss: 0.0095\n",
      "Epoch 150/150\n",
      "157/157 [==============================] - 61s 386ms/step - loss: 0.0099 - val_loss: 0.0091\n",
      "Model: \"vanilla_model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_1 (Encoder)          multiple                  8990268   \n",
      "_________________________________________________________________\n",
      "decoder_1 (Decoder)          multiple                  3822068   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  8882948   \n",
      "=================================================================\n",
      "Total params: 21,695,284\n",
      "Trainable params: 21,695,284\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vanilla  = vanilla_model(encoder_inputs_length=20,decoder_inputs_length=20,output_vocab_size=vocab_size_CORRECT_SENTENCE)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "vanilla.compile(optimizer=optimizer,loss='sparse_categorical_crossentropy')\n",
    "train_steps=train.shape[0]//1024\n",
    "valid_steps=validation.shape[0]//1024\n",
    "#TRANING THE MODEL FOR 50 EPOCHS CAUSE , MORE TRAINING GIVES MORE RESULTS\n",
    "vanilla.fit_generator(train_dataloader, steps_per_epoch=train_steps, epochs=150 , validation_data=train_dataloader, validation_steps=valid_steps )#, callbacks=[stp, chkpt, tfboard]\n",
    "# model_1.fit_generator(train_dataloader,  epochs=4, validation_data=train_dataloader)\n",
    "vanilla.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W36gv_QP0O1U"
   },
   "outputs": [],
   "source": [
    "os.mkdir('saved_model_newww')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6mDA7LIXvBwh"
   },
   "outputs": [],
   "source": [
    "vanilla.save_weights('saved_model_newww/vanilla.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t6UQOxDPBDeu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "30jMi19gBXv4",
    "outputId": "70a53795-3f9d-45f2-ef66-37020b0f51b3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 92s 738ms/step - loss: 4.9768 - val_loss: 4.1294\n",
      "Model: \"vanilla_model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_4 (Encoder)          multiple                  10858468  \n",
      "_________________________________________________________________\n",
      "decoder_4 (Decoder)          multiple                  3798768   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  8823067   \n",
      "=================================================================\n",
      "Total params: 23,480,303\n",
      "Trainable params: 23,480,303\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vanilla  = vanilla_model(encoder_inputs_length=20,decoder_inputs_length=20,output_vocab_size=vocab_size_CORRECT_SENTENCE)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "vanilla.compile(optimizer=optimizer,loss='sparse_categorical_crossentropy')\n",
    "train_steps=train.shape[0]//1024\n",
    "valid_steps=validation.shape[0]//1024\n",
    "#TRANING THE MODEL FOR 50 EPOCHS CAUSE , MORE TRAINING GIVES MORE RESULTS\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\"best_Model.h5\",save_best_only=True)\n",
    "vanilla.fit_generator(train_dataloader, steps_per_epoch=train_steps, epochs=1 , validation_data=train_dataloader, validation_steps=valid_steps\\\n",
    "                      , callbacks=[model_checkpoint_callback])#, callbacks=[stp, chkpt, tfboard]\n",
    "# model_1.fit_generator(train_dataloader,  epochs=4, validation_data=train_dataloader)\n",
    "vanilla.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IFcQC8r3GD2T"
   },
   "outputs": [],
   "source": [
    "vanilla.load_weights(\"//content/drive/MyDrive/saved_model_newww/vanilla.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A3O-hpedF3iN",
    "outputId": "e6cd3a37-72f5-4630-daea-70dce957220a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vanilla_model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_1 (Encoder)          multiple                  8990268   \n",
      "_________________________________________________________________\n",
      "decoder_1 (Decoder)          multiple                  3822068   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  8882948   \n",
      "=================================================================\n",
      "Total params: 21,695,284\n",
      "Trainable params: 21,695,284\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vanilla.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SFjlz-AG1t4"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "DZ_QeXvxG1Wu",
    "outputId": "ab266d77-0a06-4dae-8760-25ddc71223d0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<end> '"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inference(enc_inp,dec_inp):\n",
    "        \n",
    "    translation=\"\"\n",
    "\n",
    "    e_input=[]\n",
    "    for i in enc_inp.split():\n",
    "        if tknizer_ERRONEOUS_SENTENCE.word_index.get(i) == None:\n",
    "            e_input.append(0)\n",
    "        else:\n",
    "            e_input.append(tknizer_ERRONEOUS_SENTENCE.word_index.get(i))\n",
    "\n",
    "    #e_input = pad_sequences(e_input, maxlen=16, padding='post')\n",
    "\n",
    "\n",
    "    e_output, e_hidden, e_cell = vanilla.layers[0](np.array([e_input], dtype='int32'))\n",
    "\n",
    "    #there is no onestep decoder in this thing, so I have to use the decoder input to predict output\n",
    "\n",
    "    #decoder input\n",
    "    d_input=[]\n",
    "    for i in dec_inp.split():\n",
    "        if tknizer_CORRECT_SENTENCE.word_index.get(i) == None:\n",
    "            d_input.append(0)\n",
    "        else:\n",
    "            d_input.append(tknizer_CORRECT_SENTENCE.word_index.get(i))\n",
    "\n",
    "    #d_input = pad_sequences(d_input, maxlen=16, padding='post')\n",
    "\n",
    "    prediction = vanilla.layers[2](vanilla.layers[1](np.array([d_input], dtype='int32'),e_hidden,e_cell))\n",
    "\n",
    "    for word in prediction[0]:\n",
    "        word = tknizer_CORRECT_SENTENCE.index_word[tf.argmax(word).numpy()]\n",
    "        if word == \"<end>\":\n",
    "            break\n",
    "    translation += word + \" \"\n",
    "    \n",
    "    return translation\n",
    "a = train.ERRONEOUS_SENTENCE[4]\n",
    "b =  train.english_inp[4]\n",
    "pred = inference(a,b)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z3otXDvr4-_F",
    "outputId": "6a0bf7fc-af07-40b7-8982-453359c09b33"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('But I warn you if you don t tell me',\n",
       " '<start> But I warn you if you don t tell me')"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.ERRONEOUS_SENTENCE[1] , train.english_inp[1] , "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PfuQQ1Kk41Ia"
   },
   "outputs": [],
   "source": [
    "enc_inp = train.ERRONEOUS_SENTENCE[1]\n",
    "dec_inp =  train.english_inp[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "HP239Hia4uAE",
    "outputId": "fe41a160-3f05-4154-886d-5929101ef0ba"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'warn warn warn you if you don t tell me '"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation=\"\"\n",
    "\n",
    "e_input=[]\n",
    "for i in enc_inp.split():\n",
    "    if tknizer_ERRONEOUS_SENTENCE.word_index.get(i) == None:\n",
    "        e_input.append(0)\n",
    "    else:\n",
    "        e_input.append(tknizer_ERRONEOUS_SENTENCE.word_index.get(i))\n",
    "\n",
    "#e_input = pad_sequences(e_input, maxlen=16, padding='post')\n",
    "\n",
    "\n",
    "e_output, e_hidden, e_cell = vanilla.layers[0](np.array([e_input], dtype='int32'))\n",
    "\n",
    "#there is no onestep decoder in this thing, so I have to use the decoder input to predict output\n",
    "\n",
    "#decoder input\n",
    "d_input=[]\n",
    "for i in dec_inp.split():\n",
    "    if tknizer_CORRECT_SENTENCE.word_index.get(i) == None:\n",
    "        d_input.append(0)\n",
    "    else:\n",
    "        d_input.append(tknizer_CORRECT_SENTENCE.word_index.get(i))\n",
    "\n",
    "#d_input = pad_sequences(d_input, maxlen=16, padding='post')\n",
    "\n",
    "prediction = vanilla.layers[2](vanilla.layers[1](np.array([d_input], dtype='int32'),e_hidden,e_cell))\n",
    "\n",
    "for word in prediction[0]:\n",
    "    word = tknizer_CORRECT_SENTENCE.index_word[tf.argmax(word).numpy()]\n",
    "    if word == \"<end>\":\n",
    "        break\n",
    "    translation += word + \" \"\n",
    "translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s8o6HWhTFkw1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "nBF1J4nK4f_4",
    "outputId": "afb02441-dd33-4bc2-e84a-82433c0bbe4d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ERRONEOUS_SENTENCE</th>\n",
       "      <th>english_inp</th>\n",
       "      <th>english_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14352</th>\n",
       "      <td>Why do you say that replied Princess Mary</td>\n",
       "      <td>&lt;start&gt; Why do you say that replied Princess Mary</td>\n",
       "      <td>Why do you say that replied Princess Mary &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66179</th>\n",
       "      <td>The basilisk kills people by looking at them</td>\n",
       "      <td>&lt;start&gt; The basilisk kills people by looking a...</td>\n",
       "      <td>The basilisk kills people by looking at them &lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76953</th>\n",
       "      <td>Several people nearby screamed</td>\n",
       "      <td>&lt;start&gt; Several people nearby screamed</td>\n",
       "      <td>Several people nearby screamed &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192440</th>\n",
       "      <td>Madigan J</td>\n",
       "      <td>&lt;start&gt; Madigan J</td>\n",
       "      <td>Madigan J &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161989</th>\n",
       "      <td>He wss rhsr rrricer whrm rhe Wsr Minisrry hsd ...</td>\n",
       "      <td>&lt;start&gt; He was that officer whom the War Minis...</td>\n",
       "      <td>He was that officer whom the War Ministry had ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38546</th>\n",
       "      <td>From someohnng ohao he oold me nn our journey ...</td>\n",
       "      <td>&lt;start&gt; From something that he told me in our ...</td>\n",
       "      <td>From something that he told me in our journey ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189090</th>\n",
       "      <td>He encouraged me Davison</td>\n",
       "      <td>&lt;start&gt; He encouraged me Davison</td>\n",
       "      <td>He encouraged me Davison &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13207</th>\n",
       "      <td>Only for Christ s sake sake</td>\n",
       "      <td>&lt;start&gt; Only for Christ s sake</td>\n",
       "      <td>Only for Christ s sake &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57492</th>\n",
       "      <td>He left without another word</td>\n",
       "      <td>&lt;start&gt; He left without another word</td>\n",
       "      <td>He left without another word &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46651</th>\n",
       "      <td>Sam walked along at Frodo s side as if in</td>\n",
       "      <td>&lt;start&gt; Sam walked along at Frodo s side as if in</td>\n",
       "      <td>Sam walked along at Frodo s side as if in &lt;end&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       ERRONEOUS_SENTENCE  ...                                        english_out\n",
       "14352           Why do you say that replied Princess Mary  ...    Why do you say that replied Princess Mary <end>\n",
       "66179        The basilisk kills people by looking at them  ...  The basilisk kills people by looking at them <...\n",
       "76953                      Several people nearby screamed  ...               Several people nearby screamed <end>\n",
       "192440                                          Madigan J  ...                                    Madigan J <end>\n",
       "161989  He wss rhsr rrricer whrm rhe Wsr Minisrry hsd ...  ...  He was that officer whom the War Ministry had ...\n",
       "...                                                   ...  ...                                                ...\n",
       "38546   From someohnng ohao he oold me nn our journey ...  ...  From something that he told me in our journey ...\n",
       "189090                           He encouraged me Davison  ...                     He encouraged me Davison <end>\n",
       "13207                         Only for Christ s sake sake  ...                       Only for Christ s sake <end>\n",
       "57492                        He left without another word  ...                 He left without another word <end>\n",
       "46651           Sam walked along at Frodo s side as if in  ...    Sam walked along at Frodo s side as if in <end>\n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_train = train.sample(1000)\n",
    "sample_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "T-NAasQ3Q3Jh",
    "outputId": "2b4daa70-2e27-4e83-edc0-f9772737402d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'was was as she said suffering from la grippe grippe '"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inference(enc_inp,dec_inp):\n",
    "            \n",
    "    translation=\"\"\n",
    "\n",
    "    e_input=[]\n",
    "    for i in enc_inp.split():\n",
    "        if tknizer_ERRONEOUS_SENTENCE.word_index.get(i) == None:\n",
    "            e_input.append(0)\n",
    "        else:\n",
    "            e_input.append(tknizer_ERRONEOUS_SENTENCE.word_index.get(i))\n",
    "\n",
    "    #e_input = pad_sequences(e_input, maxlen=16, padding='post')\n",
    "\n",
    "\n",
    "    e_output, e_hidden, e_cell = vanilla.layers[0](np.array([e_input], dtype='int32'))\n",
    "\n",
    "    #there is no onestep decoder in this thing, so I have to use the decoder input to predict output\n",
    "\n",
    "    #decoder input\n",
    "    d_input=[]\n",
    "    for i in dec_inp.split():\n",
    "        if tknizer_CORRECT_SENTENCE.word_index.get(i) == None:\n",
    "            d_input.append(0)\n",
    "        else:\n",
    "            d_input.append(tknizer_CORRECT_SENTENCE.word_index.get(i))\n",
    "\n",
    "    #d_input = pad_sequences(d_input, maxlen=16, padding='post')\n",
    "\n",
    "    prediction = vanilla.layers[2](vanilla.layers[1](np.array([d_input], dtype='int32'),e_hidden,e_cell))\n",
    "\n",
    "    for word in prediction[0]:\n",
    "        word = tknizer_CORRECT_SENTENCE.index_word[tf.argmax(word).numpy()]\n",
    "        if word == \"<end>\":\n",
    "            break\n",
    "        translation += word + \" \"\n",
    "    return translation\n",
    "a = train.ERRONEOUS_SENTENCE[5]\n",
    "b =  train.english_inp[5]\n",
    "pred = inference(a,b)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7noJ9eFPS8DY",
    "outputId": "5c22f69a-fcd6-4fba-ecfe-99e40493209c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('He spoke in that refined French in which our grandfathers',\n",
       " '<start> He spoke in that refined French in which our grandfathers',\n",
       " 'spoke spoke in that refined in in which came regiment ')"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = train.ERRONEOUS_SENTENCE[10]\n",
    "b =  train.english_inp[10]\n",
    "pred = inference(a,b)\n",
    "a  , b , pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wlrh_8nNTDEB",
    "outputId": "cdd5e646-1654-4041-985b-65fa8c802ea8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('First of all dear friend tell me how you are',\n",
       " '<start> First of all dear friend tell me how you are',\n",
       " 'of of all you friend tell me how you are ')"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = train.ERRONEOUS_SENTENCE[12]\n",
    "b =  train.english_inp[12]\n",
    "pred = inference(a,b)\n",
    "a  , b , pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YKWEeGnnTTKV",
    "outputId": "a99046d8-0402-4775-a79b-30ecb4b6f17d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('He spoke with such self confixence that his hearers coulx',\n",
       " '<start> He spoke with such self confidence that his hearers could',\n",
       " 'spoke spoke with such self confidence that the hearers thought ')"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = train.ERRONEOUS_SENTENCE[200]\n",
    "b =  train.english_inp[200]\n",
    "pred = inference(a,b)\n",
    "a  , b , pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j4Lb_lfzq3KP",
    "outputId": "f4a971fd-b082-415f-ed74-47337f1164dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The sovereigns Why y are sending ambassadors to compliment usurper',\n",
       " '<start> The sovereigns Why they are sending ambassadors to compliment the',\n",
       " 'negroes negroes they they are sending to to compliment ease honourable ')"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = train.ERRONEOUS_SENTENCE[300]\n",
    "b =  train.english_inp[300]\n",
    "pred = inference(a,b)\n",
    "a  , b , pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZdvQD-7Dq2-V",
    "outputId": "1784e330-1359-461a-bc1f-138d9bce7384"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('What is it you are afraid of Lise I don',\n",
       " '<start> What is it you are afraid of Lise I don',\n",
       " 'is is it you are afraid of don don don ')"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = train.ERRONEOUS_SENTENCE[500]\n",
    "b =  train.english_inp[500]\n",
    "pred = inference(a,b)\n",
    "a  , b , pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xxQ7BDQsq2wo",
    "outputId": "b324848f-aa9a-47dd-d74d-af817e3155e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Secrets indeed All have secrets ff their fwn answered Natasha',\n",
       " '<start> Secrets indeed All have secrets of their own answered Natasha',\n",
       " 'indeed indeed you you your of their own answered ')"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = train.ERRONEOUS_SENTENCE[1000]\n",
    "b =  train.english_inp[1000]\n",
    "pred = inference(a,b)\n",
    "a  , b , pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "txAQpSyVThEs",
    "outputId": "8d3eca47-19f3-48c2-f01b-44cebc1f9327"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 2/1000 [00:00<01:01, 16.18it/s]\u001b[A\n",
      "  0%|          | 5/1000 [00:00<00:57, 17.34it/s]\u001b[A\n",
      "  1%|          | 7/1000 [00:00<00:57, 17.42it/s]\u001b[A\n",
      "  1%|          | 10/1000 [00:00<00:51, 19.17it/s]\u001b[A\n",
      "  1%|▏         | 13/1000 [00:00<00:47, 20.57it/s]\u001b[A\n",
      "  2%|▏         | 15/1000 [00:00<00:49, 19.90it/s]\u001b[A\n",
      "  2%|▏         | 18/1000 [00:00<00:47, 20.68it/s]\u001b[A\n",
      "  2%|▏         | 20/1000 [00:00<00:50, 19.41it/s]\u001b[A\n",
      "  2%|▏         | 22/1000 [00:01<00:52, 18.50it/s]\u001b[A\n",
      "  2%|▎         | 25/1000 [00:01<00:51, 19.01it/s]\u001b[A\n",
      "  3%|▎         | 27/1000 [00:01<00:53, 18.25it/s]\u001b[A\n",
      "  3%|▎         | 29/1000 [00:01<00:52, 18.35it/s]\u001b[A\n",
      "  3%|▎         | 32/1000 [00:01<00:49, 19.72it/s]\u001b[A\n",
      "  4%|▎         | 35/1000 [00:01<00:50, 19.06it/s]\u001b[A\n",
      "  4%|▍         | 38/1000 [00:01<00:45, 21.33it/s]\u001b[A\n",
      "  4%|▍         | 41/1000 [00:01<00:43, 21.99it/s]\u001b[A\n",
      "  4%|▍         | 44/1000 [00:02<00:47, 20.19it/s]\u001b[A\n",
      "  5%|▍         | 47/1000 [00:02<00:49, 19.37it/s]\u001b[A\n",
      "  5%|▌         | 50/1000 [00:02<00:49, 19.07it/s]\u001b[A\n",
      "  5%|▌         | 53/1000 [00:02<00:48, 19.69it/s]\u001b[A\n",
      "  6%|▌         | 56/1000 [00:02<00:49, 19.26it/s]\u001b[A\n",
      "  6%|▌         | 58/1000 [00:02<00:50, 18.65it/s]\u001b[A\n",
      "  6%|▌         | 61/1000 [00:03<00:48, 19.52it/s]\u001b[A\n",
      "  6%|▋         | 64/1000 [00:03<00:47, 19.69it/s]\u001b[A\n",
      "  7%|▋         | 66/1000 [00:03<00:49, 18.84it/s]\u001b[A\n",
      "  7%|▋         | 69/1000 [00:03<00:44, 20.83it/s]\u001b[A\n",
      "  7%|▋         | 72/1000 [00:03<00:46, 19.99it/s]\u001b[A\n",
      "  8%|▊         | 75/1000 [00:03<00:48, 19.16it/s]\u001b[A\n",
      "  8%|▊         | 77/1000 [00:03<00:51, 18.05it/s]\u001b[A\n",
      "  8%|▊         | 79/1000 [00:04<00:49, 18.54it/s]\u001b[A\n",
      "  8%|▊         | 81/1000 [00:04<00:49, 18.40it/s]\u001b[A\n",
      "  8%|▊         | 83/1000 [00:04<00:51, 17.94it/s]\u001b[A\n",
      "  8%|▊         | 85/1000 [00:04<00:52, 17.57it/s]\u001b[A\n",
      "  9%|▊         | 87/1000 [00:04<00:53, 17.14it/s]\u001b[A\n",
      "  9%|▉         | 90/1000 [00:04<00:50, 17.94it/s]\u001b[A\n",
      "  9%|▉         | 93/1000 [00:04<00:45, 20.11it/s]\u001b[A\n",
      " 10%|▉         | 96/1000 [00:04<00:43, 20.95it/s]\u001b[A\n",
      " 10%|▉         | 99/1000 [00:05<00:44, 20.04it/s]\u001b[A\n",
      " 10%|█         | 102/1000 [00:05<00:43, 20.60it/s]\u001b[A\n",
      " 10%|█         | 105/1000 [00:05<00:42, 21.22it/s]\u001b[A\n",
      " 11%|█         | 108/1000 [00:05<00:45, 19.79it/s]\u001b[A\n",
      " 11%|█         | 111/1000 [00:05<00:45, 19.44it/s]\u001b[A\n",
      " 11%|█▏        | 114/1000 [00:05<00:44, 19.98it/s]\u001b[A\n",
      " 12%|█▏        | 117/1000 [00:05<00:47, 18.59it/s]\u001b[A\n",
      " 12%|█▏        | 120/1000 [00:06<00:45, 19.55it/s]\u001b[A\n",
      " 12%|█▏        | 122/1000 [00:06<00:47, 18.35it/s]\u001b[A\n",
      " 12%|█▎        | 125/1000 [00:06<00:45, 19.41it/s]\u001b[A\n",
      " 13%|█▎        | 127/1000 [00:06<00:47, 18.28it/s]\u001b[A\n",
      " 13%|█▎        | 129/1000 [00:06<00:49, 17.76it/s]\u001b[A\n",
      " 13%|█▎        | 131/1000 [00:06<00:49, 17.47it/s]\u001b[A\n",
      " 13%|█▎        | 133/1000 [00:06<00:51, 16.83it/s]\u001b[A\n",
      " 14%|█▎        | 135/1000 [00:06<00:52, 16.53it/s]\u001b[A\n",
      " 14%|█▎        | 137/1000 [00:07<00:52, 16.30it/s]\u001b[A\n",
      " 14%|█▍        | 139/1000 [00:07<00:53, 16.07it/s]\u001b[A\n",
      " 14%|█▍        | 141/1000 [00:07<00:53, 16.14it/s]\u001b[A\n",
      " 14%|█▍        | 144/1000 [00:07<00:48, 17.75it/s]\u001b[A\n",
      " 15%|█▍        | 146/1000 [00:07<00:50, 16.81it/s]\u001b[A\n",
      " 15%|█▍        | 149/1000 [00:07<00:48, 17.38it/s]\u001b[A\n",
      " 15%|█▌        | 151/1000 [00:07<00:47, 17.78it/s]\u001b[A\n",
      " 15%|█▌        | 153/1000 [00:07<00:48, 17.51it/s]\u001b[A\n",
      " 16%|█▌        | 155/1000 [00:08<00:47, 17.92it/s]\u001b[A\n",
      " 16%|█▌        | 157/1000 [00:08<00:48, 17.30it/s]\u001b[A\n",
      " 16%|█▌        | 159/1000 [00:08<00:49, 16.86it/s]\u001b[A\n",
      " 16%|█▌        | 161/1000 [00:08<00:50, 16.76it/s]\u001b[A\n",
      " 16%|█▋        | 163/1000 [00:08<00:49, 16.80it/s]\u001b[A\n",
      " 16%|█▋        | 165/1000 [00:08<00:49, 16.75it/s]\u001b[A\n",
      " 17%|█▋        | 168/1000 [00:08<00:45, 18.20it/s]\u001b[A\n",
      " 17%|█▋        | 171/1000 [00:08<00:44, 18.82it/s]\u001b[A\n",
      " 17%|█▋        | 173/1000 [00:09<00:45, 18.14it/s]\u001b[A\n",
      " 18%|█▊        | 175/1000 [00:09<00:46, 17.83it/s]\u001b[A\n",
      " 18%|█▊        | 177/1000 [00:09<00:46, 17.51it/s]\u001b[A\n",
      " 18%|█▊        | 179/1000 [00:09<00:47, 17.36it/s]\u001b[A\n",
      " 18%|█▊        | 182/1000 [00:09<00:44, 18.36it/s]\u001b[A\n",
      " 18%|█▊        | 184/1000 [00:09<00:45, 18.07it/s]\u001b[A\n",
      " 19%|█▊        | 186/1000 [00:09<00:43, 18.55it/s]\u001b[A\n",
      " 19%|█▉        | 188/1000 [00:09<00:45, 17.91it/s]\u001b[A\n",
      " 19%|█▉        | 190/1000 [00:10<00:45, 17.67it/s]\u001b[A\n",
      " 19%|█▉        | 192/1000 [00:10<00:46, 17.32it/s]\u001b[A\n",
      " 19%|█▉        | 194/1000 [00:10<00:46, 17.29it/s]\u001b[A\n",
      " 20%|█▉        | 196/1000 [00:10<00:46, 17.28it/s]\u001b[A\n",
      " 20%|█▉        | 198/1000 [00:10<00:46, 17.11it/s]\u001b[A\n",
      "  1%|          | 851/161784 [01:00<2:20:23, 19.11it/s]\n",
      " 20%|██        | 202/1000 [00:10<00:47, 16.63it/s]\u001b[A\n",
      " 20%|██        | 204/1000 [00:10<00:48, 16.38it/s]\u001b[A\n",
      " 21%|██        | 206/1000 [00:11<00:48, 16.49it/s]\u001b[A\n",
      " 21%|██        | 208/1000 [00:11<00:47, 16.68it/s]\u001b[A\n",
      " 21%|██        | 210/1000 [00:11<00:46, 17.04it/s]\u001b[A\n",
      " 21%|██▏       | 213/1000 [00:11<00:42, 18.50it/s]\u001b[A\n",
      " 22%|██▏       | 215/1000 [00:11<00:43, 18.21it/s]\u001b[A\n",
      " 22%|██▏       | 218/1000 [00:11<00:38, 20.45it/s]\u001b[A\n",
      " 22%|██▏       | 221/1000 [00:11<00:39, 19.61it/s]\u001b[A\n",
      " 22%|██▏       | 224/1000 [00:11<00:40, 18.96it/s]\u001b[A\n",
      " 23%|██▎       | 226/1000 [00:12<00:43, 17.97it/s]\u001b[A\n",
      " 23%|██▎       | 229/1000 [00:12<00:38, 20.22it/s]\u001b[A\n",
      " 23%|██▎       | 232/1000 [00:12<00:38, 20.20it/s]\u001b[A\n",
      " 24%|██▎       | 235/1000 [00:12<00:39, 19.61it/s]\u001b[A\n",
      " 24%|██▍       | 238/1000 [00:12<00:38, 19.54it/s]\u001b[A\n",
      " 24%|██▍       | 241/1000 [00:12<00:40, 18.82it/s]\u001b[A\n",
      " 24%|██▍       | 243/1000 [00:12<00:40, 18.59it/s]\u001b[A\n",
      " 24%|██▍       | 245/1000 [00:13<00:41, 18.24it/s]\u001b[A\n",
      " 25%|██▍       | 247/1000 [00:13<00:41, 18.09it/s]\u001b[A\n",
      " 25%|██▍       | 249/1000 [00:13<00:42, 17.70it/s]\u001b[A\n",
      " 25%|██▌       | 251/1000 [00:13<00:42, 17.64it/s]\u001b[A\n",
      " 25%|██▌       | 254/1000 [00:13<00:40, 18.43it/s]\u001b[A\n",
      " 26%|██▌       | 256/1000 [00:13<00:41, 17.95it/s]\u001b[A\n",
      " 26%|██▌       | 259/1000 [00:13<00:37, 19.99it/s]\u001b[A\n",
      " 26%|██▌       | 262/1000 [00:13<00:38, 18.96it/s]\u001b[A\n",
      " 26%|██▋       | 264/1000 [00:14<00:40, 18.22it/s]\u001b[A\n",
      " 27%|██▋       | 267/1000 [00:14<00:37, 19.62it/s]\u001b[A\n",
      " 27%|██▋       | 270/1000 [00:14<00:38, 19.20it/s]\u001b[A\n",
      " 27%|██▋       | 272/1000 [00:14<00:39, 18.37it/s]\u001b[A\n",
      " 27%|██▋       | 274/1000 [00:14<00:40, 17.84it/s]\u001b[A\n",
      " 28%|██▊       | 277/1000 [00:14<00:38, 19.02it/s]\u001b[A\n",
      " 28%|██▊       | 280/1000 [00:14<00:35, 20.48it/s]\u001b[A\n",
      " 28%|██▊       | 283/1000 [00:15<00:36, 19.38it/s]\u001b[A\n",
      " 29%|██▊       | 286/1000 [00:15<00:36, 19.81it/s]\u001b[A\n",
      " 29%|██▉       | 289/1000 [00:15<00:34, 20.42it/s]\u001b[A\n",
      " 29%|██▉       | 292/1000 [00:15<00:34, 20.56it/s]\u001b[A\n",
      " 30%|██▉       | 295/1000 [00:15<00:35, 19.64it/s]\u001b[A\n",
      " 30%|██▉       | 298/1000 [00:15<00:34, 20.25it/s]\u001b[A\n",
      " 30%|███       | 302/1000 [00:15<00:31, 21.94it/s]\u001b[A\n",
      " 30%|███       | 305/1000 [00:16<00:33, 20.47it/s]\u001b[A\n",
      " 31%|███       | 308/1000 [00:16<00:33, 20.66it/s]\u001b[A\n",
      " 31%|███       | 311/1000 [00:16<00:35, 19.60it/s]\u001b[A\n",
      " 31%|███▏      | 314/1000 [00:16<00:36, 19.05it/s]\u001b[A\n",
      " 32%|███▏      | 316/1000 [00:16<00:37, 18.37it/s]\u001b[A\n",
      " 32%|███▏      | 319/1000 [00:16<00:35, 19.42it/s]\u001b[A\n",
      " 32%|███▏      | 322/1000 [00:16<00:34, 19.56it/s]\u001b[A\n",
      " 32%|███▏      | 324/1000 [00:17<00:35, 19.06it/s]\u001b[A\n",
      " 33%|███▎      | 327/1000 [00:17<00:34, 19.72it/s]\u001b[A\n",
      " 33%|███▎      | 330/1000 [00:17<00:33, 19.93it/s]\u001b[A\n",
      " 33%|███▎      | 333/1000 [00:17<00:33, 20.13it/s]\u001b[A\n",
      " 34%|███▎      | 336/1000 [00:17<00:34, 19.13it/s]\u001b[A\n",
      " 34%|███▍      | 338/1000 [00:17<00:36, 18.39it/s]\u001b[A\n",
      " 34%|███▍      | 340/1000 [00:17<00:35, 18.45it/s]\u001b[A\n",
      " 34%|███▍      | 342/1000 [00:17<00:36, 18.14it/s]\u001b[A\n",
      " 34%|███▍      | 344/1000 [00:18<00:37, 17.72it/s]\u001b[A\n",
      " 35%|███▍      | 346/1000 [00:18<00:37, 17.47it/s]\u001b[A\n",
      " 35%|███▍      | 348/1000 [00:18<00:36, 17.83it/s]\u001b[A\n",
      " 35%|███▌      | 350/1000 [00:18<00:36, 17.57it/s]\u001b[A\n",
      " 35%|███▌      | 352/1000 [00:18<00:37, 17.32it/s]\u001b[A\n",
      " 35%|███▌      | 354/1000 [00:18<00:36, 17.61it/s]\u001b[A\n",
      " 36%|███▌      | 356/1000 [00:18<00:37, 17.21it/s]\u001b[A\n",
      " 36%|███▌      | 358/1000 [00:18<00:37, 17.00it/s]\u001b[A\n",
      " 36%|███▌      | 361/1000 [00:19<00:36, 17.47it/s]\u001b[A\n",
      " 36%|███▋      | 363/1000 [00:19<00:37, 17.08it/s]\u001b[A\n",
      " 36%|███▋      | 365/1000 [00:19<00:37, 17.08it/s]\u001b[A\n",
      " 37%|███▋      | 367/1000 [00:19<00:37, 16.73it/s]\u001b[A\n",
      " 37%|███▋      | 370/1000 [00:19<00:35, 17.80it/s]\u001b[A\n",
      " 37%|███▋      | 372/1000 [00:19<00:35, 17.52it/s]\u001b[A\n",
      " 37%|███▋      | 374/1000 [00:19<00:34, 17.99it/s]\u001b[A\n",
      " 38%|███▊      | 376/1000 [00:19<00:35, 17.80it/s]\u001b[A\n",
      " 38%|███▊      | 378/1000 [00:20<00:36, 17.13it/s]\u001b[A\n",
      " 38%|███▊      | 380/1000 [00:20<00:36, 17.03it/s]\u001b[A\n",
      " 38%|███▊      | 382/1000 [00:20<00:36, 16.80it/s]\u001b[A\n",
      " 38%|███▊      | 384/1000 [00:20<00:36, 16.74it/s]\u001b[A\n",
      " 39%|███▊      | 386/1000 [00:20<00:36, 16.76it/s]\u001b[A\n",
      " 39%|███▉      | 388/1000 [00:20<00:36, 16.63it/s]\u001b[A\n",
      " 39%|███▉      | 391/1000 [00:20<00:33, 18.03it/s]\u001b[A\n",
      " 39%|███▉      | 394/1000 [00:20<00:30, 19.66it/s]\u001b[A\n",
      " 40%|███▉      | 397/1000 [00:21<00:32, 18.50it/s]\u001b[A\n",
      " 40%|████      | 400/1000 [00:21<00:31, 19.00it/s]\u001b[A\n",
      " 40%|████      | 403/1000 [00:21<00:28, 20.79it/s]\u001b[A\n",
      " 41%|████      | 406/1000 [00:21<00:29, 19.88it/s]\u001b[A\n",
      " 41%|████      | 409/1000 [00:21<00:28, 20.88it/s]\u001b[A\n",
      " 41%|████      | 412/1000 [00:21<00:28, 20.61it/s]\u001b[A\n",
      " 42%|████▏     | 415/1000 [00:21<00:27, 21.32it/s]\u001b[A\n",
      " 42%|████▏     | 418/1000 [00:22<00:27, 21.12it/s]\u001b[A\n",
      " 42%|████▏     | 421/1000 [00:22<00:27, 21.34it/s]\u001b[A\n",
      " 42%|████▏     | 424/1000 [00:22<00:27, 20.99it/s]\u001b[A\n",
      " 43%|████▎     | 427/1000 [00:22<00:26, 21.60it/s]\u001b[A\n",
      " 43%|████▎     | 430/1000 [00:22<00:28, 19.67it/s]\u001b[A\n",
      " 43%|████▎     | 433/1000 [00:22<00:28, 19.61it/s]\u001b[A\n",
      " 44%|████▎     | 435/1000 [00:22<00:29, 19.09it/s]\u001b[A\n",
      " 44%|████▎     | 437/1000 [00:23<00:30, 18.49it/s]\u001b[A\n",
      " 44%|████▍     | 439/1000 [00:23<00:31, 17.96it/s]\u001b[A\n",
      " 44%|████▍     | 442/1000 [00:23<00:29, 19.21it/s]\u001b[A\n",
      " 44%|████▍     | 445/1000 [00:23<00:27, 20.50it/s]\u001b[A\n",
      " 45%|████▍     | 448/1000 [00:23<00:28, 19.45it/s]\u001b[A\n",
      " 45%|████▌     | 450/1000 [00:23<00:29, 18.46it/s]\u001b[A\n",
      " 45%|████▌     | 452/1000 [00:23<00:30, 18.25it/s]\u001b[A\n",
      " 45%|████▌     | 454/1000 [00:23<00:29, 18.59it/s]\u001b[A\n",
      " 46%|████▌     | 456/1000 [00:24<00:30, 18.00it/s]\u001b[A\n",
      " 46%|████▌     | 459/1000 [00:24<00:27, 19.48it/s]\u001b[A\n",
      " 46%|████▌     | 461/1000 [00:24<00:28, 18.60it/s]\u001b[A\n",
      " 46%|████▋     | 463/1000 [00:24<00:29, 18.51it/s]\u001b[A\n",
      " 46%|████▋     | 465/1000 [00:24<00:29, 18.09it/s]\u001b[A\n",
      " 47%|████▋     | 467/1000 [00:24<00:30, 17.45it/s]\u001b[A\n",
      " 47%|████▋     | 469/1000 [00:24<00:30, 17.19it/s]\u001b[A\n",
      " 47%|████▋     | 472/1000 [00:24<00:28, 18.82it/s]\u001b[A\n",
      " 48%|████▊     | 475/1000 [00:25<00:27, 19.19it/s]\u001b[A\n",
      " 48%|████▊     | 477/1000 [00:25<00:28, 18.37it/s]\u001b[A\n",
      " 48%|████▊     | 479/1000 [00:25<00:28, 18.59it/s]\u001b[A\n",
      " 48%|████▊     | 481/1000 [00:25<00:27, 18.83it/s]\u001b[A\n",
      " 48%|████▊     | 484/1000 [00:25<00:27, 19.07it/s]\u001b[A\n",
      " 49%|████▊     | 486/1000 [00:25<00:27, 18.51it/s]\u001b[A\n",
      " 49%|████▉     | 488/1000 [00:25<00:28, 18.09it/s]\u001b[A\n",
      " 49%|████▉     | 490/1000 [00:25<00:29, 17.54it/s]\u001b[A\n",
      " 49%|████▉     | 492/1000 [00:25<00:29, 17.45it/s]\u001b[A\n",
      " 50%|████▉     | 495/1000 [00:26<00:26, 18.82it/s]\u001b[A\n",
      " 50%|████▉     | 497/1000 [00:26<00:27, 18.18it/s]\u001b[A\n",
      " 50%|█████     | 500/1000 [00:26<00:26, 18.93it/s]\u001b[A\n",
      " 50%|█████     | 502/1000 [00:26<00:27, 18.44it/s]\u001b[A\n",
      " 50%|█████     | 504/1000 [00:26<00:27, 17.96it/s]\u001b[A\n",
      " 51%|█████     | 506/1000 [00:26<00:28, 17.63it/s]\u001b[A\n",
      " 51%|█████     | 509/1000 [00:26<00:26, 18.39it/s]\u001b[A\n",
      " 51%|█████     | 511/1000 [00:27<00:27, 18.03it/s]\u001b[A\n",
      " 51%|█████▏    | 513/1000 [00:27<00:27, 17.52it/s]\u001b[A\n",
      " 52%|█████▏    | 515/1000 [00:27<00:27, 17.61it/s]\u001b[A\n",
      " 52%|█████▏    | 517/1000 [00:27<00:27, 17.34it/s]\u001b[A\n",
      " 52%|█████▏    | 519/1000 [00:27<00:26, 18.04it/s]\u001b[A\n",
      " 52%|█████▏    | 521/1000 [00:27<00:25, 18.54it/s]\u001b[A\n",
      " 52%|█████▏    | 523/1000 [00:27<00:26, 18.24it/s]\u001b[A\n",
      " 52%|█████▎    | 525/1000 [00:27<00:26, 17.82it/s]\u001b[A\n",
      " 53%|█████▎    | 527/1000 [00:27<00:27, 17.13it/s]\u001b[A\n",
      " 53%|█████▎    | 529/1000 [00:28<00:28, 16.73it/s]\u001b[A\n",
      " 53%|█████▎    | 531/1000 [00:28<00:26, 17.49it/s]\u001b[A\n",
      " 53%|█████▎    | 533/1000 [00:28<00:27, 17.05it/s]\u001b[A\n",
      " 54%|█████▎    | 535/1000 [00:28<00:27, 17.02it/s]\u001b[A\n",
      " 54%|█████▍    | 538/1000 [00:28<00:26, 17.70it/s]\u001b[A\n",
      " 54%|█████▍    | 540/1000 [00:28<00:25, 17.84it/s]\u001b[A\n",
      " 54%|█████▍    | 542/1000 [00:28<00:25, 18.16it/s]\u001b[A\n",
      " 55%|█████▍    | 545/1000 [00:28<00:24, 18.65it/s]\u001b[A\n",
      " 55%|█████▍    | 547/1000 [00:29<00:25, 17.95it/s]\u001b[A\n",
      " 55%|█████▌    | 550/1000 [00:29<00:23, 19.25it/s]\u001b[A\n",
      " 55%|█████▌    | 553/1000 [00:29<00:22, 19.45it/s]\u001b[A\n",
      " 56%|█████▌    | 555/1000 [00:29<00:24, 18.34it/s]\u001b[A\n",
      " 56%|█████▌    | 558/1000 [00:29<00:23, 19.05it/s]\u001b[A\n",
      " 56%|█████▌    | 560/1000 [00:29<00:24, 18.25it/s]\u001b[A\n",
      " 56%|█████▋    | 563/1000 [00:29<00:23, 18.79it/s]\u001b[A\n",
      " 57%|█████▋    | 566/1000 [00:29<00:22, 19.15it/s]\u001b[A\n",
      " 57%|█████▋    | 568/1000 [00:30<00:23, 18.72it/s]\u001b[A\n",
      " 57%|█████▋    | 570/1000 [00:30<00:23, 18.02it/s]\u001b[A\n",
      " 57%|█████▋    | 572/1000 [00:30<00:23, 17.89it/s]\u001b[A\n",
      " 57%|█████▋    | 574/1000 [00:30<00:24, 17.72it/s]\u001b[A\n",
      " 58%|█████▊    | 576/1000 [00:30<00:23, 17.82it/s]\u001b[A\n",
      " 58%|█████▊    | 578/1000 [00:30<00:24, 17.54it/s]\u001b[A\n",
      " 58%|█████▊    | 581/1000 [00:30<00:22, 18.65it/s]\u001b[A\n",
      " 58%|█████▊    | 583/1000 [00:30<00:23, 17.86it/s]\u001b[A\n",
      " 58%|█████▊    | 585/1000 [00:31<00:23, 17.34it/s]\u001b[A\n",
      " 59%|█████▊    | 587/1000 [00:31<00:23, 17.21it/s]\u001b[A\n",
      " 59%|█████▉    | 589/1000 [00:31<00:24, 16.88it/s]\u001b[A\n",
      " 59%|█████▉    | 591/1000 [00:31<00:24, 16.95it/s]\u001b[A\n",
      " 59%|█████▉    | 593/1000 [00:31<00:24, 16.81it/s]\u001b[A\n",
      " 60%|█████▉    | 595/1000 [00:31<00:23, 17.04it/s]\u001b[A\n",
      " 60%|█████▉    | 598/1000 [00:31<00:20, 19.37it/s]\u001b[A\n",
      " 60%|██████    | 601/1000 [00:31<00:19, 20.29it/s]\u001b[A\n",
      " 60%|██████    | 604/1000 [00:32<00:18, 21.62it/s]\u001b[A\n",
      " 61%|██████    | 607/1000 [00:32<00:18, 21.08it/s]\u001b[A\n",
      " 61%|██████    | 610/1000 [00:32<00:17, 21.94it/s]\u001b[A\n",
      " 61%|██████▏   | 613/1000 [00:32<00:19, 19.81it/s]\u001b[A\n",
      " 62%|██████▏   | 616/1000 [00:32<00:20, 19.11it/s]\u001b[A\n",
      " 62%|██████▏   | 619/1000 [00:32<00:18, 20.56it/s]\u001b[A\n",
      " 62%|██████▏   | 622/1000 [00:32<00:19, 19.50it/s]\u001b[A\n",
      " 62%|██████▎   | 625/1000 [00:33<00:20, 18.67it/s]\u001b[A\n",
      " 63%|██████▎   | 627/1000 [00:33<00:21, 17.75it/s]\u001b[A\n",
      " 63%|██████▎   | 630/1000 [00:33<00:19, 18.97it/s]\u001b[A\n",
      " 63%|██████▎   | 632/1000 [00:33<00:20, 18.13it/s]\u001b[A\n",
      " 64%|██████▎   | 635/1000 [00:33<00:19, 18.52it/s]\u001b[A\n",
      " 64%|██████▍   | 638/1000 [00:33<00:18, 19.07it/s]\u001b[A\n",
      " 64%|██████▍   | 641/1000 [00:33<00:18, 19.16it/s]\u001b[A\n",
      " 64%|██████▍   | 643/1000 [00:34<00:19, 18.31it/s]\u001b[A\n",
      " 64%|██████▍   | 645/1000 [00:34<00:19, 18.52it/s]\u001b[A\n",
      " 65%|██████▍   | 648/1000 [00:34<00:17, 20.59it/s]\u001b[A\n",
      " 65%|██████▌   | 651/1000 [00:34<00:16, 20.93it/s]\u001b[A\n",
      " 65%|██████▌   | 654/1000 [00:34<00:17, 20.33it/s]\u001b[A\n",
      " 66%|██████▌   | 657/1000 [00:34<00:16, 20.98it/s]\u001b[A\n",
      " 66%|██████▌   | 660/1000 [00:34<00:15, 21.32it/s]\u001b[A\n",
      " 66%|██████▋   | 663/1000 [00:34<00:15, 21.33it/s]\u001b[A\n",
      " 67%|██████▋   | 666/1000 [00:35<00:16, 20.84it/s]\u001b[A\n",
      " 67%|██████▋   | 669/1000 [00:35<00:15, 20.99it/s]\u001b[A\n",
      " 67%|██████▋   | 672/1000 [00:35<00:16, 19.86it/s]\u001b[A\n",
      " 68%|██████▊   | 675/1000 [00:35<00:17, 18.61it/s]\u001b[A\n",
      " 68%|██████▊   | 677/1000 [00:35<00:17, 17.98it/s]\u001b[A\n",
      " 68%|██████▊   | 679/1000 [00:35<00:18, 17.50it/s]\u001b[A\n",
      " 68%|██████▊   | 681/1000 [00:35<00:18, 17.65it/s]\u001b[A\n",
      " 68%|██████▊   | 684/1000 [00:36<00:15, 20.12it/s]\u001b[A\n",
      " 69%|██████▊   | 687/1000 [00:36<00:16, 19.00it/s]\u001b[A\n",
      " 69%|██████▉   | 690/1000 [00:36<00:14, 21.12it/s]\u001b[A\n",
      " 69%|██████▉   | 693/1000 [00:36<00:15, 20.33it/s]\u001b[A\n",
      " 70%|██████▉   | 696/1000 [00:36<00:15, 19.22it/s]\u001b[A\n",
      " 70%|██████▉   | 699/1000 [00:36<00:15, 19.69it/s]\u001b[A\n",
      " 70%|███████   | 702/1000 [00:36<00:14, 20.94it/s]\u001b[A\n",
      " 70%|███████   | 705/1000 [00:37<00:14, 20.75it/s]\u001b[A\n",
      " 71%|███████   | 708/1000 [00:37<00:14, 19.70it/s]\u001b[A\n",
      " 71%|███████   | 711/1000 [00:37<00:14, 19.37it/s]\u001b[A\n",
      " 71%|███████▏  | 713/1000 [00:37<00:15, 18.53it/s]\u001b[A\n",
      " 72%|███████▏  | 715/1000 [00:37<00:15, 18.02it/s]\u001b[A\n",
      " 72%|███████▏  | 717/1000 [00:37<00:16, 17.65it/s]\u001b[A\n",
      " 72%|███████▏  | 719/1000 [00:37<00:16, 17.12it/s]\u001b[A\n",
      " 72%|███████▏  | 722/1000 [00:38<00:14, 19.21it/s]\u001b[A\n",
      " 72%|███████▎  | 725/1000 [00:38<00:15, 18.15it/s]\u001b[A\n",
      " 73%|███████▎  | 727/1000 [00:38<00:15, 17.32it/s]\u001b[A\n",
      " 73%|███████▎  | 730/1000 [00:38<00:13, 19.33it/s]\u001b[A\n",
      " 73%|███████▎  | 733/1000 [00:38<00:12, 20.72it/s]\u001b[A\n",
      " 74%|███████▎  | 736/1000 [00:38<00:13, 19.78it/s]\u001b[A\n",
      " 74%|███████▍  | 739/1000 [00:38<00:13, 19.15it/s]\u001b[A\n",
      " 74%|███████▍  | 741/1000 [00:39<00:14, 18.44it/s]\u001b[A\n",
      " 74%|███████▍  | 744/1000 [00:39<00:13, 19.19it/s]\u001b[A\n",
      " 75%|███████▍  | 746/1000 [00:39<00:14, 18.13it/s]\u001b[A\n",
      " 75%|███████▍  | 748/1000 [00:39<00:13, 18.61it/s]\u001b[A\n",
      " 75%|███████▌  | 751/1000 [00:39<00:12, 19.65it/s]\u001b[A\n",
      " 75%|███████▌  | 754/1000 [00:39<00:12, 18.94it/s]\u001b[A\n",
      " 76%|███████▌  | 756/1000 [00:39<00:13, 18.28it/s]\u001b[A\n",
      " 76%|███████▌  | 758/1000 [00:39<00:13, 17.81it/s]\u001b[A\n",
      " 76%|███████▌  | 760/1000 [00:40<00:13, 17.36it/s]\u001b[A\n",
      " 76%|███████▌  | 762/1000 [00:40<00:13, 17.00it/s]\u001b[A\n",
      " 76%|███████▋  | 764/1000 [00:40<00:14, 16.85it/s]\u001b[A\n",
      " 77%|███████▋  | 766/1000 [00:40<00:13, 16.84it/s]\u001b[A\n",
      " 77%|███████▋  | 768/1000 [00:40<00:13, 17.00it/s]\u001b[A\n",
      " 77%|███████▋  | 770/1000 [00:40<00:13, 17.08it/s]\u001b[A\n",
      " 77%|███████▋  | 772/1000 [00:40<00:13, 16.79it/s]\u001b[A\n",
      " 77%|███████▋  | 774/1000 [00:40<00:13, 16.58it/s]\u001b[A\n",
      " 78%|███████▊  | 776/1000 [00:41<00:13, 16.52it/s]\u001b[A\n",
      " 78%|███████▊  | 779/1000 [00:41<00:12, 17.56it/s]\u001b[A\n",
      " 78%|███████▊  | 782/1000 [00:41<00:11, 19.44it/s]\u001b[A\n",
      " 78%|███████▊  | 785/1000 [00:41<00:11, 18.80it/s]\u001b[A\n",
      " 79%|███████▊  | 787/1000 [00:41<00:11, 18.28it/s]\u001b[A\n",
      " 79%|███████▉  | 790/1000 [00:41<00:11, 18.91it/s]\u001b[A\n",
      " 79%|███████▉  | 792/1000 [00:41<00:11, 17.82it/s]\u001b[A\n",
      " 79%|███████▉  | 794/1000 [00:41<00:11, 17.51it/s]\u001b[A\n",
      " 80%|███████▉  | 796/1000 [00:42<00:11, 17.38it/s]\u001b[A\n",
      " 80%|███████▉  | 798/1000 [00:42<00:11, 17.24it/s]\u001b[A\n",
      " 80%|████████  | 800/1000 [00:42<00:11, 16.97it/s]\u001b[A\n",
      " 80%|████████  | 802/1000 [00:42<00:11, 16.89it/s]\u001b[A\n",
      " 80%|████████  | 804/1000 [00:42<00:11, 17.17it/s]\u001b[A\n",
      " 81%|████████  | 806/1000 [00:42<00:11, 16.97it/s]\u001b[A\n",
      " 81%|████████  | 809/1000 [00:42<00:10, 19.06it/s]\u001b[A\n",
      " 81%|████████  | 811/1000 [00:42<00:10, 18.30it/s]\u001b[A\n",
      " 81%|████████▏ | 813/1000 [00:43<00:10, 17.78it/s]\u001b[A\n",
      " 82%|████████▏ | 815/1000 [00:43<00:10, 17.32it/s]\u001b[A\n",
      " 82%|████████▏ | 817/1000 [00:43<00:10, 17.10it/s]\u001b[A\n",
      " 82%|████████▏ | 819/1000 [00:43<00:10, 16.91it/s]\u001b[A\n",
      " 82%|████████▏ | 821/1000 [00:43<00:10, 16.88it/s]\u001b[A\n",
      " 82%|████████▏ | 823/1000 [00:43<00:10, 17.15it/s]\u001b[A\n",
      " 82%|████████▎ | 825/1000 [00:43<00:10, 17.23it/s]\u001b[A\n",
      " 83%|████████▎ | 827/1000 [00:43<00:10, 17.05it/s]\u001b[A\n",
      " 83%|████████▎ | 829/1000 [00:43<00:10, 16.91it/s]\u001b[A\n",
      " 83%|████████▎ | 831/1000 [00:44<00:10, 16.43it/s]\u001b[A\n",
      " 83%|████████▎ | 834/1000 [00:44<00:09, 17.76it/s]\u001b[A\n",
      " 84%|████████▎ | 836/1000 [00:44<00:09, 17.96it/s]\u001b[A\n",
      " 84%|████████▍ | 839/1000 [00:44<00:08, 18.53it/s]\u001b[A\n",
      " 84%|████████▍ | 841/1000 [00:44<00:08, 18.03it/s]\u001b[A\n",
      " 84%|████████▍ | 843/1000 [00:44<00:08, 17.79it/s]\u001b[A\n",
      " 85%|████████▍ | 846/1000 [00:44<00:08, 18.51it/s]\u001b[A\n",
      " 85%|████████▍ | 849/1000 [00:45<00:07, 19.09it/s]\u001b[A\n",
      " 85%|████████▌ | 852/1000 [00:45<00:07, 19.19it/s]\u001b[A\n",
      " 85%|████████▌ | 854/1000 [00:45<00:07, 18.34it/s]\u001b[A\n",
      " 86%|████████▌ | 856/1000 [00:45<00:08, 17.03it/s]\u001b[A\n",
      " 86%|████████▌ | 858/1000 [00:45<00:08, 16.83it/s]\u001b[A\n",
      " 86%|████████▌ | 860/1000 [00:45<00:07, 17.66it/s]\u001b[A\n",
      " 86%|████████▋ | 863/1000 [00:45<00:07, 18.68it/s]\u001b[A\n",
      " 86%|████████▋ | 865/1000 [00:45<00:07, 18.18it/s]\u001b[A\n",
      " 87%|████████▋ | 867/1000 [00:46<00:07, 18.49it/s]\u001b[A\n",
      " 87%|████████▋ | 869/1000 [00:46<00:07, 18.13it/s]\u001b[A\n",
      " 87%|████████▋ | 871/1000 [00:46<00:07, 17.61it/s]\u001b[A\n",
      " 87%|████████▋ | 874/1000 [00:46<00:06, 18.54it/s]\u001b[A\n",
      " 88%|████████▊ | 877/1000 [00:46<00:06, 19.11it/s]\u001b[A\n",
      " 88%|████████▊ | 880/1000 [00:46<00:06, 19.33it/s]\u001b[A\n",
      " 88%|████████▊ | 882/1000 [00:46<00:06, 18.39it/s]\u001b[A\n",
      " 88%|████████▊ | 884/1000 [00:46<00:06, 17.43it/s]\u001b[A\n",
      " 89%|████████▊ | 886/1000 [00:47<00:06, 17.29it/s]\u001b[A\n",
      " 89%|████████▉ | 888/1000 [00:47<00:06, 17.16it/s]\u001b[A\n",
      " 89%|████████▉ | 891/1000 [00:47<00:05, 19.03it/s]\u001b[A\n",
      " 89%|████████▉ | 893/1000 [00:47<00:05, 18.10it/s]\u001b[A\n",
      " 90%|████████▉ | 896/1000 [00:47<00:05, 18.97it/s]\u001b[A\n",
      " 90%|████████▉ | 898/1000 [00:47<00:05, 18.07it/s]\u001b[A\n",
      " 90%|█████████ | 900/1000 [00:47<00:05, 17.29it/s]\u001b[A\n",
      " 90%|█████████ | 902/1000 [00:47<00:05, 17.98it/s]\u001b[A\n",
      " 90%|█████████ | 905/1000 [00:48<00:04, 19.39it/s]\u001b[A\n",
      " 91%|█████████ | 907/1000 [00:48<00:04, 18.74it/s]\u001b[A\n",
      " 91%|█████████ | 909/1000 [00:48<00:04, 18.99it/s]\u001b[A\n",
      " 91%|█████████ | 911/1000 [00:48<00:04, 18.92it/s]\u001b[A\n",
      " 91%|█████████▏| 913/1000 [00:48<00:04, 18.32it/s]\u001b[A\n",
      " 92%|█████████▏| 915/1000 [00:48<00:04, 17.77it/s]\u001b[A\n",
      " 92%|█████████▏| 917/1000 [00:48<00:04, 17.99it/s]\u001b[A\n",
      " 92%|█████████▏| 919/1000 [00:48<00:04, 17.38it/s]\u001b[A\n",
      " 92%|█████████▏| 923/1000 [00:48<00:03, 20.33it/s]\u001b[A\n",
      " 93%|█████████▎| 926/1000 [00:49<00:03, 19.33it/s]\u001b[A\n",
      " 93%|█████████▎| 929/1000 [00:49<00:03, 18.95it/s]\u001b[A\n",
      " 93%|█████████▎| 932/1000 [00:49<00:03, 18.98it/s]\u001b[A\n",
      " 94%|█████████▎| 935/1000 [00:49<00:03, 19.42it/s]\u001b[A\n",
      " 94%|█████████▎| 937/1000 [00:49<00:03, 18.47it/s]\u001b[A\n",
      " 94%|█████████▍| 940/1000 [00:49<00:03, 18.91it/s]\u001b[A\n",
      " 94%|█████████▍| 942/1000 [00:50<00:03, 17.99it/s]\u001b[A\n",
      " 94%|█████████▍| 944/1000 [00:50<00:03, 18.23it/s]\u001b[A\n",
      " 95%|█████████▍| 946/1000 [00:50<00:03, 17.68it/s]\u001b[A\n",
      " 95%|█████████▍| 948/1000 [00:50<00:03, 17.29it/s]\u001b[A\n",
      " 95%|█████████▌| 950/1000 [00:50<00:02, 17.00it/s]\u001b[A\n",
      " 95%|█████████▌| 952/1000 [00:50<00:02, 17.22it/s]\u001b[A\n",
      " 95%|█████████▌| 954/1000 [00:50<00:02, 17.59it/s]\u001b[A\n",
      " 96%|█████████▌| 956/1000 [00:50<00:02, 17.44it/s]\u001b[A\n",
      " 96%|█████████▌| 959/1000 [00:50<00:02, 18.08it/s]\u001b[A\n",
      " 96%|█████████▋| 963/1000 [00:51<00:01, 20.00it/s]\u001b[A\n",
      " 97%|█████████▋| 966/1000 [00:51<00:01, 19.15it/s]\u001b[A\n",
      " 97%|█████████▋| 968/1000 [00:51<00:01, 18.43it/s]\u001b[A\n",
      " 97%|█████████▋| 971/1000 [00:51<00:01, 19.24it/s]\u001b[A\n",
      " 97%|█████████▋| 974/1000 [00:51<00:01, 20.57it/s]\u001b[A\n",
      " 98%|█████████▊| 977/1000 [00:51<00:01, 20.04it/s]\u001b[A\n",
      " 98%|█████████▊| 980/1000 [00:51<00:00, 20.52it/s]\u001b[A\n",
      " 98%|█████████▊| 983/1000 [00:52<00:00, 21.29it/s]\u001b[A\n",
      " 99%|█████████▊| 986/1000 [00:52<00:00, 22.50it/s]\u001b[A\n",
      " 99%|█████████▉| 989/1000 [00:52<00:00, 21.68it/s]\u001b[A\n",
      " 99%|█████████▉| 992/1000 [00:52<00:00, 20.04it/s]\u001b[A\n",
      "100%|█████████▉| 995/1000 [00:52<00:00, 20.07it/s]\u001b[A\n",
      "100%|█████████▉| 998/1000 [00:52<00:00, 19.22it/s]\u001b[A\n",
      "100%|██████████| 1000/1000 [00:52<00:00, 18.87it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['on on the following morning every hope of this kind ',\n",
       " 'any any case i i going to call you by ',\n",
       " 'that that that that the sand sand back on the ',\n",
       " 'came and taking my arm led me to a building ',\n",
       " 'has has has never a friend friend nor do ',\n",
       " 'forcheville forcheville introduced on by his had had ceased to ',\n",
       " 'kuragin kuragin said said s s having a laugh ',\n",
       " 'four four date said said lifting lifting his head ',\n",
       " 'she wasn t exactly hard to miss ',\n",
       " 'dear dear s a natural ',\n",
       " 'r ',\n",
       " 'sir sir is how much this opinion is life furthering ',\n",
       " 'paulina paulina mayor mayor to to ',\n",
       " 'had had one his father s hooked nose ',\n",
       " 'many many organizations wanted to obtain dinners in his honor ',\n",
       " 'was was not ',\n",
       " 'forgive forgive me that that that ',\n",
       " 'he he yelled pointing at a a head ',\n",
       " 'you you re all in my means means should hand ',\n",
       " 'the the upper part of the the masses of concrete ',\n",
       " 'my greatgreat grandfather see charmingly charmingly headmaster ever ever had ',\n",
       " 'see see said the old man stopping and turning round ',\n",
       " 'may may may he answer ',\n",
       " 'quite quite quite as much much for breath ',\n",
       " '3 though though though though though though the the youngest ',\n",
       " 'detached detached towards him an a a finger he felt ',\n",
       " 'they they with the young ladies to the door of ',\n",
       " 'harry harry rather rather than heard a or or ',\n",
       " 'at at this time said and and ',\n",
       " '',\n",
       " 'said said then that it was not in order ',\n",
       " 'returns returns methodically as he softly rubs his hands went went ',\n",
       " 'it it unfortunately favored the brave deaf man ',\n",
       " 'was was a stroke of civility for which she was ',\n",
       " 'i i ron round on top stool at the ground ',\n",
       " 'sisters sisters were uneasy for her but her mother was ',\n",
       " '4 ',\n",
       " 'let let i ',\n",
       " 'meeting meeting meeting to until finally it would have been ',\n",
       " 'it it is too with human acts ',\n",
       " '',\n",
       " 'beheld beheld the villagers scatter at the fall of the ',\n",
       " 'at at that very instant a cloud of smoke spread ',\n",
       " 'very very day that campaign campaign the order to cross the ',\n",
       " 'four four four two covered were went off laden to ',\n",
       " 'had had been assured of the of of of ',\n",
       " 'with with such ideas what motive have you for living ',\n",
       " 'what what s that supposed to mean a saving peoplething ',\n",
       " 'don don t like him she added in a tone ',\n",
       " 'fear fear so little for anything ',\n",
       " 'forward forward with his tiny working for to to go ',\n",
       " 'he said holding out his hand to hand copyright ',\n",
       " 'little little maid s face was so eager and her ',\n",
       " 'allen allen ricote ricote recompensed and willarski that well the ',\n",
       " 'is is hence over the master s death for this ',\n",
       " 'it it not a very common word ',\n",
       " 'bilbo bilbo bilbo chosen in much of to to to ',\n",
       " 'says says but he he he the city ',\n",
       " 'he he he her hands over her mouth ',\n",
       " 'wasn wasn t hot in ',\n",
       " 'as as good as killed them he croaked ',\n",
       " 'still still there was never enough room in room ',\n",
       " 'hope hope said ',\n",
       " 'at at took took my sad and and dejected my heart ',\n",
       " 'he he scrambled to the end of his bed for ',\n",
       " 'your your notes and and send a copy to the prophet ',\n",
       " 'naturally naturally breathed his cheeks ',\n",
       " 'i i here as i i you in my letter ',\n",
       " 'i960 ',\n",
       " 'you you see the l l l ',\n",
       " 'took took a step towards the hobbit and he seemed ',\n",
       " 'through through the war the shadow lay empty and unused ',\n",
       " 'they they say professor send back back with your answer ',\n",
       " 'control control of the barons castles loomed above the trees ',\n",
       " 'robert robert to have swum and the lake s grounds ',\n",
       " 'had had stated that he would rebuild one one one harry ',\n",
       " 'other other words they were the forerunners of the ',\n",
       " 'it it arises he continued that then we see any ',\n",
       " 'zoo zoo must be recorded ',\n",
       " 'i i sure that once do take the we we ',\n",
       " 'have have seen exclaimed exclaimed in in amazement ',\n",
       " 'washington washington macy that had had sent to the the ',\n",
       " 'one one news of the visitors visitors been the ',\n",
       " 'any any its construction was in the parks were being ',\n",
       " 't t you it hard on on on account of ',\n",
       " 'time time to time she put the blame on a ',\n",
       " 'my my my my my young cousin it is good ',\n",
       " 'turns turns people knelt down beside him trying to do her ',\n",
       " 'me me our time presses ',\n",
       " 'every every snag that arose from from playing knife opened ',\n",
       " 'stunned stunned from the cor ',\n",
       " 'as as soon as the votes were counted the truce ',\n",
       " 'seven ',\n",
       " 'wearily wearily ',\n",
       " 'result result of politicians politicians ',\n",
       " 'he he he he he he spargo to have the match ',\n",
       " 's s she she said in a terrified voice ',\n",
       " 'escalus escalus it is true said ',\n",
       " 'w w almost twice as tall a a new and ',\n",
       " 'who who it was closer ',\n",
       " 'bilbo bilbo keen eager amazement swift the windows were open ',\n",
       " 'love love you ',\n",
       " 'no no ',\n",
       " 'crookshanks crookshanks s face was stirring ',\n",
       " 'the the reader picture to himself a series of visages ',\n",
       " 'is is such a thing as the professional conscience which ',\n",
       " 'expected expected you to stay two months ',\n",
       " 'is is is killing and and can can no more ',\n",
       " 'and and looked looked at each other ',\n",
       " 'mankind mankind s spiritual disorderliness with its never settled questions ',\n",
       " 'be be be very good of ',\n",
       " 'evidently evidently evidently changed and plainly in today health but ',\n",
       " '',\n",
       " 'farther farther back in history of object of our observation ',\n",
       " 'further further measure of his his his own more settled ',\n",
       " 'do do you mean what what what help him ',\n",
       " 'his his his his own has answer the trace of ',\n",
       " '',\n",
       " 'you you think he s all right she squealed ',\n",
       " 'is is a great favourite with my girls said ',\n",
       " 'and and and were were confident that there would be ',\n",
       " 'when when he met me he knew was was no ',\n",
       " '6 000 000 wf 7 7 7 ',\n",
       " 'picked picked it up ',\n",
       " 'snape snape or a moment watching the headmaster with an expression ',\n",
       " 'was was like looking down on to a sloping cloud ',\n",
       " 'had had had had waited for this visit bring ',\n",
       " 'much much are you prepared to carry on your backs ',\n",
       " 'more more more more we stay the stronger grows our ',\n",
       " 'lady lady it may turn turn so now the turn ',\n",
       " 'at at when when that my parents were contemplating marriage ',\n",
       " 'my my lord best know that that will seem to ',\n",
       " 'had had it be portrayed to the public in attractive ',\n",
       " 's s screeches started up again running cheering below ',\n",
       " 'am am sorry for that said the provost but it ',\n",
       " 'was was it and more than once tears rose to ',\n",
       " 'not not neither by by the honour of you parents ',\n",
       " 'was was as at her closely through his half moon ',\n",
       " 'easy easy it is how little effort it needs to ',\n",
       " 'it it is not that i may become moral ill ',\n",
       " 'took took her her handkerchief and began to cry ',\n",
       " 'his own son ',\n",
       " 'their their seats on the church church the the zoo ',\n",
       " 'the the matters to a head it ',\n",
       " 'is is old and feeble and concerned concerned to condemn ',\n",
       " 'of of of answered answered answered and and and he ',\n",
       " 'without without waiting to hear him call a his horse ',\n",
       " 'can can tell ',\n",
       " 'as as good seldom or never comes pure and unmixed ',\n",
       " 'a a a a trace we d no time to ',\n",
       " 'neville neville there total 1928 ',\n",
       " 'told told him my father hated wizards wizards ',\n",
       " 'had had come from the left flank where their regiment ',\n",
       " 'many many years old girl ',\n",
       " 'last last thing he wanted was for ten ten start ',\n",
       " '134 the fellowship of the ring what what are they ',\n",
       " 'could could not explain to that that that that while ',\n",
       " 'beginning beginning to the tips of his little wooden house ',\n",
       " 'is is this woman he said staring at giving signature ',\n",
       " 'was was a little change from to to to ',\n",
       " 'he he gazes at the sky at other times he ',\n",
       " 'why why do you expect that he will leave us ',\n",
       " 'had had not known how the human shoulder was constructed ',\n",
       " 'here here feeling his dignity at such a pass safer ',\n",
       " 'it it s going to help we re the worst ',\n",
       " '',\n",
       " 'a a long time like a sick man who cannot ',\n",
       " 'was was not a gambler at any rate he did ',\n",
       " 'you you you ',\n",
       " 'sanchica sanchica to all whom it may concern de inserts ',\n",
       " 'don don hear a thing he said tensely ',\n",
       " 'is is is merely human too too human this this ',\n",
       " 'one one one one of the nymphs who it was ',\n",
       " 'across across the floor and and in a second door ',\n",
       " 'editorial editorial was incorrect the logic of arguments arguments was ',\n",
       " 'he he heard the sound of many voices all around ',\n",
       " 'really really really was lying and red patches came on ',\n",
       " 'was was it it hot touch of the combat of ',\n",
       " 'it it came to me just like that ',\n",
       " 'very very funny ',\n",
       " 'she she with it choice choice find out if he ',\n",
       " 'with with a second whoosh and and cried the ',\n",
       " 'she she long and intently at these two soldiers ',\n",
       " 'really really corking to see you said said that s ',\n",
       " 'he he ill he informed ',\n",
       " 'that that no such wants could have shown him ',\n",
       " 'went went in out of curiosity you know and there ',\n",
       " 'saw saw it then and told everybody you and ',\n",
       " 'that that remains is to set out on our journey ',\n",
       " 'is is easy to see replied that that that thou ',\n",
       " 'robert robert for a moment man and boy staring at ',\n",
       " 'reformers reformers more than slightly to to a black and ',\n",
       " 'it it as as as was the case that day ',\n",
       " 'had had been to call upon the dear old infant ',\n",
       " 'voices voices in in the knot said and and there ',\n",
       " 'it it them as should be sorry dad dad yeh ',\n",
       " 'seems seems incredible that more more more more there there ',\n",
       " 'his his his and waved at he he he stared ',\n",
       " 'he he had his his views through mass mailings of ',\n",
       " 'succeeding succeeding the mile had been perched up on ',\n",
       " 'he he felt a cold breeze on the cards of ',\n",
       " 'is is the public ouse where was was took to ',\n",
       " 'that that seems to a a worthy goal then we ',\n",
       " 'head head his nose on a corner of the filthy ',\n",
       " 'could could they let him enter that tournament he s ',\n",
       " 'dear dear he he he he he as steady as ',\n",
       " 'shall shall have a as as soon as you are ',\n",
       " 'had had always thought that some accident might happen which ',\n",
       " 'cannot cannot cannot show himself over kind to ',\n",
       " 'one one he d just left and and looked looked ',\n",
       " 'do do this art must must ',\n",
       " 'are are free to work for work work work work ',\n",
       " 'the replied replied very solemnly ',\n",
       " 'now now in our own yard we asked them in ',\n",
       " 'said staring staring staring at the three of them ',\n",
       " 'p p ',\n",
       " 'no no ',\n",
       " 'i i home with the the the streets that were ',\n",
       " 'they they re coming up here to do it said ',\n",
       " 'seeing seeing the count the major domo made a significant ',\n",
       " 's s home range the stone with with the triborough park ',\n",
       " 'that that that the the the wife of a gentleman ',\n",
       " 'she she remained for a moment stunned watching the water ',\n",
       " 'the the no notice might of any any ',\n",
       " 'suffering suffering suffering suffering and and and and ',\n",
       " 'surface surface now shone copper copper and pans had been ',\n",
       " 'mum mum ',\n",
       " '10 1937 ',\n",
       " 'the the same moment without without drawing breath screamed joyously ',\n",
       " 'had had moses each normal community thing ',\n",
       " 'a a far younger man ',\n",
       " 'his his his leg over the broom and kicked off ',\n",
       " 'together together he withdrew away into a room ',\n",
       " 'was was taken aback at the sight of them nor ',\n",
       " 'depth depth of the misunderstanding was shown in a remark ',\n",
       " 'you you go go as ',\n",
       " 'presumably presumably the federal government will have to stand a ',\n",
       " 'who who during the 1920 s had purchased key parcels ',\n",
       " 'he he harry one more mouthful each of the miruvor ',\n",
       " 'wasn wasn very queer all the time they were here ',\n",
       " 'to to being married a girl likes to be crossed ',\n",
       " 'more more of life which had seemed to her annihilated ',\n",
       " 'and and mrs mrs mrs mrs mrs ',\n",
       " 'real real estate agent told him that it had belonged ',\n",
       " 'that that all right thought thought for a moment ',\n",
       " 'dreaded dreaded succeeded term and term and vacation ',\n",
       " 'what what did you you wrong asked asked ron ron ',\n",
       " 'you you will bless us with your approval and judy ',\n",
       " 'bedroom bedroom swung flew open and and instinctively instinctively absent ',\n",
       " 'also also thought he says that many engrossed by the ',\n",
       " 'come come put put it brighter face upon it ',\n",
       " 'of of of the idea idea idea to anyone interested ',\n",
       " 'the the scene dissolved once more ',\n",
       " 'and and and and and and and mrs mrs mrs mrs ',\n",
       " 'me me says says he was the sovereign of a ',\n",
       " 'is is there there like his two old fashioned candlesticks ',\n",
       " 'right right bye said who who could sam his voice ',\n",
       " 'a happy feeling ',\n",
       " 'he he opened program ',\n",
       " 'and and her are are alone again he he soared ',\n",
       " 'am am very sure sir continued that that cried cried ',\n",
       " 'yes yes that s right said beaming beaming beaming at her ',\n",
       " 'burned burned and empty empty empty empty there of difficulty ',\n",
       " 'he he became alarmed he put his hands on his ',\n",
       " 'added added with great solemnity ',\n",
       " 'quasimodo quasimodo that there must have been a victory ',\n",
       " 'ginny ginny ginny ginny ',\n",
       " 'thought thought for a moment it had exploded a roar ',\n",
       " 'it it necessary necessary to do he said ',\n",
       " 'will will confess to my dear that that in spite ',\n",
       " 'third third time the two two back harry quickly and ',\n",
       " 'even even that point of view was not often of ',\n",
       " 'the the time he left power in 1968 they were ',\n",
       " 'at at the table table was was was one of the ',\n",
       " 'they know things ',\n",
       " 'i i said said he was like black chap ',\n",
       " 'how how an affection ungrateful little suddenly suddenly suddenly suddenly ',\n",
       " 'then then then ',\n",
       " 'and this charming young lady tells me she knows you ',\n",
       " 'they they are free to move between their own portraits ',\n",
       " 'built built up the young man s reputation by running ',\n",
       " 'said said her her with that particular position immediately sir ',\n",
       " 'has has been here in day my dear and ',\n",
       " 'i i other things to do ',\n",
       " 'walls walls were all on fire and the back wall ',\n",
       " 'in in fact were on the verge of inferi ',\n",
       " 'was was he much younger with with his wide wide ',\n",
       " 'began began so follows the the enemy door left wing ',\n",
       " '',\n",
       " 'asked asked asked them what she was they had heard ',\n",
       " 'come come walking in from the remote parking fields would ',\n",
       " 'that that s ',\n",
       " 'she she did have were a a a ',\n",
       " 'found found this and fang fang going to keep it ',\n",
       " 'in in the good old days when there was still ',\n",
       " 'wormtail wormtail he has mobilize us against the ',\n",
       " 'is is ',\n",
       " 'this this the duchess laughing all the while said george george ',\n",
       " 'take take away said mrs mrs ',\n",
       " 'i i it now attend to ',\n",
       " '',\n",
       " 'had had you that day to sleep at the ',\n",
       " 'suppose suppose they wanted to be here to see it ',\n",
       " 'is is the the late by ',\n",
       " 'things things do do you mean demanded snarled snarled ',\n",
       " 'was what he appeared to be waiting ',\n",
       " 'this this another another another particular secretary and put ',\n",
       " 'farewell farewell thy poor with and come to me ',\n",
       " 'i i little disturbances in the surrounding air was was ',\n",
       " 'he he himself facing against the wall as the teachers ',\n",
       " 'others others but but to to to be trusted away by ',\n",
       " 'he he went on with with with with his world ',\n",
       " 'the the other ladies with which men described ',\n",
       " 'not not finished she said poking her head into the ',\n",
       " 'you you re surprised at the say she rides ',\n",
       " 'the the tales that we have heard this day the ',\n",
       " 'nowadays nowadays that she remains beautiful with her waist length ',\n",
       " 'stand aside now ',\n",
       " 'so so kept her place at the table ',\n",
       " 'go go this hour ',\n",
       " 'i i to to march unlucky being that is is ',\n",
       " 'i i i i are are many different ever changing ',\n",
       " 'no no doubt it always is and must be so ',\n",
       " 'is is there cried a toothless voice ',\n",
       " 'his his jealousy which had taken more subjects than any ',\n",
       " 'usually inspired in her ',\n",
       " 'anyway anyway at was before the the the forest ',\n",
       " 'elderly elderly elderly one of three tenants left in a ',\n",
       " 'don don t know ',\n",
       " 'apparition apparition always so fatal for her and which had ',\n",
       " 'found found he could hide from monument and moonshine and ',\n",
       " 'that that s nonsense the old thing is not to ',\n",
       " 'said said karkaroff karkaroff ',\n",
       " 'each each commissioner la sworn in he walked out of ',\n",
       " 'a a few moments the only sounds were the shouts of ',\n",
       " 'the the the meeting s only for members of the ',\n",
       " 'third third thing how else was it you talked about ',\n",
       " 'will will do all she can to make it happy ',\n",
       " 'mother mother good woman who had never known what to ',\n",
       " 'am am not such a coward ',\n",
       " 'i i that out of a public cases only one ',\n",
       " 'frodo frodo he did isn t widely known ',\n",
       " 'then then we were so merry all the way home ',\n",
       " 'parents parents weren t likely to playing their children exposed ',\n",
       " 'of of the were were currently pointing to the home while ',\n",
       " 'and and bear your aged friend he he had said ',\n",
       " 'last last all of them were out ',\n",
       " 'i i might might might let us have hers she ',\n",
       " 'boromir boromir fell shaking harry out of anger but he ',\n",
       " 'that that thing the dementor stood there and looked around ',\n",
       " 'any any rate she had the pleasure of receiving and ',\n",
       " 'was was as always at evening parties wearing a dress ',\n",
       " 'he he took his family swimming he watched them from ',\n",
       " 'not not the real world she said softly ',\n",
       " 'is is an enormous acreage in which which which this ',\n",
       " 'let let me alone my my my mother were the the ',\n",
       " 'the the wharf would have been cheap as would also ',\n",
       " 'the the result must be was quite obvious and unfortunate ',\n",
       " 'water water was was curiously mirrored on hepzibah hepzibah face ',\n",
       " 'said said suddenly suddenly ',\n",
       " 'courage courage bailiff and turkeys and hams cold lobsters in ',\n",
       " 't t t you think they might wonder what s ',\n",
       " 'they they approached he took it out of his own ',\n",
       " 'have have got got their in my eyes said ',\n",
       " 'it it never think of this word in connection with ',\n",
       " 'at at any rate is will will will probably him ',\n",
       " 'one one thing was certain a thing idea of sets ',\n",
       " 'said said mr mr ',\n",
       " 'can can go back to bed dear he added to ',\n",
       " 'he he no warning cap on his head nor had ',\n",
       " 'and and handsome and clever he was soon given particular ',\n",
       " 'don don t think too badly of me once you ',\n",
       " 'you you let the wolf ',\n",
       " 'officer officer who had been sent to inquire met ',\n",
       " 'if somebody made a mistake went went on and let ',\n",
       " 'o o o he added to ',\n",
       " 'o o joke we we at him though though listening ',\n",
       " 'it it snape threaten as he did again and again ',\n",
       " 'was was almost but quite quite a state of conversion ',\n",
       " '270 the fellowship of the ring don don t know ',\n",
       " 'telephoning telephoning over a stile becoming visible right up to ',\n",
       " 'berg berg and and scratched his head and yawned like ',\n",
       " 'try try to write all this self because my heart ',\n",
       " 'trains trains trains trains trains did not always run late ',\n",
       " 'the the it out and stared at the name written ',\n",
       " 'all all hear a female going on like that before ',\n",
       " 'the the hearing hearing of names from in his own ',\n",
       " 'was was all that they could do to get the ',\n",
       " '312 ',\n",
       " 'and and how horrible a year year old boy has ',\n",
       " 'know know what you re thinking said and and ',\n",
       " 'not not even that could be said for those who ',\n",
       " 'may may go ',\n",
       " 'say say say say you ',\n",
       " 's s s report confirmed the horse patrols who were ',\n",
       " 'he he came to may it was still a rather ',\n",
       " 'only only only one one bed and set set set off ',\n",
       " 'a a a look of warning at him ',\n",
       " 'blood blood iron your candle ',\n",
       " 'harry harry harry from the the the clunk wizard while ',\n",
       " 'can can read the heart ',\n",
       " '',\n",
       " 'history history and and and the the the case case case ',\n",
       " 'have have my sources said in in a self ',\n",
       " 'anyway anyway now was was going to pull aside the ',\n",
       " 'that that that was was a slave of the ',\n",
       " 'can can t walk ',\n",
       " 'never never lived on ',\n",
       " 'we we we won the first round in a court ',\n",
       " 'the the the duke we talk of were returned again ',\n",
       " 'master master master said he ',\n",
       " 'immediately immediately alone he didn t want all that said ',\n",
       " 'people people as man jury mean so little of us ',\n",
       " 'w w quite true it it soon appeared ',\n",
       " '75 153 ',\n",
       " 'had had risked everything always to see to to help ',\n",
       " 'all all duels on friends who advise have but to ',\n",
       " 'it it is not uncommon ',\n",
       " 'got got up stretched her front legs and then moved ',\n",
       " '1 1 94 1 ',\n",
       " 'you you must admit that it was very intriguing and ',\n",
       " 'that that must be murdered murdered own ',\n",
       " 'was was still no sound of pursuit ',\n",
       " 'but but she said that it was all over and ',\n",
       " 'and and were were in it too ',\n",
       " 'blank blank picture on the wall sniggered again ',\n",
       " 'this this my brother ',\n",
       " 'never never never liked us tell people that she would not ',\n",
       " 'send send or or or or somebody to me she said ',\n",
       " 'alongside alongside to a halt clutching at the stone wall ',\n",
       " 'asked asked this and then became confused feeling that she ',\n",
       " 'a a party bursting for the the his cast by ',\n",
       " 'of of various in ',\n",
       " 'rather rather like a man for after straining his eyes ',\n",
       " 'i i to get into said said again again ',\n",
       " 'just just just as much said the landlady ',\n",
       " 'his his hands he carried on a large leaf as ',\n",
       " 'were were always good friends and are we are better ',\n",
       " 'had had told the curate and the barber of the ',\n",
       " 'must must go away too take away what you can ',\n",
       " '29 1969 ',\n",
       " 'that that depends your your fingers with with the of your ',\n",
       " 's s what you did to look the locket he ',\n",
       " 'karkaroff karkaroff feelings ',\n",
       " 'joined joined in with ',\n",
       " 'is is don don t know is a man alive ',\n",
       " 'you don t like the dementors do you said said ',\n",
       " 'on on the flowers were more numerous paler less glossy ',\n",
       " 'spread spread spread moscow moscow us year year lost its ',\n",
       " 'did did not like talking about his life as a ',\n",
       " 'evidently evidently evidently evidently by his extraordinary deference for ',\n",
       " 'sign sign of him of own own by by ',\n",
       " 'he he took with with him when he went abroad ',\n",
       " 's s your arm said ',\n",
       " 'during during a press conference he had incautiously himself himself ',\n",
       " 'it it then said said the barber and we will ',\n",
       " 'of of injustice p p ',\n",
       " 'whispered whispered whispered ',\n",
       " 'it it it it not the way state to invest ',\n",
       " 'he he for his death from from from to to ',\n",
       " 'there there was any respect for one thoughts there was ',\n",
       " '1 300 families 9 9 9 1940 ',\n",
       " 'he he know that he he did not not not ',\n",
       " 'when when months later he finally got to face ',\n",
       " 'and and and arrived arrived in the first first first ',\n",
       " 'in in order finally to touch the hearts of the ',\n",
       " 'i i i him outside said once once milk all over ',\n",
       " 'the the cabman to be discharged your honor yes yes ',\n",
       " 'in in a great exposition interested in in a place ',\n",
       " 'bagnet bagnet bagnet bagnet bassoon like ',\n",
       " 'not not answered ',\n",
       " 'up up with this might rm a callous attitude but ',\n",
       " 'would would moses parks s plan plan great was on ',\n",
       " 'cannot cannot said mr mr ',\n",
       " 'that that that that she was very perfectly perfectly free ',\n",
       " 'help help help help as as he proves true ',\n",
       " 'day day we to be a in in in the ',\n",
       " 'my my my my lord turn back ',\n",
       " 'might might have er moved on to fire and better ',\n",
       " 'it it never saw the map ',\n",
       " 'his his marriage in 1910 he worked as a librarian ',\n",
       " 'but it gave me strength it cleared my mind ',\n",
       " 'his his house in he he had a room room ',\n",
       " 'said said that if he he to a supper room ',\n",
       " 'only only long chafed at the power that had had become ',\n",
       " 's s s being very dreadful werewolf all the moment ',\n",
       " 's s not they recent if he s six hundred ',\n",
       " 'girl girl s a figure running through the dust toward ',\n",
       " 'but but a gentleman without strict conscience disdainful of all ',\n",
       " 's s your contains pictures of the fort in each ',\n",
       " 'had had with the deepest bewilderment listened and and fro ',\n",
       " 'pierre pierre unexpectedly found itself so close to the emperors emperors ',\n",
       " '19 19 silent ',\n",
       " 'he he recoiled with horror ',\n",
       " 'and and and and welcome to you else else else ',\n",
       " 'a a massive effort looked looked away from her dropped ',\n",
       " 'came came along singing across necks we did did ye ',\n",
       " 'giving giving ',\n",
       " 'is is it said then then as well as ',\n",
       " 'or did he grasped grasped the lid of the packing ',\n",
       " 'you you you them counted said said kutuzov kutuzov ',\n",
       " 'in in the intervals of these roofs of these spires ',\n",
       " 'handsome handsome of the pain from the burns covering his ',\n",
       " 'held held up ten stubby when fingers then her smile ',\n",
       " 'he he knew where to do it ',\n",
       " 'by by the autumn of 1934 thanks to the success ',\n",
       " 'will will be no need for you to accompany us ',\n",
       " 'moody moody nonplussed ',\n",
       " 'that that that gentleman may have done me the honour ',\n",
       " 'line line procession people coming up the hill from ',\n",
       " 'have have only two words to say to you he ',\n",
       " 'had had the strangest feeling that there was someone standing ',\n",
       " 'his his lifetime and likewise in the period of all all ',\n",
       " 'is is the folk folk folk kept kept kept afraid ',\n",
       " 'came came came and we are about worldly ',\n",
       " 'might might be happy because he did not kill or ',\n",
       " 'were were no longer either scholars or ambassadors or bourgeois ',\n",
       " 'should should recommend a hasty sandwich ',\n",
       " 'cousin cousin what you speak speak to us of a ',\n",
       " 'approaching approaching them having descended a decline were no longer ',\n",
       " 'wood wood son son son said ',\n",
       " 'looking looking about him panting from an ear chair ',\n",
       " 'always always liked big friends who d look after you ',\n",
       " 'of of go of face face brings up the vision ',\n",
       " 'indeed indeed desire for sharpness and exactness or for beauty ',\n",
       " 'a a feeling blow like a storm and yet not ',\n",
       " 'of of of it it no longer to be thought ',\n",
       " 'memos memos began to display marked irritation with the silence silence ',\n",
       " 'did did not know we where they all could he ',\n",
       " '7 1938 to to ',\n",
       " 'you you the biggest blood traitor was there is ',\n",
       " 'later later he would recall the feeling that had swept ',\n",
       " 'it it is too late now to arrest and turn ',\n",
       " 'voldemort voldemort wipe wipe his body in some hole ',\n",
       " 'paper paper the paper on which the park park apartment ',\n",
       " 'said said said bill bill ',\n",
       " 'to to men who do not admit that was was ',\n",
       " 'it it von me deeply that this this this my ',\n",
       " 'whether he calls himself or or or ',\n",
       " 'in in it as chief chief the received the most ',\n",
       " 'he he got a lot to learn ',\n",
       " 'would would would not have cursed the barrack square in ',\n",
       " 'our our our father with his enemies fled fled his ',\n",
       " 'it it adamant and uninterested ',\n",
       " 'were were just discussing your friend said said said to ',\n",
       " 'other other explanation as a single injured bondholder could sue ',\n",
       " 'being being nothing else for it they filed out and ',\n",
       " 'it it is said said gandalf gandalf ',\n",
       " '21 ',\n",
       " 'was was was a bit short with us earlier ',\n",
       " 'he he focused his own ',\n",
       " 'he he he fond of i i parents wished me ',\n",
       " 'not not later said he in a low voice and ',\n",
       " 'two two two galleys now joined company and at four ',\n",
       " 'said said that the route was so built up that ',\n",
       " 'no no and with their support of no no proposals ',\n",
       " 's s face ',\n",
       " 'i i it dejectedly that even and and looked looked ',\n",
       " 'don don t know who put my name has name ',\n",
       " 'with with one exception many were only studies no immediate ',\n",
       " 'stayed stayed silent ',\n",
       " 'was was galloping around away from him across the black ',\n",
       " 'ii ii s the on the the the chapter dropped ',\n",
       " 'thestrals thestrals he said loudly ',\n",
       " 'when when you nothing it was was as to as ',\n",
       " 'both both looked up when came came in and ',\n",
       " 'mundungus mundungus in the common room that evening but his ',\n",
       " 'was was soon followed by and and ',\n",
       " 'that that strange contradiction now difficult to understand between the ',\n",
       " 'was was just something nobody could be against ',\n",
       " 'would would remind everybody that the behavior of these dementors ',\n",
       " 'has has come to tell you sir ',\n",
       " 't t you see m m not well might might ',\n",
       " 'smiled smiled at him and said laughing is is a ',\n",
       " 'turned turned it over his eyes on ',\n",
       " 'is is a beautiful case a beautiful case and what ',\n",
       " 'involuntarily involuntarily flushed with pleasure at the aptitude of this ',\n",
       " 'within within five football fields as part of the ',\n",
       " 'the the unit unit unit s relocation report completed ',\n",
       " '42 ',\n",
       " 'was was sniffling in the back seat his father had ',\n",
       " 'sun sun came up the flames vanished flags were unfurled ',\n",
       " 'reading reading he passed to set from sleeping to gossip ',\n",
       " 'so so were the economic realities that had kept the ',\n",
       " 'giving giving the clerk orders about the work to be ',\n",
       " 'far far more than 10 000 000 cars ',\n",
       " 'sir sir leicester leicester lately become better acquainted with him ',\n",
       " 'seized seized it pulled the door open and pushed open open ',\n",
       " 'took took prisoner stations stations stations stations of the mention mention ',\n",
       " 'he he hard to tell whether his his his ',\n",
       " 'from from from from running distance distance quietly ',\n",
       " 'come come you re not going to ve ve ve ',\n",
       " 's s true said getting getting up her feet ',\n",
       " 'the the says says says had had been incensed ',\n",
       " 'the the parkway and the apartment houses to the left ',\n",
       " '',\n",
       " 'sister sister come said a a little more gaily you ',\n",
       " 'new new hour comes ',\n",
       " '',\n",
       " 'commander commander commander in on on 1809 1809 ',\n",
       " 'it it is go to to hour no no ',\n",
       " '',\n",
       " 'they they rushing into the lawn room so that everyone ',\n",
       " 'he he would not have heard her ',\n",
       " 'no no gasped gasped ',\n",
       " 'he he sam his bedroom colored hair all over his ',\n",
       " 'made made haste to relate to him as succinctly as ',\n",
       " 'had had had the plans for major expressways and only ',\n",
       " '',\n",
       " 'at at 10 30 a ',\n",
       " '1949 1949 the federal government enacted a new approach to ',\n",
       " 'even even he ilagin ilagin rising her too much pride ',\n",
       " 'there there everything the graveyard last night in their dreams ',\n",
       " 'perhaps perhaps sixteen miles of the through route was under ',\n",
       " 'them them just moving them out out out out ',\n",
       " 'mostly mostly would not one should the false assertions the ',\n",
       " 'they they had struggled to the bottom of the bank ',\n",
       " 'is is optimistic ',\n",
       " 'touch touch my ',\n",
       " 'magic magic of course was and and seldom kept his ',\n",
       " 'you you this wedding this over the happier st st be ',\n",
       " 'though though my nose be rather heavy though though my ',\n",
       " 'make make make make a broken delivery of the business ',\n",
       " 'sir sir take take the opportunity of stating openly to ',\n",
       " 'hermione hermione hermione sat up up up a little marble brightly ',\n",
       " 'the the little princess had grown accustomed to life at ',\n",
       " 'me me awhile with the maid my mind promises with ',\n",
       " 'the the battle of our our strength in the of ',\n",
       " '',\n",
       " 'that that is neither here nor there ',\n",
       " 'them them them picked up the bottle below others covered ',\n",
       " 'kern kern reply reply reply said this to in in ',\n",
       " 'tens tens of thousands of money used the bridge day ',\n",
       " 'looked looked merrily ',\n",
       " 'and and your turn to listen and son son ',\n",
       " 'do do not understand ',\n",
       " 'to to be almost as powerful a correspondent as ',\n",
       " 'thou thou thou thou thou thou thou pray believe me ',\n",
       " 'went went to bed and while his weary limbs were afraid ',\n",
       " 's s offer them accepted ',\n",
       " 'o o better listen more urgently devils devils these quartermasters ',\n",
       " 'gentlemen gentlemen see see that it is that that will ',\n",
       " 'the the old man came round it would be all ',\n",
       " '29 by by the door door ',\n",
       " 'was was the poor goat the agile which which had ',\n",
       " 'hurt ',\n",
       " 'he he whispered ',\n",
       " 'the the commentary near it now indelibly written in his ',\n",
       " 'there s one tomorrow evening ',\n",
       " 'old old and and your your your eye had already ',\n",
       " 'soon soon soon exclaimed us and came and where accounts ',\n",
       " 'what what must these clumsy ways of those scientificality and ',\n",
       " 'they they re so sweet ',\n",
       " 'pointing pointing to look at least least least his hero ',\n",
       " '',\n",
       " 'were were watching me work new television a welcome home ',\n",
       " 'very very instant he had uttered these words the door ',\n",
       " '503 ',\n",
       " 'and and and other newspapers throughout the state with the lakes ',\n",
       " 'people people were staring at her and a few openly ',\n",
       " 'she she lay down to sleep and any before ',\n",
       " '',\n",
       " 'the the the the one one temperature would be away ',\n",
       " 'in in that enormous illuminated theater where the bare legged ',\n",
       " 'sobbed sobbed into her shoulder ',\n",
       " 'but but but m m so sorry for him ',\n",
       " 'and and and and ',\n",
       " 'we we we we come ',\n",
       " 'two two were so indefatigable in taking out out out ',\n",
       " 'was was going to be expelled he just knew ',\n",
       " 'a a husband take her a sou said the king ',\n",
       " 'held held up a letter so they ve read the ',\n",
       " 'a a s estimates of value betray out of the ',\n",
       " 'the the had now been far into the morning and ',\n",
       " 'it it wasn t a a a or invitation invitation ',\n",
       " 'his his servant he only year time quite recovered his ',\n",
       " 'had had noticed with what dissatisfaction he turned from the ',\n",
       " 'other other other other other fortune may set upon our ',\n",
       " 'i i i he one day charley voice is crossed ',\n",
       " 'pain pain assailed him and felt felt cold ',\n",
       " 'car car was ten or perhaps three times as large ',\n",
       " 'this this one seems ',\n",
       " '',\n",
       " 'i i tell you some things about myself ',\n",
       " 'suppose suppose suppose it ll help in the long ',\n",
       " 'well well all all this groaned is dropped your grain ',\n",
       " 'therefore therefore being appeased the curate was anxious to go ',\n",
       " 'you you ',\n",
       " 'he he wanted no legal proceedings inquired my guardian ',\n",
       " '',\n",
       " 'a a memo addressed to a chart for for example ',\n",
       " 'years years post post ',\n",
       " 'sees sees them jealous when they are crossed out forestalled ',\n",
       " 'the the opposite or side side side of the park ',\n",
       " 'it it is again do you hear said he pointing ',\n",
       " 'sly sly sly impedimenta impedimenta his voice a thing he ',\n",
       " 'never never shall it be said ',\n",
       " 'sent sent on a a a word application with a ',\n",
       " 'had had said the magic words ',\n",
       " 'said said said so done is well ',\n",
       " '29 1962 ',\n",
       " 'the the my mother in my embrace and she held ',\n",
       " 'it it dead ',\n",
       " 'care care said said the duchess with a bit of ',\n",
       " 'could could see them edging awkwardly along the tunnel and ',\n",
       " 'must must must must he die ',\n",
       " 'or or or if yes or no yes or no ',\n",
       " 'early early in the morning wearing a dressing jacket she ',\n",
       " 'weally weally weally that he he harsh harsh twitched unnaturally ',\n",
       " 'at at the end of this he we will do ',\n",
       " 'you you said she she voice ',\n",
       " 'the the dim wind round the the the sharp rain ',\n",
       " 'these these these these unsatisfying vague acquainted into the destruction ',\n",
       " 'settled settled to work in the smallest bedroom which was ',\n",
       " 'then then yeh gotta find yer place is is hundreds ',\n",
       " 'with with her to prison who who shall speak for ',\n",
       " 'camilla camilla seemed embarrassed when he let me in and ',\n",
       " 'claude claude coupled with the feeling of accomplishment might well ',\n",
       " 'said said so and added that he seemed to be ',\n",
       " 'laughs and shines and plays around it like it even ',\n",
       " 'i i been shouting and found and ',\n",
       " 'shouldn shouldn shouldn t ',\n",
       " 'and and and his servants were surprised anew when they ',\n",
       " 'rest rest of the class arrived over the next five ',\n",
       " 'just just just wish to say to avoid full that ',\n",
       " '18 18 den mit dessen freiheit freiheit very eine eigene ',\n",
       " 'life life meanwhile continued as before with the same infatuations ',\n",
       " 'empty empty instinctively for the hilt of the sword and ',\n",
       " '27 ',\n",
       " 'slowly cautiously as though they could hardly believe their eyes ',\n",
       " 'they they they stay at the sir ',\n",
       " '19 1926 ',\n",
       " 'a a man rows it is not the oar that ',\n",
       " 's s what mean mean you you just dawning yourself ',\n",
       " 'again again again said her head cried ',\n",
       " 'in in front and especially from the right in the ',\n",
       " 'i i i still got the maids maids me we ',\n",
       " 'the the the reason was the news news ',\n",
       " 'we we not going to eat that are you ',\n",
       " 'she she she first noticed noticed that everyone was smiling ',\n",
       " 'a a good joke will will be can can hardly ',\n",
       " 'but that would not take her long ',\n",
       " 'my my hurrying to come to him all these what ',\n",
       " 'it it varied from of these beautiful streets were the ',\n",
       " 'with with the the field marshal expressed expressed expressed her anxious ',\n",
       " 'supposing supposing one has been walking all day and arrives ',\n",
       " 'said taking taking her her hand and shaking it ',\n",
       " 't t site ll t ',\n",
       " 'knew knew that the reality of certain circumstances which he ',\n",
       " 'said said said ',\n",
       " 'is is a wizard s duel ',\n",
       " 'this this piece of which my poor feeble tongue has ',\n",
       " 'is is a great comfort to have you so rich i ',\n",
       " 'he he understood at last words had had been listening ',\n",
       " 'the the the the state so good good good friends ',\n",
       " 'could could possibly do that to a ghost people asked ',\n",
       " 'we we got to be quick panted panted ',\n",
       " 'that that he was awake came came came bustling ',\n",
       " 'and and genius i the the victory at crickhollow crickhollow ',\n",
       " 'had had had to our birth to the child in ',\n",
       " 'will will not walk blindfold like a beggar or a ',\n",
       " 'areas areas of the maps on which the dots were ',\n",
       " 'am am not the king s physician and his majesty ',\n",
       " 'saw saw his hands tighten still harder over his massive ',\n",
       " 'on on a summer evening the melodious sky growls like ',\n",
       " 'toll toll the great of her her life every ',\n",
       " 'then then mounted and his friend the barber did the ',\n",
       " 'one one another information that was was pleading with an ',\n",
       " 's s mind he he heard noises noises at ',\n",
       " 'had had my doubts of their caring so very much ',\n",
       " '99 101 gives the law law general handling of joy joy ',\n",
       " 'the the moment the door opened one feeling alone appeared ',\n",
       " 'its its list of triborough which concentrated on the technicalities ',\n",
       " 'reckon reckon they ve ever had a teacher who lasted ',\n",
       " 'i i i can can can go just as well ',\n",
       " 'a a a a little little said to herself not ',\n",
       " 'and said said ',\n",
       " 'i i t want us till much yet when when ',\n",
       " 'officer officer of the suite ventured to remark to the ',\n",
       " 'p p p ',\n",
       " 'just just just closed the book book book book book ',\n",
       " 'well well ',\n",
       " 'father father put every security measure known to kind kind ',\n",
       " 's s me s been putting the nifflers in her ',\n",
       " 'with with him because is is arrested by and and ',\n",
       " 'next next as if would would immediately to on the ',\n",
       " 'hurrah hurrah rou yelled the crowd echoing the crash of ',\n",
       " 'trelawney trelawney ',\n",
       " 'for for the painter he was overjoyed at the prospect ',\n",
       " 'full full of the brightest visions of the future which ',\n",
       " 'it it in this way the the the king of the ',\n",
       " 'though though of of this class are are afraid afraid ',\n",
       " 'going going to look in before the match he added ',\n",
       " 'show show get before before before back with obvious satisfaction ',\n",
       " 'who who was in his seventh and final year at ',\n",
       " 'and and and who who would believe this have have ',\n",
       " 'gave gave gave the a a secret account not only ',\n",
       " 'bridge bridge was was jammed and the traffic jams them ',\n",
       " 'should should go go looking for three feet on a ',\n",
       " 'all all conversation was thereby at an end asked asked inquired ',\n",
       " 'of of the question no longer served any useful purpose ',\n",
       " 'casual casual like looking back over their shoulders at the ',\n",
       " 'had had on a new new uniform showing the creases ',\n",
       " 'these these men were still knew ',\n",
       " 'suggested suggested that he must have made or been planning ',\n",
       " 'even even that did not quite express his sense of ',\n",
       " 'as as reports reports moses ',\n",
       " 'he he won t do it with a good ',\n",
       " '1 1954 ',\n",
       " 'he he thing cannot show be what about mathematics for ',\n",
       " 'with with your hand what means might not not find ',\n",
       " 'round round earth wants to talk to you this tale ',\n",
       " 'city city s liberals observing such favors and inferring from ',\n",
       " 'hence hence all his papers and all his effects ',\n",
       " 'bilbo bilbo that urgent been in and and and to ',\n",
       " 'who who was now helping himself to vault vault toast ',\n",
       " 'destroyed destroyed destroyed reputation reputation was because he had to say ',\n",
       " 'said said timokhin timokhin disappointment sight left him in ',\n",
       " 'giving giving the and and and the price of the ',\n",
       " 'in in the island of years years have been found ',\n",
       " 'placed placed the point of the garden on the paper ',\n",
       " 'fine fine fine mother instantly else her ',\n",
       " 'young young witch with short blonde hair poked her head ',\n",
       " 'the the the the aide closest to authorities authorities authorities ',\n",
       " 'and and took took to the dance floor next ',\n",
       " 'saw saw and and and distinctly distinctly awkwardly out of the ',\n",
       " 'my my my my my lord m ',\n",
       " 'very very near and yet far near it seems ',\n",
       " 'sir sir such as the place is in long long ',\n",
       " 'were were afraid of opinions passions and when there was ',\n",
       " 'has has has had informed me that you have been ',\n",
       " 'he s very good ',\n",
       " 'given given by fours with the ribbon insignia of their ',\n",
       " 'to to order to to share on pain of his ',\n",
       " 'summoning summoning the point of the hat and pulled it ',\n",
       " 'he he call round that that that ',\n",
       " 'i i i off to bed i i not so i ',\n",
       " 'sounded sounded odd ',\n",
       " 'my my dear father she cried come back and write ',\n",
       " 'was was was too true an artist considered spoil his ',\n",
       " 'possibly possibly possibly you they d have about about the ',\n",
       " 'the the stern officer of of of the the and ',\n",
       " 'and and and decided decided that now they were ',\n",
       " 'highly highly accomplished its purpose ',\n",
       " 'loved loved her feelings almost as dearly ',\n",
       " 'master master sent me word of my father s escape ',\n",
       " 'nothing nothing that that ',\n",
       " 'doge doge was not confined to the press the ',\n",
       " 'dementors dementors didn t affect me at all right right ',\n",
       " 'are are queer folk about ',\n",
       " 'did did not say that how will you ever hear ',\n",
       " 'are are you thinking of madame madame your pardon said ',\n",
       " 'this this is always the way with with with with what ',\n",
       " 'it it happened didn didn t mind know how to do ',\n",
       " 'a a new york york was was was appointed for mr ',\n",
       " 'had had been swimming on elves elves bed you you you ',\n",
       " 'in in in just just they reached some rum in ',\n",
       " 'the the sergeant taking one of the men by the ',\n",
       " 'moment moment it made contact with its surface the potion ',\n",
       " 'malfoy malfoy brandishing his wand ',\n",
       " 'to to morrow repeated ',\n",
       " 'sir sir of course said quickly quickly ',\n",
       " 'things things can can command to extent extent reveal she ',\n",
       " 'opening opening of a bridge was the the was its ',\n",
       " 'george george and grandfather had words with him of a ',\n",
       " 'you you asked asked professor professor the snake s snout ',\n",
       " 'are are they me ',\n",
       " 'at at length perceiving what he meant ',\n",
       " 'complete complete cannon the team managed to make its way ',\n",
       " '23 23 23 attention attention himself because because of only ',\n",
       " 'the the time the story was being told she sat ',\n",
       " 'woman woman ',\n",
       " 'cannot cannot endure their restraint and will walk alone in ',\n",
       " 'got got up pulled to to bat feet and they looked ',\n",
       " 'asked asked him why ',\n",
       " 'hearing hearing after a late breakfast the first was sitting ',\n",
       " 'drawn drawn down off ron ron bunk and threw them over ',\n",
       " 'knew knew that the phrase was going to speak to ',\n",
       " 'i i you okay ',\n",
       " 'her her at it it was bad enough that he ',\n",
       " 'not not only did not renounce them subsequently but when ',\n",
       " 'had had been more than a year since ',\n",
       " 'third third lay prone so that his face was not ',\n",
       " 'have have seen by a thousand signs that this master ',\n",
       " 's s a bit odd said for for for some ',\n",
       " 'it it much later when reports on the battle of ',\n",
       " 'the the civil inquiries this this poured in and amongst ',\n",
       " '61 note note reascended in silence to his retreat and ',\n",
       " 'amiable amiable amiable sympathy sells them ',\n",
       " '',\n",
       " 'it it a cry of outrage visible from that segment ',\n",
       " 'what what most of all made me my my hand ',\n",
       " 'seeing seeing seeing him march with with such a and ',\n",
       " 'he he returned he was not reassuring ',\n",
       " 'so so felt slightly sick ',\n",
       " 'it it simply a simply lasted might those small creatures ',\n",
       " 'that that poor monarch taught thee to cure measure measure ',\n",
       " 'i i i i get in her repeated aloud ',\n",
       " 'merope merope spread in the that that the vision vision vision ',\n",
       " 'all all ran forward those of us who were still ',\n",
       " 'a a paper paper ',\n",
       " 'these these words and a laugh and laying a bitter ',\n",
       " 'can can read transport home afterward ',\n",
       " 'one one was very popular once too ',\n",
       " 'am am fond of boating ',\n",
       " 'know know if know know know alive patrician family ',\n",
       " 'had had had fallen to fred knees as though ',\n",
       " 'shall shall shall always say he are my daughter extremely ',\n",
       " 'were were happy then recalls ',\n",
       " 'apart apart from the was was was supposed to know ',\n",
       " 'said said nothing to any rate me ',\n",
       " 'us us thus this armour hung up on our tree ',\n",
       " 'on on the reverse side was a scribbled note from ',\n",
       " 'was was staying at her her her father father in ',\n",
       " 'could could also see the half healed puncture marks to ',\n",
       " 'it it it is said gandalf gandalf ',\n",
       " '45 cars cars would would made ourselves ridiculous at length length ',\n",
       " 'at at once a hundred torches the light of which ',\n",
       " 'princess princess but nothing but suddenly her short downy lip ',\n",
       " 'angry angry angry angry move move laughter laughter and ',\n",
       " '',\n",
       " '',\n",
       " 'and and ',\n",
       " 'rose rose and shook a a hand and on the ',\n",
       " 'two two ways many complaints and several tantrums one involving ',\n",
       " 'for for the easiest of those monster roads those traversing ',\n",
       " 'kissed kissed my hand and looks very calm ',\n",
       " 'o o from hold hold those gates and triumphs thy thy ',\n",
       " 'so so and if that be thy pleasure ',\n",
       " 'their their heads off shouted the ',\n",
       " 'the the latest room the farthest of judges sitting on ',\n",
       " 'you you said desperately desperately turning to ',\n",
       " 'she she not weird hermione all it s perfect sense ',\n",
       " 'forget it grunted grunted grunted ',\n",
       " 'they they stood stood up and blinking stupidly ',\n",
       " 'can can can that be all over at at to ',\n",
       " 'that that normal for a wizard that that tell that ',\n",
       " 'said said said mr mr ',\n",
       " 's s nothing seriously wrong with him said bending bending ',\n",
       " 'they they they retired farther and to to the the ',\n",
       " 'of of of what s s been telling you ',\n",
       " 'it it seemed that to the the fact that their ',\n",
       " 'never never meant it to happen ',\n",
       " 'the the reformers were finished with all their hollering and ',\n",
       " 'thought thought thought it best to encourage him in ',\n",
       " 'he he came level with the he he he noticed ',\n",
       " 'can can see to do their amorous rites their their ',\n",
       " 'with with a strong tendency in his clump of hair ',\n",
       " 'about about the average height broad with huge red fingers ',\n",
       " 'worked worked there there was work to be done that ',\n",
       " 'harry harry long been expecting a proposal from her melancholy ',\n",
       " 'it it s ',\n",
       " 'must must be a very pretty dance said timidly timidly ',\n",
       " 'merely merely moves her head in reply ',\n",
       " 'what what do you think dear friend years years have ',\n",
       " 'is is almost always a symptom of what is lacking ',\n",
       " 'well well little woman take take on my dear ',\n",
       " 'day day we you all ',\n",
       " 'know know there wasn t one hell of a lot ',\n",
       " '',\n",
       " 'around around around ',\n",
       " 'was was a thing like a whip crack ',\n",
       " 'day day which followed the night of my misfortune did ',\n",
       " 'i i sure shall shall break said said ',\n",
       " 'the the greater right on on on my the my ',\n",
       " 'a a a rumor going around that got got up ',\n",
       " 'voices voices battered him sobs and shouts and wails stabbed ',\n",
       " 'sire sire sire sire sire we will not time of ',\n",
       " 'i i i the planning that said ',\n",
       " '261 261 of ',\n",
       " 'next next moment and and shot shot away like two ',\n",
       " 'ht ht s trick p p ',\n",
       " 'he he he put out about something ',\n",
       " 'weinberg weinberg four sandwiches inside ',\n",
       " 'he he is ',\n",
       " 'thought thought thought i were looking after the door door door ',\n",
       " 's s writing writing to me for the the to ',\n",
       " 's s good you re being separated from normal people ',\n",
       " 'you you the same as we did did did ',\n",
       " '',\n",
       " 'at at length by s s sake he was speaking ',\n",
       " 'he he saw an old wizard wizard who with untidy ',\n",
       " '',\n",
       " 'you you ll follow me ',\n",
       " '26 1934 ',\n",
       " 'floor floor of the stage of of smooth boards at ',\n",
       " 'it it said he stoically ',\n",
       " 'counted counted the vacant feverishly adding a few seconds to ',\n",
       " 'vholes vholes they quasimodo instant whether to speak or not ',\n",
       " 'understand understand understand nothing it had stopped has lately sunk ',\n",
       " 'and and its its dropped so remained in the corridors ',\n",
       " 'didn didn t feel like sharing it with anyone else ',\n",
       " 'seriously seriously ill mind not ill enough to get you ',\n",
       " 'she she was la guardia guardia ',\n",
       " 'that that for for for it it it it ',\n",
       " 'but but in her usual manner which that this day ',\n",
       " 'of of course was was out out get out ',\n",
       " 'her her husband publicly insulted it it it was ',\n",
       " 'pudding pudding fell to the floor with a heart stopping ',\n",
       " 'he he arriving at it it it it was written ']"
      ]
     },
     "execution_count": 57,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_train = train.sample(1000)\n",
    "result = []\n",
    "i=0\n",
    "for i, j , k in tqdm(sample_train.values):\n",
    "    # print(i , j , k)\n",
    "    pred = inference(i , j)\n",
    "    result.append(pred)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 907
    },
    "id": "uMFeNbysHjWI",
    "outputId": "8711c005-8d19-4b61-ea68-cf093d5f1919"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ERRONEOUS_SENTENCE</th>\n",
       "      <th>english_inp</th>\n",
       "      <th>english_out</th>\n",
       "      <th>PREDICTED_SENTENCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37624</th>\n",
       "      <td>But on the following morning every hope of thi...</td>\n",
       "      <td>&lt;start&gt; But on the following morning every hop...</td>\n",
       "      <td>But on the following morning every hope of thi...</td>\n",
       "      <td>on on the following morning every hope of this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129898</th>\n",
       "      <td>In any case I m going to call you by</td>\n",
       "      <td>&lt;start&gt; In any case I m going to call you by</td>\n",
       "      <td>In any case I m going to call you by &lt;end&gt;</td>\n",
       "      <td>any any case i i going to call you by</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165511</th>\n",
       "      <td>The treth is that the Senator is back on the</td>\n",
       "      <td>&lt;start&gt; The truth is that the Senator is back ...</td>\n",
       "      <td>The truth is that the Senator is back on the &lt;...</td>\n",
       "      <td>that that that that the sand sand back on the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10084</th>\n",
       "      <td>came and oaking my arm led me oo a building</td>\n",
       "      <td>&lt;start&gt; came and taking my arm led me to a bui...</td>\n",
       "      <td>came and taking my arm led me to a building &lt;end&gt;</td>\n",
       "      <td>came and taking my arm led me to a building</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108058</th>\n",
       "      <td>Lord Voldemort has never a friend nor do I bel...</td>\n",
       "      <td>&lt;start&gt; Lord Voldemort has never had a friend ...</td>\n",
       "      <td>Lord Voldemort has never had a friend nor do I...</td>\n",
       "      <td>has has has never a friend friend nor do</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193868</th>\n",
       "      <td>His actors spurred on by him had had not ceased</td>\n",
       "      <td>&lt;start&gt; His actors spurred on by him had not c...</td>\n",
       "      <td>His actors spurred on by him had not ceased to...</td>\n",
       "      <td>forcheville forcheville introduced on by his h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121821</th>\n",
       "      <td>Behind him Ron Ron said He s having a laugh</td>\n",
       "      <td>&lt;start&gt; Behind him Ron said He s having a laugh</td>\n",
       "      <td>Behind him Ron said He s having a laugh &lt;end&gt;</td>\n",
       "      <td>kuragin kuragin said said s s having a laugh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80480</th>\n",
       "      <td>Really romantic date Hagrid said Charlie shaki...</td>\n",
       "      <td>&lt;start&gt; Really romantic date Hagrid said Charl...</td>\n",
       "      <td>Really romantic date Hagrid said Charlie shaki...</td>\n",
       "      <td>four four date said said lifting lifting his h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80442</th>\n",
       "      <td>she wasn t exactly hard to miss</td>\n",
       "      <td>&lt;start&gt; she wasn t exactly hard to miss</td>\n",
       "      <td>she wasn t exactly hard to miss &lt;end&gt;</td>\n",
       "      <td>she wasn t exactly hard to miss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58488</th>\n",
       "      <td>The boy s a a natural</td>\n",
       "      <td>&lt;start&gt; The boy s a natural</td>\n",
       "      <td>The boy s a natural &lt;end&gt;</td>\n",
       "      <td>dear dear s a natural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178181</th>\n",
       "      <td>r</td>\n",
       "      <td>&lt;start&gt; r</td>\n",
       "      <td>r &lt;end&gt;</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41718</th>\n",
       "      <td>The question is how far an opinion is life fur...</td>\n",
       "      <td>&lt;start&gt; The question is how far an opinion is ...</td>\n",
       "      <td>The question is how far an opinion is life fur...</td>\n",
       "      <td>sir sir is how much this opinion is life furth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190960</th>\n",
       "      <td>RM asks depuoy mayor RM oo Corcoran Nov</td>\n",
       "      <td>&lt;start&gt; RM asks deputy mayor RM to Corcoran Nov</td>\n",
       "      <td>RM asks deputy mayor RM to Corcoran Nov &lt;end&gt;</td>\n",
       "      <td>paulina paulina mayor mayor to to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85435</th>\n",
       "      <td>He had inherited his father s hooked nose</td>\n",
       "      <td>&lt;start&gt; He had inherited his father s hooked nose</td>\n",
       "      <td>He had inherited his father s hooked nose &lt;end&gt;</td>\n",
       "      <td>had had one his father s hooked nose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174757</th>\n",
       "      <td>So many organizations wanted to dinners in his...</td>\n",
       "      <td>&lt;start&gt; So many organizations wanted to hold d...</td>\n",
       "      <td>So many organizations wanted to hold dinners i...</td>\n",
       "      <td>many many organizations wanted to obtain dinne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89631</th>\n",
       "      <td>It was not Mrs</td>\n",
       "      <td>&lt;start&gt; It was not Mrs</td>\n",
       "      <td>It was not Mrs &lt;end&gt;</td>\n",
       "      <td>was was not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25900</th>\n",
       "      <td>Uncle forgive me I did that</td>\n",
       "      <td>&lt;start&gt; Uncle forgive me I did that</td>\n",
       "      <td>Uncle forgive me I did that &lt;end&gt;</td>\n",
       "      <td>forgive forgive me that that that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71842</th>\n",
       "      <td>AAARGH he yelled pointing at Harry s head</td>\n",
       "      <td>&lt;start&gt; AAARGH he yelled pointing at Harry s head</td>\n",
       "      <td>AAARGH he yelled pointing at Harry s head &lt;end&gt;</td>\n",
       "      <td>he he yelled pointing at a a head</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69660</th>\n",
       "      <td>As you re all in my House you should hand</td>\n",
       "      <td>&lt;start&gt; As you re all in my House you should hand</td>\n",
       "      <td>As you re all in my House you should hand &lt;end&gt;</td>\n",
       "      <td>you you re all in my means means should hand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176392</th>\n",
       "      <td>In the upper part ff Manhattan the masses ff c...</td>\n",
       "      <td>&lt;start&gt; In the upper part of Manhattan the mas...</td>\n",
       "      <td>In the upper part of Manhattan the masses of c...</td>\n",
       "      <td>the the upper part of the the masses of concrete</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       ERRONEOUS_SENTENCE  ...                                 PREDICTED_SENTENCE\n",
       "37624   But on the following morning every hope of thi...  ...  on on the following morning every hope of this...\n",
       "129898               In any case I m going to call you by  ...             any any case i i going to call you by \n",
       "165511       The treth is that the Senator is back on the  ...     that that that that the sand sand back on the \n",
       "10084         came and oaking my arm led me oo a building  ...       came and taking my arm led me to a building \n",
       "108058  Lord Voldemort has never a friend nor do I bel...  ...          has has has never a friend friend nor do \n",
       "193868    His actors spurred on by him had had not ceased  ...  forcheville forcheville introduced on by his h...\n",
       "121821        Behind him Ron Ron said He s having a laugh  ...      kuragin kuragin said said s s having a laugh \n",
       "80480   Really romantic date Hagrid said Charlie shaki...  ...  four four date said said lifting lifting his h...\n",
       "80442                     she wasn t exactly hard to miss  ...                   she wasn t exactly hard to miss \n",
       "58488                               The boy s a a natural  ...                             dear dear s a natural \n",
       "178181                                                  r  ...                                                 r \n",
       "41718   The question is how far an opinion is life fur...  ...  sir sir is how much this opinion is life furth...\n",
       "190960            RM asks depuoy mayor RM oo Corcoran Nov  ...                 paulina paulina mayor mayor to to \n",
       "85435           He had inherited his father s hooked nose  ...              had had one his father s hooked nose \n",
       "174757  So many organizations wanted to dinners in his...  ...  many many organizations wanted to obtain dinne...\n",
       "89631                                      It was not Mrs  ...                                       was was not \n",
       "25900                         Uncle forgive me I did that  ...                 forgive forgive me that that that \n",
       "71842           AAARGH he yelled pointing at Harry s head  ...                 he he yelled pointing at a a head \n",
       "69660           As you re all in my House you should hand  ...      you you re all in my means means should hand \n",
       "176392  In the upper part ff Manhattan the masses ff c...  ...  the the upper part of the the masses of concrete \n",
       "\n",
       "[20 rows x 4 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_train['PREDICTED_SENTENCE']= result\n",
    "sample_train.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Gk2shbPIceM"
   },
   "source": [
    "## BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01z2NYvpJFCO"
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KBE1SSvSJdxb",
    "outputId": "7a79ab92-5bb7-4084-a2cd-d3c0c9d29dad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my greatgreat grandfather see Least vovular headmaster Hogwarts ever had\n",
      "my greatgreat grandfather see charmingly charmingly headmaster ever ever had \n",
      "BLEU score: 0.6643548861507491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "inp = sample_train.ERRONEOUS_SENTENCE.values[20]\n",
    "translate = sample_train.PREDICTED_SENTENCE.values[20]\n",
    "print(inp)\n",
    "print(translate)\n",
    "print('BLEU score: {}'.format(nltk.translate.bleu_score.sentence_bleu(inp, translate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-w_sHG_HIbNt",
    "outputId": "b32856f2-9095-49d1-fa3c-fe9c81be1a88"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]\u001b[A\u001b[A/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "\n",
      "\n",
      "  3%|▎         | 34/1000 [00:00<00:02, 334.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▋         | 65/1000 [00:00<00:02, 326.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 98/1000 [00:00<00:02, 326.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 131/1000 [00:00<00:02, 322.14it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 161/1000 [00:00<00:02, 313.26it/s]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▉        | 190/1000 [00:00<00:02, 305.89it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 220/1000 [00:00<00:02, 303.28it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▍       | 248/1000 [00:00<00:02, 290.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 277/1000 [00:00<00:02, 288.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███       | 309/1000 [00:01<00:02, 296.62it/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▍      | 339/1000 [00:01<00:02, 296.12it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 369/1000 [00:01<00:02, 283.15it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 400/1000 [00:01<00:02, 288.41it/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▍     | 438/1000 [00:01<00:01, 308.56it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 471/1000 [00:01<00:01, 314.16it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 503/1000 [00:01<00:01, 297.32it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 534/1000 [00:01<00:01, 284.27it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 566/1000 [00:01<00:01, 292.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|█████▉    | 596/1000 [00:02<00:01, 280.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 627/1000 [00:02<00:01, 287.26it/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▌   | 657/1000 [00:02<00:01, 290.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████▉   | 691/1000 [00:02<00:01, 301.56it/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▏  | 722/1000 [00:02<00:00, 293.30it/s]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▌  | 752/1000 [00:02<00:00, 292.82it/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|███████▊  | 782/1000 [00:02<00:00, 284.95it/s]\u001b[A\u001b[A\n",
      "\n",
      " 81%|████████  | 811/1000 [00:02<00:00, 273.30it/s]\u001b[A\u001b[A\n",
      "\n",
      " 84%|████████▍ | 840/1000 [00:02<00:00, 277.25it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████▋ | 871/1000 [00:02<00:00, 284.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████ | 903/1000 [00:03<00:00, 292.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████▎| 933/1000 [00:03<00:00, 292.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 96%|█████████▋| 963/1000 [00:03<00:00, 293.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 1000/1000 [00:03<00:00, 295.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average BLEU score of these sentences. are : 0.7350925622799785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk.translate.bleu_score as bleu\n",
    "\n",
    "score = 0\n",
    "for i in tqdm(range(1000)):\n",
    "    inp = sample_train.ERRONEOUS_SENTENCE.values[i]\n",
    "\n",
    "    # print(inp)\n",
    "    translate = sample_train.PREDICTED_SENTENCE.values[i]\n",
    "    # print(translate)\n",
    "    bleu = nltk.translate.bleu_score.sentence_bleu(inp, translate)\n",
    "    score = score + bleu\n",
    "average_bleu = score / 1000\n",
    "print('the average BLEU score of these sentences. are :' , average_bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AwPLy29_IbJ7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "immediate-opinion"
   },
   "source": [
    "# ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BcdUmYYRBbWt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "KqLB3tQ1dxAe",
    "outputId": "e6fbfc59-77b3-4dc1-b05c-671d073839da"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all imports\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk import pos_tag \n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Conv2D, Flatten , Input , Conv1D , Concatenate , MaxPooling1D , Dropout , Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM\n",
    "import datetime\n",
    "\n",
    "from keras.layers import Concatenate\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.layers import Embedding\n",
    "from sklearn.metrics import  f1_score , roc_auc_score\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import nltk.translate.bleu_score as bleu\n",
    "\n",
    "\n",
    "tf.__version__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PU4KIsGxLOfK"
   },
   "source": [
    "### <font color='blue'>**Implement custom encoder decoder and attention layers**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMm3ADQDLOfK"
   },
   "source": [
    "<font color='blue'>**Encoder**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Lx_5NA24KzRp"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,inp_vocab_size,embedding_size,lstm_size,input_length):\n",
    "        super().__init__()\n",
    "        self.inp_vocab_size = inp_vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.input_length = input_length\n",
    "        self.lstm_size= lstm_size\n",
    "        self.lstm_output = 0\n",
    "        self.lstm_state_h=0\n",
    "        self.lstm_state_c=0\n",
    "        \n",
    "        self.embedding = Embedding(input_dim=self.inp_vocab_size, output_dim=self.embedding_size, input_length=self.input_length,\n",
    "                           mask_zero=True, name=\"embedding_layer_encoder\")\n",
    "        self.lstm = LSTM(self.lstm_size, return_state=True, return_sequences=True, name=\"Encoder_LSTM\")\n",
    "\n",
    "\n",
    "    def call(self,input_sequence,states):\n",
    "        input_embedd = self.embedding(input_sequence)\n",
    "        self.lstm_output, self.lstm_state_h,self.lstm_state_c = self.lstm(input_embedd, states)\n",
    "        return self.lstm_output, self.lstm_state_h,self.lstm_state_c\n",
    "    \n",
    "    def initialize_states(self,batch_size):\n",
    "      self.lstm_state_h = tf.zeros([batch_size , self.lstm_size])\n",
    "      self.lstm_state_c = tf.zeros([batch_size , self.lstm_size])\n",
    "      return self.lstm_state_h,self.lstm_state_c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lXn278lhLYRM"
   },
   "source": [
    "<font color='blue'>**Attention**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ab5SNdPZLlur"
   },
   "outputs": [],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self,scoring_function, att_units):\n",
    "    super(Attention, self).__init__()\n",
    "    self.scoring_function = scoring_function\n",
    "    self.att_units = att_units\n",
    "    self.W1 = tf.keras.layers.Dense(att_units)\n",
    "    self.W2 = tf.keras.layers.Dense(att_units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  \n",
    "  def call(self,decoder_hidden_state,encoder_output):\n",
    "    \n",
    "    if self.scoring_function == 'dot':\n",
    "        decoder_hidden_state_reshaped = tf.reshape(decoder_hidden_state , (decoder_hidden_state.shape[0],decoder_hidden_state.shape[1],1))\n",
    "\n",
    "        #I WAS USING tf.keras.layers.Dot FOR DOT PRODUCT , BUT IT GAVE INCOMPATIBILITY IN SHAPES , SO NOW I VE USED tf.keras.layers.dot\n",
    "        score =  tf.keras.layers.dot([ encoder_output , decoder_hidden_state_reshaped] , [2,1]) \n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vector = attention_weights * encoder_output  #\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "        pass\n",
    "    elif self.scoring_function == 'general':\n",
    "        decoder_hidden_state_reshaped = tf.reshape(decoder_hidden_state , (decoder_hidden_state.shape[0],decoder_hidden_state.shape[1],1))\n",
    "        W = tf.random.uniform(shape=[encoder_output.shape[0] , self.att_units , self.att_units])\n",
    "        score =  tf.keras.layers.dot([ encoder_output , W] , [2,1]) \n",
    "        score =  tf.keras.layers.dot([ score , decoder_hidden_state_reshaped] , [2,1]) \n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vector = attention_weights * encoder_output  #\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "        pass\n",
    "    elif self.scoring_function == 'concat':\n",
    "\n",
    "        decoder_hidden_state_reshaped = tf.expand_dims(decoder_hidden_state, 1)\n",
    "        score =  self.V(tf.nn.tanh(self.W1(decoder_hidden_state_reshaped) + self.W2(encoder_output)) )\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * encoder_output  #\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "        pass\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ic-FNEbfL2DN"
   },
   "source": [
    "<font color='blue'>**OneStepDecoder**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Kc8m7lmOL097"
   },
   "outputs": [],
   "source": [
    "class One_Step_Decoder(tf.keras.Model):\n",
    "    def __init__(self,tar_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
    "        super(One_Step_Decoder, self).__init__()\n",
    "\n",
    "        # Initialize decoder embedding layer, LSTM \n",
    "        self.tar_vocab_size = tar_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.input_length = input_length\n",
    "        self.dec_units = dec_units\n",
    "        self.score_fun = score_fun\n",
    "        self.att_units = att_units\n",
    "\n",
    "        self.attention=Attention(score_fun,att_units)\n",
    "        self.embedding = tf.keras.layers.Embedding(tar_vocab_size, embedding_dim)\n",
    "        self.lstm = LSTM(self.dec_units , return_state=True, return_sequences=True, name=\"Decoder_LSTM\")\n",
    "        self.dense = tf.keras.layers.Dense(self.tar_vocab_size)\n",
    "\n",
    "    def call(self,input_to_decoder, encoder_output, state_h,state_c):\n",
    "        output = self.embedding(input_to_decoder) # (32, 1, 12)\n",
    "        context_vector,attention_weights=self.attention(state_h,encoder_output)\n",
    "        concat = tf.concat([tf.expand_dims(context_vector, 1), output], axis=-1)\n",
    "        lstm_output, state_h, state_c = self.lstm(concat)\n",
    "        \n",
    "        output = self.dense(lstm_output)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        return output,state_h,state_c,attention_weights,context_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FHrurjUMGAi"
   },
   "source": [
    "<font color='blue'>**Decoder**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "XwZ2t4tncnUr"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,out_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
    "        super(Decoder , self).__init__()\n",
    "        self.out_vocab_size = out_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.input_length = input_length\n",
    "        self.dec_units = dec_units\n",
    "        self.score_fun = score_fun\n",
    "        self.att_units = att_units\n",
    "\n",
    "        self.onestep_decoder=One_Step_Decoder(out_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units)\n",
    "\n",
    "    def call(self, input_to_decoder,encoder_output,decoder_hidden_state,decoder_cell_state ):\n",
    "\n",
    "        all_outputs = tf.TensorArray(tf.float32, size=tf.shape(input_to_decoder)[1])\n",
    "        for timestep in range(tf.shape(input_to_decoder)[1]):\n",
    "            output,state_h,state_c,attention_weights,context_vector = self.onestep_decoder(input_to_decoder[: , timestep : timestep + 1] , \\\n",
    "                                                                                           encoder_output , decoder_hidden_state , decoder_cell_state)\n",
    "\n",
    "\n",
    "            all_outputs = all_outputs.write(timestep , output)\n",
    "        all_outputs = tf.transpose(all_outputs.stack() , [1,0,2])\n",
    "        return all_outputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V3KY8oCFYD51",
    "outputId": "fdc41181-2442-4ec7-939a-88f0aa7d4515"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 10, 13)\n"
     ]
    }
   ],
   "source": [
    "out_vocab_size=13 \n",
    "embedding_dim=12 \n",
    "input_length=10\n",
    "dec_units=16 \n",
    "att_units=16\n",
    "batch_size=32\n",
    "\n",
    "target_sentences=tf.random.uniform(shape=(batch_size,input_length),maxval=10,minval=0,dtype=tf.int32)\n",
    "encoder_output=tf.random.uniform(shape=[batch_size,input_length,dec_units])\n",
    "state_h=tf.random.uniform(shape=[batch_size,dec_units])\n",
    "state_c=tf.random.uniform(shape=[batch_size,dec_units])\n",
    "score_fun = 'concat'\n",
    "decoder=Decoder(out_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units)\n",
    "output=decoder(target_sentences,encoder_output, state_h, state_c)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fC1T1EOoMTqC"
   },
   "source": [
    "<font color='blue'>**Encoder Decoder model**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "QnPvt-oBLNJT"
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, data, tknizer_ERRONEOUS_SENTENCE, tknizer_CORRECT_SENTENCE, max_len):\n",
    "        self.encoder_inps = data['ERRONEOUS_SENTENCE'].values\n",
    "        self.decoder_inps = data['english_inp'].values\n",
    "        self.decoder_outs = data['english_out'].values\n",
    "        self.tknizer_CORRECT_SENTENCE = tknizer_CORRECT_SENTENCE\n",
    "        self.tknizer_ERRONEOUS_SENTENCE = tknizer_ERRONEOUS_SENTENCE\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        self.encoder_seq = self.tknizer_ERRONEOUS_SENTENCE.texts_to_sequences([self.encoder_inps[i]]) # need to pass list of values\n",
    "        self.decoder_inp_seq = self.tknizer_CORRECT_SENTENCE.texts_to_sequences([self.decoder_inps[i]])\n",
    "        self.decoder_out_seq = self.tknizer_CORRECT_SENTENCE.texts_to_sequences([self.decoder_outs[i]])\n",
    "\n",
    "        self.encoder_seq = pad_sequences(self.encoder_seq, maxlen=self.max_len, dtype='int32', padding='post')\n",
    "        self.decoder_inp_seq = pad_sequences(self.decoder_inp_seq, maxlen=self.max_len, dtype='int32', padding='post')\n",
    "        self.decoder_out_seq = pad_sequences(self.decoder_out_seq, maxlen=self.max_len, dtype='int32', padding='post')\n",
    "        return self.encoder_seq, self.decoder_inp_seq, self.decoder_out_seq\n",
    "\n",
    "    def __len__(self): # your model.fit_gen requires this function\n",
    "        return len(self.encoder_inps)\n",
    "\n",
    "\n",
    "class Dataloder(tf.keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, dataset, batch_size=1):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.indexes = np.arange(len(self.dataset.encoder_inps))\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        start = i * self.batch_size\n",
    "        stop = (i + 1) * self.batch_size\n",
    "        data = []\n",
    "        for j in range(start, stop):\n",
    "            data.append(self.dataset[j])\n",
    "\n",
    "        batch = [np.squeeze(np.stack(samples, axis=1), axis=0) for samples in zip(*data)]\n",
    "        \n",
    "        # we are creating data like ([italian, english_inp], english_out) these are already converted into seq\n",
    "        \n",
    "        return [batch[0],batch[1]],batch[2]\n",
    "\n",
    "    def __len__(self):  # your model.fit_gen requires this function\n",
    "        return len(self.indexes) // self.batch_size\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.random.permutation(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8tAlrQSNLNJV",
    "outputId": "faba10fe-d331-4366-a127-da33bfe5d6dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 20) (512, 20) (512, 20)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset(train, tknizer_ERRONEOUS_SENTENCE, tknizer_CORRECT_SENTENCE, 16)\n",
    "test_dataset  = Dataset(validation, tknizer_ERRONEOUS_SENTENCE, tknizer_CORRECT_SENTENCE, 16)\n",
    "\n",
    "train_dataset = Dataset(train, tknizer_ERRONEOUS_SENTENCE, tknizer_CORRECT_SENTENCE, 20)\n",
    "test_dataset  = Dataset(validation, tknizer_ERRONEOUS_SENTENCE, tknizer_CORRECT_SENTENCE, 20)\n",
    "\n",
    "train_dataloader = Dataloder(train_dataset, batch_size=512)\n",
    "test_dataloader = Dataloder(test_dataset, batch_size=512)\n",
    "\n",
    "print(train_dataloader[0][0][0].shape, train_dataloader[0][0][1].shape, train_dataloader[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "FfqBIe20MT3D"
   },
   "outputs": [],
   "source": [
    "class encoder_decoder(tf.keras.Model):\n",
    "    def __init__(self,score_fun , encoder_inputs_length,decoder_inputs_length, output_vocab_size):\n",
    "        super().__init__()\n",
    "        #encoder decoder\n",
    "\n",
    "        self.score_fun = score_fun\n",
    "\n",
    "        self.encoder=Encoder(inp_vocab_size = vocab_size_ERRONEOUS_SENTENCE+1,embedding_size = 50,lstm_size = 64,input_length = encoder_inputs_length)\n",
    "        self.decoder=Decoder(out_vocab_size = vocab_size_CORRECT_SENTENCE+1, embedding_dim = 100, input_length = decoder_inputs_length, dec_units =  64 \\\n",
    "                             ,score_fun = self.score_fun ,att_units = 64)\n",
    "\n",
    "    def call(self,data):\n",
    "\n",
    "        input,output = data[0], data[1]\n",
    "        initial_state= self.encoder.initialize_states(batch_size)\n",
    "        encoder_output, encoder_h, encoder_c = self.encoder(input , initial_state)\n",
    "\n",
    "        decoder_output= self.decoder(output,encoder_output, encoder_h, encoder_c)\n",
    "\n",
    "        return decoder_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TV5Vr3yiNEpC",
    "outputId": "90a447b0-2057-4a71-fa98-1e9d46f0c6d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "157/157 [==============================] - 88s 532ms/step - loss: 5.3506 - val_loss: 4.1200\n",
      "Epoch 2/150\n",
      "157/157 [==============================] - 81s 519ms/step - loss: 4.0746 - val_loss: 3.9506\n",
      "Epoch 3/150\n",
      "157/157 [==============================] - 81s 519ms/step - loss: 3.8922 - val_loss: 3.9636\n",
      "Epoch 4/150\n",
      "157/157 [==============================] - 82s 519ms/step - loss: 3.8116 - val_loss: 3.9484\n",
      "Epoch 5/150\n",
      "157/157 [==============================] - 82s 519ms/step - loss: 3.9153 - val_loss: 3.8861\n",
      "Epoch 6/150\n",
      "157/157 [==============================] - 81s 518ms/step - loss: 3.7552 - val_loss: 3.8725\n",
      "Epoch 7/150\n",
      "157/157 [==============================] - 81s 516ms/step - loss: 3.7337 - val_loss: 3.8188\n",
      "Epoch 8/150\n",
      "157/157 [==============================] - 81s 517ms/step - loss: 3.7929 - val_loss: 3.8618\n",
      "Epoch 9/150\n",
      "157/157 [==============================] - 81s 517ms/step - loss: 3.7123 - val_loss: 3.8828\n",
      "Epoch 10/150\n",
      "157/157 [==============================] - 81s 518ms/step - loss: 3.9003 - val_loss: 3.8649\n",
      "Epoch 11/150\n",
      "157/157 [==============================] - 81s 515ms/step - loss: 3.7537 - val_loss: 4.0094\n",
      "Epoch 12/150\n",
      "157/157 [==============================] - 81s 516ms/step - loss: 3.8039 - val_loss: 3.9439\n",
      "Epoch 13/150\n",
      "157/157 [==============================] - 81s 516ms/step - loss: 3.8290 - val_loss: 3.8100\n",
      "Epoch 14/150\n",
      "157/157 [==============================] - 81s 517ms/step - loss: 3.7251 - val_loss: 3.8066\n",
      "Epoch 15/150\n",
      "157/157 [==============================] - 81s 517ms/step - loss: 3.6354 - val_loss: 3.7875\n",
      "Epoch 16/150\n",
      "157/157 [==============================] - 81s 517ms/step - loss: 3.5822 - val_loss: 3.7584\n",
      "Epoch 17/150\n",
      "157/157 [==============================] - 81s 518ms/step - loss: 3.5833 - val_loss: 3.7428\n",
      "Epoch 18/150\n",
      "157/157 [==============================] - 81s 516ms/step - loss: 3.6991 - val_loss: 4.0471\n",
      "Epoch 19/150\n",
      "157/157 [==============================] - 81s 517ms/step - loss: 3.8935 - val_loss: 3.8504\n",
      "Epoch 20/150\n",
      "157/157 [==============================] - 81s 517ms/step - loss: 3.6357 - val_loss: 3.7843\n",
      "Epoch 21/150\n",
      "157/157 [==============================] - 81s 517ms/step - loss: 3.7012 - val_loss: 3.8533\n",
      "Epoch 22/150\n",
      "157/157 [==============================] - 81s 515ms/step - loss: 3.6023 - val_loss: 3.8902\n",
      "Epoch 23/150\n",
      "157/157 [==============================] - 81s 515ms/step - loss: 3.6253 - val_loss: 3.7345\n",
      "Epoch 24/150\n",
      "157/157 [==============================] - 81s 513ms/step - loss: 3.6009 - val_loss: 3.8339\n",
      "Epoch 25/150\n",
      "157/157 [==============================] - 80s 512ms/step - loss: 3.6067 - val_loss: 3.8265\n",
      "Epoch 26/150\n",
      "157/157 [==============================] - 81s 514ms/step - loss: 3.5289 - val_loss: 3.7589\n",
      "Epoch 27/150\n",
      "157/157 [==============================] - 81s 514ms/step - loss: 3.4781 - val_loss: 3.8677\n",
      "Epoch 28/150\n",
      "157/157 [==============================] - 81s 514ms/step - loss: 3.5425 - val_loss: 3.8934\n",
      "Epoch 29/150\n",
      "157/157 [==============================] - 81s 516ms/step - loss: 3.5447 - val_loss: 3.8096\n",
      "Epoch 30/150\n",
      "157/157 [==============================] - 81s 515ms/step - loss: 3.4938 - val_loss: 3.7290\n",
      "Epoch 31/150\n",
      "157/157 [==============================] - 81s 516ms/step - loss: 3.4586 - val_loss: 3.7832\n",
      "Epoch 32/150\n",
      "157/157 [==============================] - 82s 520ms/step - loss: 3.5689 - val_loss: 3.8383\n",
      "Epoch 33/150\n",
      "157/157 [==============================] - 81s 518ms/step - loss: 3.5279 - val_loss: 3.7375\n",
      "Epoch 34/150\n",
      "157/157 [==============================] - 81s 516ms/step - loss: 3.5272 - val_loss: 3.7411\n",
      "Epoch 35/150\n",
      "157/157 [==============================] - 81s 517ms/step - loss: 3.4788 - val_loss: 3.7623\n",
      "Epoch 36/150\n",
      "157/157 [==============================] - 81s 516ms/step - loss: 3.4384 - val_loss: 3.7537\n",
      "Epoch 37/150\n",
      "157/157 [==============================] - 81s 516ms/step - loss: 3.5054 - val_loss: 3.7258\n",
      "Epoch 38/150\n",
      "157/157 [==============================] - 81s 517ms/step - loss: 3.4522 - val_loss: 3.7348\n",
      "Epoch 39/150\n",
      "157/157 [==============================] - 81s 517ms/step - loss: 3.4551 - val_loss: 3.9089\n",
      "Epoch 40/150\n",
      "157/157 [==============================] - 81s 515ms/step - loss: 3.6857 - val_loss: 3.8189\n",
      "Epoch 41/150\n",
      "157/157 [==============================] - 81s 517ms/step - loss: 3.5846 - val_loss: 3.8753\n",
      "Epoch 42/150\n",
      "157/157 [==============================] - 81s 517ms/step - loss: 3.5472 - val_loss: 3.8004\n",
      "Epoch 43/150\n",
      "157/157 [==============================] - 81s 518ms/step - loss: 3.3857 - val_loss: 3.7959\n",
      "Epoch 44/150\n",
      "157/157 [==============================] - 81s 516ms/step - loss: 3.4510 - val_loss: 3.7237\n",
      "Epoch 45/150\n",
      "157/157 [==============================] - 81s 515ms/step - loss: 3.4230 - val_loss: 3.8361\n",
      "Epoch 46/150\n",
      "157/157 [==============================] - 81s 518ms/step - loss: 3.3941 - val_loss: 3.7490\n",
      "Epoch 47/150\n",
      "157/157 [==============================] - 81s 517ms/step - loss: 3.7441 - val_loss: 3.9263\n",
      "Epoch 48/150\n",
      "157/157 [==============================] - 81s 515ms/step - loss: 3.6207 - val_loss: 3.7895\n",
      "Epoch 49/150\n",
      "157/157 [==============================] - 81s 514ms/step - loss: 3.4151 - val_loss: 3.8138\n",
      "Epoch 50/150\n",
      "157/157 [==============================] - 81s 514ms/step - loss: 3.4970 - val_loss: 3.8535\n",
      "Epoch 51/150\n",
      "157/157 [==============================] - 81s 515ms/step - loss: 3.6426 - val_loss: 3.8190\n",
      "Epoch 52/150\n",
      "157/157 [==============================] - 81s 515ms/step - loss: 3.5654 - val_loss: 3.8419\n",
      "Epoch 53/150\n",
      "157/157 [==============================] - 81s 516ms/step - loss: 3.4133 - val_loss: 3.7470\n",
      "Epoch 54/150\n",
      "157/157 [==============================] - 81s 517ms/step - loss: 3.4715 - val_loss: 3.8222\n",
      "Epoch 55/150\n",
      "157/157 [==============================] - 81s 516ms/step - loss: 3.3893 - val_loss: 3.7460\n",
      "Epoch 56/150\n",
      "157/157 [==============================] - 81s 516ms/step - loss: 3.3955 - val_loss: 3.8507\n",
      "Epoch 57/150\n",
      "157/157 [==============================] - 81s 513ms/step - loss: 3.5694 - val_loss: 3.8866\n",
      "Epoch 58/150\n",
      "157/157 [==============================] - 81s 514ms/step - loss: 3.6362 - val_loss: 3.6960\n",
      "Epoch 59/150\n",
      "157/157 [==============================] - 81s 514ms/step - loss: 3.4304 - val_loss: 3.7544\n",
      "Epoch 60/150\n",
      "157/157 [==============================] - 81s 514ms/step - loss: 3.6322 - val_loss: 3.7877\n",
      "Epoch 61/150\n",
      "157/157 [==============================] - 81s 514ms/step - loss: 3.4288 - val_loss: 3.7407\n",
      "Epoch 62/150\n",
      "157/157 [==============================] - 81s 515ms/step - loss: 3.3979 - val_loss: 3.7284\n",
      "Epoch 63/150\n",
      "157/157 [==============================] - 81s 517ms/step - loss: 3.4120 - val_loss: 3.7273\n",
      "Epoch 64/150\n",
      "157/157 [==============================] - 80s 512ms/step - loss: 3.3577 - val_loss: 3.7074\n",
      "Epoch 65/150\n",
      "157/157 [==============================] - 81s 516ms/step - loss: 3.3967 - val_loss: 3.6922\n",
      "Epoch 66/150\n",
      "157/157 [==============================] - 81s 516ms/step - loss: 3.4923 - val_loss: 3.9719\n",
      "Epoch 67/150\n",
      "157/157 [==============================] - 81s 516ms/step - loss: 3.5675 - val_loss: 3.7103\n",
      "Epoch 68/150\n",
      "157/157 [==============================] - 81s 515ms/step - loss: 3.2966 - val_loss: 3.7107\n",
      "Epoch 69/150\n",
      "157/157 [==============================] - 81s 514ms/step - loss: 3.3885 - val_loss: 3.7960\n",
      "Epoch 70/150\n",
      "157/157 [==============================] - 81s 514ms/step - loss: 3.5501 - val_loss: 3.7624\n",
      "Epoch 71/150\n",
      "157/157 [==============================] - 81s 517ms/step - loss: 3.3377 - val_loss: 3.6951\n",
      "Epoch 72/150\n",
      "157/157 [==============================] - 81s 515ms/step - loss: 3.2804 - val_loss: 3.7452\n",
      "Epoch 73/150\n",
      "157/157 [==============================] - 82s 521ms/step - loss: 3.2956 - val_loss: 3.6816\n",
      "Epoch 74/150\n",
      "157/157 [==============================] - 82s 520ms/step - loss: 3.3077 - val_loss: 3.8704\n",
      "Epoch 75/150\n",
      "157/157 [==============================] - 81s 518ms/step - loss: 3.3898 - val_loss: 3.7447\n",
      "Epoch 76/150\n",
      "157/157 [==============================] - 81s 518ms/step - loss: 3.3420 - val_loss: 3.7771\n",
      "Epoch 77/150\n",
      "157/157 [==============================] - 81s 517ms/step - loss: 3.3016 - val_loss: 3.7317\n",
      "Epoch 78/150\n",
      "157/157 [==============================] - 81s 516ms/step - loss: 3.2519 - val_loss: 3.7073\n",
      "Epoch 79/150\n",
      "157/157 [==============================] - 81s 517ms/step - loss: 3.2470 - val_loss: 3.7079\n",
      "Epoch 80/150\n",
      "157/157 [==============================] - 81s 517ms/step - loss: 3.2418 - val_loss: 3.6738\n",
      "Epoch 81/150\n",
      "157/157 [==============================] - 81s 516ms/step - loss: 3.3183 - val_loss: 3.7510\n",
      "Epoch 82/150\n",
      "157/157 [==============================] - 82s 523ms/step - loss: 3.4187 - val_loss: 3.8452\n",
      "Epoch 83/150\n",
      "157/157 [==============================] - 81s 517ms/step - loss: 3.3350 - val_loss: 3.8368\n",
      "Epoch 84/150\n",
      "157/157 [==============================] - 81s 517ms/step - loss: 3.4752 - val_loss: 3.7382\n",
      "Epoch 85/150\n",
      "157/157 [==============================] - 81s 518ms/step - loss: 3.2956 - val_loss: 3.7020\n",
      "Epoch 86/150\n",
      "157/157 [==============================] - 81s 516ms/step - loss: 3.2484 - val_loss: 3.7096\n",
      "Epoch 87/150\n",
      "157/157 [==============================] - 81s 514ms/step - loss: 3.4241 - val_loss: 3.8440\n",
      "Epoch 88/150\n",
      "157/157 [==============================] - 81s 515ms/step - loss: 3.4875 - val_loss: 3.8222\n",
      "Epoch 89/150\n",
      "157/157 [==============================] - 81s 514ms/step - loss: 3.5078 - val_loss: 3.7184\n",
      "Epoch 90/150\n",
      "157/157 [==============================] - 81s 515ms/step - loss: 3.4136 - val_loss: 3.7239\n",
      "Epoch 91/150\n",
      "157/157 [==============================] - 81s 516ms/step - loss: 3.4064 - val_loss: 3.8186\n",
      "Epoch 92/150\n",
      "157/157 [==============================] - 81s 517ms/step - loss: 3.3632 - val_loss: 3.7825\n",
      "Epoch 93/150\n",
      "157/157 [==============================] - 81s 516ms/step - loss: 3.3991 - val_loss: 3.7592\n",
      "Epoch 94/150\n",
      "157/157 [==============================] - 81s 515ms/step - loss: 3.3042 - val_loss: 3.7649\n",
      "Epoch 95/150\n",
      "157/157 [==============================] - 81s 516ms/step - loss: 3.3034 - val_loss: 3.7667\n",
      "Epoch 96/150\n",
      "157/157 [==============================] - 80s 512ms/step - loss: 3.3153 - val_loss: 3.7264\n",
      "Epoch 97/150\n",
      "157/157 [==============================] - 80s 512ms/step - loss: 3.2587 - val_loss: 3.6618\n",
      "Epoch 98/150\n",
      "157/157 [==============================] - 80s 511ms/step - loss: 3.2111 - val_loss: 3.7220\n",
      "Epoch 99/150\n",
      "157/157 [==============================] - 80s 513ms/step - loss: 3.3698 - val_loss: 3.7711\n",
      "Epoch 100/150\n",
      "157/157 [==============================] - 81s 513ms/step - loss: 3.3499 - val_loss: 3.7108\n",
      "Epoch 101/150\n",
      "157/157 [==============================] - 81s 514ms/step - loss: 3.2841 - val_loss: 3.6452\n",
      "Epoch 102/150\n",
      "157/157 [==============================] - 81s 516ms/step - loss: 3.2967 - val_loss: 3.7918\n",
      "Epoch 103/150\n",
      "157/157 [==============================] - 81s 516ms/step - loss: 3.2487 - val_loss: 3.8413\n",
      "Epoch 104/150\n",
      "157/157 [==============================] - 81s 516ms/step - loss: 3.2545 - val_loss: 3.8075\n",
      "Epoch 105/150\n",
      "157/157 [==============================] - 81s 514ms/step - loss: 3.3848 - val_loss: 3.6288\n",
      "Epoch 106/150\n",
      "157/157 [==============================] - 81s 516ms/step - loss: 3.2091 - val_loss: 3.6549\n",
      "Epoch 107/150\n",
      "157/157 [==============================] - 81s 515ms/step - loss: 3.3392 - val_loss: 3.6850\n",
      "Epoch 108/150\n",
      "157/157 [==============================] - 81s 514ms/step - loss: 3.2286 - val_loss: 3.6916\n",
      "Epoch 109/150\n",
      "157/157 [==============================] - 81s 516ms/step - loss: 3.2262 - val_loss: 3.6630\n",
      "Epoch 110/150\n",
      "157/157 [==============================] - 81s 516ms/step - loss: 3.4210 - val_loss: 3.6041\n",
      "Epoch 111/150\n",
      "157/157 [==============================] - 81s 518ms/step - loss: 3.1934 - val_loss: 3.7720\n",
      "Epoch 112/150\n",
      "157/157 [==============================] - 81s 516ms/step - loss: 3.2423 - val_loss: 3.6697\n",
      "Epoch 113/150\n",
      "157/157 [==============================] - 81s 515ms/step - loss: 3.3477 - val_loss: 3.6625\n",
      "Epoch 114/150\n",
      "157/157 [==============================] - 82s 520ms/step - loss: 3.2273 - val_loss: 3.6759\n",
      "Epoch 115/150\n",
      "157/157 [==============================] - 81s 515ms/step - loss: 3.4087 - val_loss: 3.7306\n",
      "Epoch 116/150\n",
      "157/157 [==============================] - 81s 516ms/step - loss: 3.3926 - val_loss: 3.5764\n",
      "Epoch 117/150\n",
      "157/157 [==============================] - 82s 520ms/step - loss: 3.2148 - val_loss: 3.6064\n",
      "Epoch 118/150\n",
      "157/157 [==============================] - 82s 519ms/step - loss: 3.2933 - val_loss: 3.7516\n",
      "Epoch 119/150\n",
      "157/157 [==============================] - 81s 517ms/step - loss: 3.2557 - val_loss: 3.6112\n",
      "Epoch 120/150\n",
      "157/157 [==============================] - 81s 516ms/step - loss: 3.1440 - val_loss: 3.7330\n",
      "Epoch 121/150\n",
      "157/157 [==============================] - 81s 518ms/step - loss: 3.2441 - val_loss: 3.7971\n",
      "Epoch 122/150\n",
      "157/157 [==============================] - 81s 519ms/step - loss: 3.3491 - val_loss: 3.7208\n",
      "Epoch 123/150\n",
      "157/157 [==============================] - 81s 518ms/step - loss: 3.2901 - val_loss: 3.7283\n",
      "Epoch 124/150\n",
      "157/157 [==============================] - 81s 517ms/step - loss: 3.3843 - val_loss: 3.6927\n",
      "Epoch 125/150\n",
      "157/157 [==============================] - 81s 518ms/step - loss: 3.2255 - val_loss: 3.6973\n",
      "Epoch 126/150\n",
      "157/157 [==============================] - 81s 518ms/step - loss: 3.2265 - val_loss: 3.6858\n",
      "Epoch 127/150\n",
      "157/157 [==============================] - 81s 517ms/step - loss: 3.1612 - val_loss: 3.6436\n",
      "Epoch 128/150\n",
      "157/157 [==============================] - 81s 518ms/step - loss: 3.2308 - val_loss: 3.6983\n",
      "Epoch 129/150\n",
      "157/157 [==============================] - 81s 517ms/step - loss: 3.3024 - val_loss: 3.7098\n",
      "Epoch 130/150\n",
      "157/157 [==============================] - 81s 518ms/step - loss: 3.3258 - val_loss: 3.6440\n",
      "Epoch 131/150\n",
      "157/157 [==============================] - 81s 518ms/step - loss: 3.2103 - val_loss: 3.5728\n",
      "Epoch 132/150\n",
      "157/157 [==============================] - 81s 518ms/step - loss: 3.1828 - val_loss: 3.6711\n",
      "Epoch 133/150\n",
      "157/157 [==============================] - 82s 519ms/step - loss: 3.2280 - val_loss: 3.6209\n",
      "Epoch 134/150\n",
      "157/157 [==============================] - 81s 518ms/step - loss: 3.2057 - val_loss: 3.8009\n",
      "Epoch 135/150\n",
      "157/157 [==============================] - 81s 517ms/step - loss: 3.2855 - val_loss: 3.6363\n",
      "Epoch 136/150\n",
      "157/157 [==============================] - 81s 517ms/step - loss: 3.1255 - val_loss: 3.6767\n",
      "Epoch 137/150\n",
      "157/157 [==============================] - 82s 520ms/step - loss: 3.1448 - val_loss: 3.6936\n",
      "Epoch 138/150\n",
      "157/157 [==============================] - 81s 518ms/step - loss: 3.1749 - val_loss: 3.7232\n",
      "Epoch 139/150\n",
      "157/157 [==============================] - 81s 519ms/step - loss: 3.2517 - val_loss: 3.5502\n",
      "Epoch 140/150\n",
      "157/157 [==============================] - 81s 517ms/step - loss: 3.1076 - val_loss: 3.6486\n",
      "Epoch 141/150\n",
      "157/157 [==============================] - 81s 518ms/step - loss: 3.3301 - val_loss: 3.6864\n",
      "Epoch 142/150\n",
      "157/157 [==============================] - 81s 517ms/step - loss: 3.2099 - val_loss: 3.6551\n",
      "Epoch 143/150\n",
      "157/157 [==============================] - 81s 516ms/step - loss: 3.1031 - val_loss: 3.6719\n",
      "Epoch 144/150\n",
      "157/157 [==============================] - 82s 521ms/step - loss: 3.1664 - val_loss: 3.6052\n",
      "Epoch 145/150\n",
      "157/157 [==============================] - 81s 519ms/step - loss: 3.2527 - val_loss: 3.6455\n",
      "Epoch 146/150\n",
      "157/157 [==============================] - 81s 515ms/step - loss: 3.2498 - val_loss: 3.6434\n",
      "Epoch 147/150\n",
      "157/157 [==============================] - 81s 518ms/step - loss: 3.2190 - val_loss: 3.6171\n",
      "Epoch 148/150\n",
      "157/157 [==============================] - 81s 519ms/step - loss: 3.1556 - val_loss: 3.6783\n",
      "Epoch 149/150\n",
      "157/157 [==============================] - 81s 518ms/step - loss: 3.1925 - val_loss: 3.5685\n",
      "Epoch 150/150\n",
      "157/157 [==============================] - 81s 517ms/step - loss: 3.1673 - val_loss: 3.5918\n",
      "Model: \"encoder_decoder_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_1 (Encoder)          multiple                  4347640   \n",
      "_________________________________________________________________\n",
      "decoder_2 (Decoder)          multiple                  5753269   \n",
      "=================================================================\n",
      "Total params: 10,100,909\n",
      "Trainable params: 10,100,909\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size=512\n",
    "score_fun  = 'general'\n",
    "att_units = 64\n",
    "model_2  = encoder_decoder(score_fun = score_fun , encoder_inputs_length=20,decoder_inputs_length=10,output_vocab_size=vocab_size_CORRECT_SENTENCE)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "model_2.compile(optimizer=optimizer,loss='sparse_categorical_crossentropy')\n",
    "train_steps=train.shape[0]//1024\n",
    "valid_steps=validation.shape[0]//1024\n",
    "#TRANING THE MODEL FOR 20 EPOCHS CAUSE , MORE TRAINING GIVES MORE RESULTS\n",
    "log_dir=\"logs1\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "checkpoint = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1, write_graph=True,write_grads=True)\n",
    "model_2.fit_generator(train_dataloader, steps_per_epoch=train_steps, epochs=150, validation_data=train_dataloader, validation_steps=valid_steps , callbacks = checkpoint)\n",
    "# model_1.fit_generator(train_dataloader,  epochs=4, validation_data=train_dataloader)\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "SwuT0ilOWyep"
   },
   "outputs": [],
   "source": [
    "os.mkdir('saved_model_attention')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "9nBeay4BWxZm"
   },
   "outputs": [],
   "source": [
    "model_2.save_weights('saved_model_attention/attention.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fkn1yHlWHjJQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A_mc_2MyWxSx",
    "outputId": "52d00bd1-fd00-4fce-ba11-01ae4167888d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder_decoder_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_1 (Encoder)          multiple                  4347640   \n",
      "_________________________________________________________________\n",
      "decoder_2 (Decoder)          multiple                  5753269   \n",
      "=================================================================\n",
      "Total params: 10,100,909\n",
      "Trainable params: 10,100,909\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.load_weights(\"/content/drive/MyDrive/saved_model_attention/attention.h5\")\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DpC9zlzMcXp"
   },
   "source": [
    "## <font color='blue'>**Inference**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1IhdBrgQYJr"
   },
   "source": [
    "<font color='blue'>**CORRECTING THE INCORRECT SENTENCES**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MP3kLZoPMvSu",
    "outputId": "3670fdec-f808-4b2b-8025-4cb4859ae1c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figg had perched hersela nervously on the very edge oa\n",
      "and\n",
      "BLEU score: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "def predict(input_sentence):\n",
    "    pred = []\n",
    "    input_sequence = tknizer_CORRECT_SENTENCE.texts_to_sequences([input_sentence])\n",
    "    result = ' '\n",
    "    encoder_seq = pad_sequences(input_sequence, maxlen=20, dtype='int32', padding='post')  \n",
    "    initial_state = model_2.layers[0].initialize_states(1)\n",
    "    encoder_output, encoder_h, encoder_c = model_2.layers[0](tf.constant(encoder_seq), initial_state)\n",
    "    start_index = tf.constant([[tknizer_CORRECT_SENTENCE.word_index['<start>']]])\n",
    "    states = [encoder_h, encoder_c]\n",
    "\n",
    "    for i in range(20): \n",
    "        # print(start_index)\n",
    "        predicted_out,state_h,state_c,attention_weights,context_vector = model_2.layers[1].onestep_decoder(start_index, encoder_output , encoder_h, encoder_c )\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        start_index = np.reshape(np.argmax(predicted_out), (1, 1))\n",
    "        pred.append(tknizer_CORRECT_SENTENCE.index_word[start_index[0][0]])\n",
    "        if(pred[-1]=='<end>'):\n",
    "            break\n",
    "        translated_sentence = ' '.join(pred)\n",
    "    return translated_sentence\n",
    "\n",
    "inp = validation.values[2000][0]\n",
    "print(inp)\n",
    "translate = predict(inp)\n",
    "print(translate)\n",
    "print('BLEU score: {}'.format(bleu.sentence_bleu(inp, translate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7OH25UaQH1DZ",
    "outputId": "b5fa7d6b-7669-4319-9cc1-1f242202638c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He s just started at the Ministry and this is\n",
      "i have the\n",
      "BLEU score: 0.8801117367933934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "inp = validation.values[2][0]\n",
    "print(inp)\n",
    "translate = predict(inp)\n",
    "print(translate)\n",
    "print('BLEU score: {}'.format(bleu.sentence_bleu(inp, translate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "prdmzWvhH1Db",
    "outputId": "b752e656-dcd8-4c13-d563-f1d45fb56911"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Io he were his genius oor light music would be\n",
      "i have the\n",
      "BLEU score: 0.8408964152537145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "inp = validation.values[20][0]\n",
    "print(inp)\n",
    "translate = predict(inp)\n",
    "print(translate)\n",
    "print('BLEU score: {}'.format(bleu.sentence_bleu(inp, translate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ItU3DRZ_H1Dc",
    "outputId": "ee826c0c-b89a-4d18-f313-8cab06fb9d40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figg had perched hersela nervously on the very edge oa\n",
      "and have the\n",
      "BLEU score: 0.9036020036098448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "inp = validation.values[2000][0]\n",
    "print(inp)\n",
    "translate = predict(inp)\n",
    "print(translate)\n",
    "print('BLEU score: {}'.format(bleu.sentence_bleu(inp, translate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "s6KDp62yH1Dd"
   },
   "outputs": [],
   "source": [
    "#USING THIS METHOD TO GET THE BLEU SCORE , THE REFERENCE NOTEBOOK METHOD SHOWS ME AN ERROR FOR 1000 FILES , BUT IT WORKS FINE FOR SINGLE FILE\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WnnOeacuH1De",
    "outputId": "6990980e-94e3-4188-fd6c-17841a8e9cc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He s just started at the Ministry and this is\n",
      "i have the\n",
      "BLEU score: 0.8801117367933934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "inp = validation.values[2][0]\n",
    "print(inp)\n",
    "translate = predict(inp)\n",
    "print(translate)\n",
    "print('BLEU score: {}'.format(nltk.translate.bleu_score.sentence_bleu(inp, translate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "DiFzVJGhXUr6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9F0i-eyyXUmV",
    "outputId": "def2f7d0-1454-431c-88fe-31ff16f3e063"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:46<00:00, 21.37it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i have been to have been to have been to have been to have been to have been to have',\n",
       " 'asked',\n",
       " 'i have the',\n",
       " 'they have the',\n",
       " 'you don t have the',\n",
       " 'and have the',\n",
       " 'i have the',\n",
       " 'he asked',\n",
       " 'i have to have to have to have to have to have to have to have to have to have',\n",
       " 'harry could have the',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'i have the',\n",
       " 'i have the',\n",
       " 'i have the',\n",
       " 'when they have been to be asked',\n",
       " 'he s going to an',\n",
       " 'he was a',\n",
       " 'have the',\n",
       " 'and have the',\n",
       " 'his',\n",
       " 'he no more he no more he no more he no more he no more he no more he no',\n",
       " 'have the',\n",
       " 'and have come and have come and have been to have been to have the',\n",
       " 'have the',\n",
       " 'the',\n",
       " 'he asked',\n",
       " 'she have the',\n",
       " 'i have the',\n",
       " 'he',\n",
       " 'he',\n",
       " 'when i have the',\n",
       " 'this have the',\n",
       " 'i have an',\n",
       " 'from him from him from him from him from him from him from him from him from him from him',\n",
       " 'as if the',\n",
       " 'i have been to have been to have been to have been to have been to have the',\n",
       " 'he',\n",
       " 'he asked',\n",
       " 'i have it s not have it s not have it s not have it s not have it s',\n",
       " 'you have the',\n",
       " 'i have to have to have to have to have to have to have to have to have to have',\n",
       " 'he had to have he had to have the',\n",
       " 'he asked one another',\n",
       " 'you have the',\n",
       " 'i have the',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'i have the',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'i have my',\n",
       " 'still have the',\n",
       " 'i have it and',\n",
       " 'he asked',\n",
       " 'they have the',\n",
       " 'for his',\n",
       " 'told him',\n",
       " 'he asked the',\n",
       " 'you don t have the',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'and then and then and then and then and then and then and then and then and then and then',\n",
       " 'you don t have the',\n",
       " 'she have the',\n",
       " 'he asked as if he asked as if he asked as if he asked as if he asked as if',\n",
       " 'to have the',\n",
       " 'he',\n",
       " 'he',\n",
       " 'he asked',\n",
       " 'it have the',\n",
       " 'i have the',\n",
       " 'he asked',\n",
       " 'he asked to have the',\n",
       " 'he asked',\n",
       " 'he said',\n",
       " 'i have the',\n",
       " 'what he s not have been for his',\n",
       " 'he asked',\n",
       " 'an',\n",
       " 'we have the',\n",
       " 'he asked as if the',\n",
       " 'have the',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'he',\n",
       " 'as if the',\n",
       " 'the',\n",
       " 'i have the',\n",
       " 'a',\n",
       " 'i have been to have been to have been to have been to have been to have been to have',\n",
       " 'he asked',\n",
       " 'i have the',\n",
       " 'when i have the',\n",
       " 'he asked',\n",
       " 'i have the',\n",
       " 'and have the',\n",
       " 'he had to have been to have no more than he had to have been to have been to have',\n",
       " 'he had it have it have it have it have it have it have it have it have it have',\n",
       " 'the',\n",
       " 'he',\n",
       " 'he',\n",
       " 'they have the',\n",
       " 'they have the',\n",
       " 'not have not have not have not have not have not have not have not have not have not have',\n",
       " 'he asked',\n",
       " 'the',\n",
       " 'to have to have to have to have to have to have to have to have to have to have',\n",
       " 'they have the',\n",
       " 'i have the',\n",
       " 'he asked',\n",
       " 'it was a',\n",
       " 'if he would have it have it have it have it have it have it have it have it have',\n",
       " 'it have the',\n",
       " 'it s not have it s not have it s not have it s not have it s not have',\n",
       " 'he asked',\n",
       " 'he s',\n",
       " 'and then and then and then and then and then and then and then and then and then and then',\n",
       " 'he asked',\n",
       " 'he could have been to have been to have been to have been to have been to have been to',\n",
       " 'i have the',\n",
       " 'he had to have the',\n",
       " 'he had just have just have just have just have just have just have just have just have just have',\n",
       " 'when i have the',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'he had to have the',\n",
       " 'i have the',\n",
       " 'he',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'i have the',\n",
       " 'it',\n",
       " 'the',\n",
       " 'he would have been to have been to have been to have been to have been to have been to',\n",
       " 'he asked',\n",
       " 'when have the',\n",
       " 'a',\n",
       " 'in the',\n",
       " 'when i have the',\n",
       " 'he asked',\n",
       " 'it',\n",
       " 'i have the',\n",
       " 'they have been to have been to have been to have been to have been to have been to have',\n",
       " 'he asked',\n",
       " 'he',\n",
       " 'she have the',\n",
       " 'he asked',\n",
       " 'i have the',\n",
       " 'he',\n",
       " 'he asked as if he asked as if he asked as if he asked as if he asked as if',\n",
       " 'for his',\n",
       " 'and have the',\n",
       " 'i have the',\n",
       " 'the',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'he',\n",
       " 'as if you have the',\n",
       " 'what he asked',\n",
       " 'i have been to have been to have been to have been to have been to have been to have',\n",
       " 'when he had come and have come and have come and have come and have come and have come',\n",
       " 'and',\n",
       " 'and have the',\n",
       " 'i have the',\n",
       " 'he',\n",
       " 'he asked',\n",
       " 'you have the',\n",
       " 'he asked',\n",
       " 'i have the',\n",
       " 'she have the',\n",
       " 'have the',\n",
       " 'they have been to have been to have been to have been to have been to have been to have',\n",
       " 'and have the',\n",
       " 'no one',\n",
       " 'they have the',\n",
       " 'you don t have the',\n",
       " 'his',\n",
       " 'i have been to have to have been to have been to have to have been to have been to',\n",
       " 'when have the',\n",
       " 'i have been to have been to have been one another',\n",
       " 'he',\n",
       " 'have been to have been to have been to have been to have been to have been to have been',\n",
       " 'i have the',\n",
       " 'i have been to have been to have been to have been to have been to have been to have',\n",
       " 'i have the',\n",
       " 'for his',\n",
       " 'harry could have the',\n",
       " 'he asked',\n",
       " 'as if the',\n",
       " 'when they have the',\n",
       " 'he had to have the',\n",
       " 'she have the',\n",
       " 'it to have no one asked',\n",
       " 'what he asked',\n",
       " 'to have to have to have to have to have to have to have to have to have to have',\n",
       " 'he said he said he said he said he said he said he said he said he said he asked',\n",
       " 'i have it have it have it have it would have it have it have it have it have it',\n",
       " 'when he had been to have the',\n",
       " 'and have the',\n",
       " 'he asked',\n",
       " 'have been to have been to have been to have been to have been to have been to have been',\n",
       " 'not have not have not have not have the',\n",
       " 'he asked',\n",
       " 'so many one another',\n",
       " 'it',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'no more to have no more than i have no more to have no one',\n",
       " 'and',\n",
       " 'more than',\n",
       " 'i have the',\n",
       " 'i have the',\n",
       " 'i have the',\n",
       " 'i have the',\n",
       " 'the',\n",
       " 'he asked',\n",
       " 'the',\n",
       " 'the',\n",
       " 'as if he asked as if he asked as if he asked as if he asked as if the',\n",
       " 'he had made him',\n",
       " 'he had been to have been to have been to have been to have been to have been to have',\n",
       " 'he asked',\n",
       " 'they have the',\n",
       " 'the',\n",
       " 'asked asked asked asked asked asked asked',\n",
       " 'he asked',\n",
       " 'the',\n",
       " 'i have the',\n",
       " 'as if the',\n",
       " 'have the',\n",
       " 'he asked',\n",
       " 'he had to have the',\n",
       " 'he asked',\n",
       " 'i have the',\n",
       " 'he asked as if the',\n",
       " 'and a',\n",
       " 'he',\n",
       " 'he was a',\n",
       " 'it was so many of it was it was so many of it was so many of it was it',\n",
       " 'all',\n",
       " 'he asked',\n",
       " 'he',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'i have the',\n",
       " 'he',\n",
       " 'the',\n",
       " 'and have the',\n",
       " 'and then asked and then asked',\n",
       " 'he asked',\n",
       " 'harry asked harry asked harry asked harry asked harry asked harry asked harry asked harry asked harry could have the',\n",
       " 'he had to have the',\n",
       " 'have the',\n",
       " 'i have the',\n",
       " 'but have the',\n",
       " 'he was',\n",
       " 'we have the',\n",
       " 'he asked him',\n",
       " 'so many one another',\n",
       " 'dumbledore',\n",
       " 'he had come',\n",
       " 'he',\n",
       " 'he asked',\n",
       " 'if he were the',\n",
       " 'and have the',\n",
       " 'as if you don t have the',\n",
       " 'i have it have it have it have it have it have it have it have it have it have',\n",
       " 'it',\n",
       " 'we have been to have been to have been here he s something',\n",
       " 'i have the',\n",
       " 'i have the',\n",
       " 'have the',\n",
       " 'he asked',\n",
       " 'he was',\n",
       " 'have to have to have to have to have to have to have to have to have to have to',\n",
       " 'have the',\n",
       " 'i have the',\n",
       " 'i have been to have been to have been to have been to have been to have been to have',\n",
       " 'he asked',\n",
       " 'he',\n",
       " 'as if his',\n",
       " 'for his',\n",
       " 'he could have been to have been to have been to have been to have been to have been to',\n",
       " 'he would have the',\n",
       " 'he said',\n",
       " 'they have the',\n",
       " 'he asked',\n",
       " 'they have the',\n",
       " 'i have the',\n",
       " 'have the',\n",
       " 'and then asked',\n",
       " 'you have the',\n",
       " 'i have just have the',\n",
       " 'he s not have the',\n",
       " 'i have the',\n",
       " 'i have the',\n",
       " 'i have the',\n",
       " 'and then asked and then asked and then asked and then asked and then asked and then asked and then',\n",
       " 'have been to have been to have been to have been to have been to have been to have been',\n",
       " 'they have the',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'he was a',\n",
       " 'he asked',\n",
       " 'he',\n",
       " 'his',\n",
       " 'the',\n",
       " 'he asked to have the',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'as if he s not have the',\n",
       " 'and',\n",
       " 'it to have it to have it to have it to have it to have it to have it to',\n",
       " 'he was a',\n",
       " 'he asked',\n",
       " 'when have been to have been to have been to have been to have been to have been to have',\n",
       " 'they have the',\n",
       " 'have told him',\n",
       " 'when have been to have been to have been to have been to have the',\n",
       " 'it was a',\n",
       " 'he asked',\n",
       " 'he s not have the',\n",
       " 'he said harry could have the',\n",
       " 'he had made',\n",
       " 'and then a',\n",
       " 'it s not have it s not have it s not have it s not have it s not have',\n",
       " 'i have the',\n",
       " 'when they have the',\n",
       " 'i have the',\n",
       " 'he',\n",
       " 'his',\n",
       " 'he had made to have the',\n",
       " 'he asked',\n",
       " 'and have the',\n",
       " 'we have the',\n",
       " 'when he asked',\n",
       " 'he said',\n",
       " 'and asked as on the',\n",
       " 'have the',\n",
       " 'the',\n",
       " 'he had made',\n",
       " 'he asked as if the',\n",
       " 'he asked',\n",
       " 'if you don t have it have it to have it have it to have the',\n",
       " 'i have the',\n",
       " 'and have the',\n",
       " 'he could have the',\n",
       " 'it have the',\n",
       " 'he',\n",
       " 'it was a',\n",
       " 'they have the',\n",
       " 'have the',\n",
       " 'and have the',\n",
       " 'he',\n",
       " 'and have the',\n",
       " 'he had made',\n",
       " 'i have the',\n",
       " 'he asked',\n",
       " 'i have the',\n",
       " 'what he asked that he asked that he asked that he asked',\n",
       " 'they have the',\n",
       " 'he asked',\n",
       " 'no one of',\n",
       " 'he asked',\n",
       " 'he said',\n",
       " 'it was not have not have not have not have not have not have not have not have not have',\n",
       " 'and have the',\n",
       " 'and then asked and then asked and then asked and then asked and then asked and then asked',\n",
       " 'and then a',\n",
       " 'he',\n",
       " 'he',\n",
       " 'for his',\n",
       " 'he asked',\n",
       " 'i have the',\n",
       " 'he',\n",
       " 'he asked',\n",
       " 'an',\n",
       " 'he asked',\n",
       " 'he s not have the',\n",
       " 'they have the',\n",
       " 'and have the',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'is not to have the',\n",
       " 'he',\n",
       " 'he asked',\n",
       " 'when he had been to have the',\n",
       " 'i have the',\n",
       " 'on the',\n",
       " 'he',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'he had to have been to have been to have been to have been to have been to have been',\n",
       " 'he s not have the',\n",
       " 'he was a',\n",
       " 'if',\n",
       " 'he asked',\n",
       " 'i have to have to have to have to have to have to have to have to have to have',\n",
       " 'when have the',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'the',\n",
       " 'and have the',\n",
       " 'when he is',\n",
       " 'a',\n",
       " 'she have the',\n",
       " 'and have the',\n",
       " 'he had made',\n",
       " 'if he asked',\n",
       " 'he asked',\n",
       " 'for his',\n",
       " 'the',\n",
       " 'when have the',\n",
       " 'he had to have the',\n",
       " 'when have the',\n",
       " 'i have the',\n",
       " 'i have the',\n",
       " 'he was a',\n",
       " 'he was a',\n",
       " 'he had to have the',\n",
       " 'i have the',\n",
       " 'she have the',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'no more than i have no more to his',\n",
       " 'the',\n",
       " 'he asked',\n",
       " 'harry could have the',\n",
       " 'i have the',\n",
       " 'i have the',\n",
       " 'have the',\n",
       " 'he would have the',\n",
       " 'but it have the',\n",
       " 'he asked as if the',\n",
       " 'i have the',\n",
       " 'as if you have the',\n",
       " 'his',\n",
       " 'i have the',\n",
       " 'permanent assemblage of',\n",
       " 'and have the',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'i have the',\n",
       " 'she have the',\n",
       " 'he',\n",
       " 'i have the',\n",
       " 'you have the',\n",
       " 'he',\n",
       " 'he asked',\n",
       " 'as if you have the',\n",
       " 'i have the',\n",
       " 'i have it is it is it is it is it is it is it is it is it is',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'and',\n",
       " 'he',\n",
       " 'he asked',\n",
       " 'a',\n",
       " 'he asked',\n",
       " 'for his',\n",
       " 'he asked harry asked harry asked harry asked harry asked',\n",
       " 'he asked',\n",
       " 'if he was a',\n",
       " 'he would have the',\n",
       " 'not have not have not have not have not have not have not have not have not have not a',\n",
       " 'no more have no one would have no more have no more have no one would have no more have',\n",
       " 'he had to have the',\n",
       " 'as if his',\n",
       " 'he would have the',\n",
       " 'have the',\n",
       " 'he said',\n",
       " 'he',\n",
       " 'he asked',\n",
       " 'when they have the',\n",
       " 'have the',\n",
       " 'one another',\n",
       " 'he was',\n",
       " 'i have the',\n",
       " 'he asked as if he asked as if he asked as if he asked as if he asked as if',\n",
       " 'that would have the',\n",
       " 'all',\n",
       " 'i have the',\n",
       " 'he was a',\n",
       " 'it s not have it s not',\n",
       " 'he asked',\n",
       " 'when have the',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'it was no more it was no more it was no more than he was no more it was no',\n",
       " 'that asked',\n",
       " 'asked',\n",
       " 'i have the',\n",
       " 'he looked back',\n",
       " 'i have it',\n",
       " 'it would have it would have it would have it would have it would have it would have it would',\n",
       " 'he s have he s have the',\n",
       " 'he asked as if the',\n",
       " 'and have the',\n",
       " 'he asked as if he asked as if he',\n",
       " 'they have the',\n",
       " 'he have he have the',\n",
       " 'after him not have the',\n",
       " 'when he asked him',\n",
       " 'he',\n",
       " 'he asked as if he asked as if he asked as if he asked as if the',\n",
       " 'he asked',\n",
       " 'i have been to have the',\n",
       " 'he was to have the',\n",
       " 'he asked',\n",
       " 'he had if he would have he had if he would have he would have he would have the',\n",
       " 'i have the',\n",
       " 'and have the',\n",
       " 'he had made',\n",
       " 'he said',\n",
       " 'i have the',\n",
       " 'he asked',\n",
       " 'he asked him',\n",
       " 'as if as if as if as if as if as if as if as if as if as if',\n",
       " 'i have the',\n",
       " 'one another',\n",
       " 'and have the',\n",
       " 'i have it have it have it have it have it have it have it have it have it have',\n",
       " 'i have the',\n",
       " 'she have the',\n",
       " 'he asked as if he asked as if he asked as if he asked as if he asked as if',\n",
       " 'i have the',\n",
       " 'harry could not have the',\n",
       " 'i have been to have been to have been to have been to have been to have been to have',\n",
       " 'he asked',\n",
       " 'harry asked harry asked',\n",
       " 'he asked',\n",
       " 'i have been to have been to have been to have been to have the',\n",
       " 'i have the',\n",
       " 'there were',\n",
       " 'i have the',\n",
       " 'have been to have been to have been to have the',\n",
       " 'i have the',\n",
       " 'he asked one another',\n",
       " 'she have the',\n",
       " 'the',\n",
       " 'i have the',\n",
       " 'he asked as if he asked as if he asked as if he asked as if he asked as if',\n",
       " 'he asked as if he asked as if he asked as if he asked as if he asked as if',\n",
       " 'you don t have the',\n",
       " 'i have the',\n",
       " 'he',\n",
       " 'don t have been to have the',\n",
       " 'he asked him',\n",
       " 'he s asked',\n",
       " 'as if he had not have the',\n",
       " 'and then asked',\n",
       " 'no no no no no no no no no no no no no no no no no no no no',\n",
       " 'i have an',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'his',\n",
       " 'i have the',\n",
       " 'this have the',\n",
       " 'he said',\n",
       " 'i have the',\n",
       " 'he',\n",
       " 'have the',\n",
       " 'and have the',\n",
       " 'they have the',\n",
       " 'he asked',\n",
       " 'he asked him',\n",
       " 'he',\n",
       " 'i have just as if his',\n",
       " 'she have the',\n",
       " 'it have the',\n",
       " 'harry was not harry was not harry was not know it was not harry was not harry was not harry',\n",
       " 'he',\n",
       " 'he asked',\n",
       " 'he',\n",
       " 'there was an',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'she have the',\n",
       " 'he asked him',\n",
       " 'it asked',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'he was a',\n",
       " 'this asked',\n",
       " 'you don t have the',\n",
       " 'he had not have the',\n",
       " 'he s not have the',\n",
       " 'he asked',\n",
       " 'a',\n",
       " 'the',\n",
       " 'he is to have the',\n",
       " 'they have the',\n",
       " 'he asked',\n",
       " 'he',\n",
       " 'as if his',\n",
       " 'i have the',\n",
       " 'i have the',\n",
       " 'have the',\n",
       " 'when i have the',\n",
       " 'have been to have been to have been to have been to have been to have been to have been',\n",
       " 'i have the',\n",
       " 'he asked him',\n",
       " 'i have the',\n",
       " 'i have the',\n",
       " 'it have it s not have the',\n",
       " 'it',\n",
       " 'he asked as if asked as if asked as if asked as if asked as if asked as if asked',\n",
       " 'he asked harry could have the',\n",
       " 'he asked him',\n",
       " 'when i have come and have come and have come',\n",
       " 'he asked',\n",
       " 'he would have the',\n",
       " 'he asked',\n",
       " 'you don t have the',\n",
       " 'one another',\n",
       " 'he was a',\n",
       " 'he asked',\n",
       " 'his',\n",
       " 'he asked',\n",
       " 'then have the',\n",
       " 'don t have the',\n",
       " 'but he',\n",
       " 'he s not have the',\n",
       " 'to have to have the',\n",
       " 'but',\n",
       " 'he',\n",
       " 'he asked him',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'and have the',\n",
       " 'it would have the',\n",
       " 'the',\n",
       " 'a',\n",
       " 'he asked',\n",
       " 'he',\n",
       " 'he asked',\n",
       " 'i have the',\n",
       " 'i have the',\n",
       " 'he was a',\n",
       " 'as if have the',\n",
       " 'he',\n",
       " 'they have the',\n",
       " 'i have not have not have not have not have the',\n",
       " 'he s not have been to have the',\n",
       " 'it have the',\n",
       " 'he',\n",
       " 'i have the',\n",
       " 'his',\n",
       " 'i have the',\n",
       " 'have the',\n",
       " 'when he had been to have been to have been to have been to have been to have been to',\n",
       " 'he had to have the',\n",
       " 'his',\n",
       " 'they have the',\n",
       " 'he asked',\n",
       " 'and have the',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'he had to have the',\n",
       " 'he',\n",
       " 'and have the',\n",
       " 'no one another',\n",
       " 'his',\n",
       " 'he asked',\n",
       " 'he',\n",
       " 'and have the',\n",
       " 'i have the',\n",
       " 'have the',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'if he have the',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'as if she have the',\n",
       " 'when i have been to have been to have been to have been to have been to have been to',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'and have the',\n",
       " 'he asked',\n",
       " 'he was a',\n",
       " 'what he asked as if you don t have the',\n",
       " 'i have the',\n",
       " 'he',\n",
       " 'for him',\n",
       " 'when i have the',\n",
       " 'you don t have the',\n",
       " 'he had made',\n",
       " 'then have been to have been to have been to have been to have been to have been to have',\n",
       " 'he',\n",
       " 'i have it',\n",
       " 'he had made',\n",
       " 'he asked',\n",
       " 'he',\n",
       " 'and have the',\n",
       " 'when i have the',\n",
       " 'he asked',\n",
       " 'he',\n",
       " 'they have the',\n",
       " 'his',\n",
       " 'he',\n",
       " 'i have just as if your',\n",
       " 'it have the',\n",
       " 'he was',\n",
       " 'he said he said he said he said he said he said he said he said he said he said',\n",
       " 'he had been to have been to have been to have been to have been to have been to have',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'and have not have the',\n",
       " 'he never have never have no one would have never have no one would have never have no one would',\n",
       " 'i have it was his',\n",
       " 'it s not have the',\n",
       " 'the',\n",
       " 'he was',\n",
       " 'i have the',\n",
       " 'i have the',\n",
       " 'so many one another',\n",
       " 'as if the',\n",
       " 'he had to have the',\n",
       " 'the',\n",
       " 'have the',\n",
       " 'he asked',\n",
       " 'he was',\n",
       " 'when have been to have been to have been to have been to have been to have been to have',\n",
       " 'he',\n",
       " 'i have the',\n",
       " 'he said',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'i have the',\n",
       " 'his',\n",
       " 'he asked',\n",
       " 'you have the',\n",
       " 'i have to have to have to have to have to have to have to have to have to have',\n",
       " 'when he had to have the',\n",
       " 'he said',\n",
       " 'he asked',\n",
       " 'when have been to have been to have been to have been to have been to have been to have',\n",
       " 'he asked',\n",
       " 'it was',\n",
       " 'they have the',\n",
       " 'and as if the',\n",
       " 'he asked',\n",
       " 'he asked harry could have the',\n",
       " 'we have just have the',\n",
       " 'he',\n",
       " 'it would have it would have it would have it would have it would have it would have it would',\n",
       " 'i have the',\n",
       " 'he asked',\n",
       " 'i have to have to have the',\n",
       " 'when i have the',\n",
       " 'he had made',\n",
       " 'all asked',\n",
       " 'i have the',\n",
       " 'he asked',\n",
       " 'but it have the',\n",
       " 'he asked',\n",
       " 'have the',\n",
       " 'he',\n",
       " 'the',\n",
       " 'he asked',\n",
       " 'have the',\n",
       " 'it asked',\n",
       " 'he asked',\n",
       " 'he had made',\n",
       " 'then have been to have been to have just have been to have been to have been to have been',\n",
       " 'he asked',\n",
       " 'have the',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'for his',\n",
       " 'and have the',\n",
       " 'he had made to have been to have been to have been to have been to have been to have',\n",
       " 'he asked',\n",
       " 'i have come from',\n",
       " 'he',\n",
       " 'he asked',\n",
       " 'he',\n",
       " 'he had made the',\n",
       " 'he',\n",
       " 'she said harry asked',\n",
       " 'he asked',\n",
       " 'to to to to to to to to to to to to to to to to to to to to',\n",
       " 'he asked',\n",
       " 'have the',\n",
       " 'for his',\n",
       " 'he asked as if he asked as if he asked as if he asked as if he asked as if',\n",
       " 'have the',\n",
       " 'i have the',\n",
       " 'he asked as if the',\n",
       " 'he asked as if he asked as if he asked as if he asked as if he asked as if',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'i have no one',\n",
       " 'i have the',\n",
       " 'he',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'when they have the',\n",
       " 'don t have to have to have to have to have to have to have to have to have to',\n",
       " 'i have the',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'i have it was a',\n",
       " 'he',\n",
       " 'he asked him',\n",
       " 'i have the',\n",
       " 'so many one another',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'i have the',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'no one another',\n",
       " 'your',\n",
       " 'he asked',\n",
       " 'i have just to be one s not have just to be one s not have just to be one',\n",
       " 'and have the',\n",
       " 'i have the',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'in an',\n",
       " 'he asked',\n",
       " 'i have the',\n",
       " 'and',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'what he asked',\n",
       " 'when he had not have the',\n",
       " 'he was',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'so many one another',\n",
       " 'he was a',\n",
       " 'i have the',\n",
       " 'when have the',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'i have the',\n",
       " 'he asked',\n",
       " 'and have the',\n",
       " 'he asked',\n",
       " 'his',\n",
       " 'he could not have the',\n",
       " 'they have the',\n",
       " 'she have the',\n",
       " 'have the',\n",
       " 'he s not have the',\n",
       " 'it',\n",
       " 'he had been to have the',\n",
       " 'i have the',\n",
       " 'then he is to be',\n",
       " 'it was a',\n",
       " 'and have the',\n",
       " 'when have the',\n",
       " 'he asked',\n",
       " 'i have him',\n",
       " 'it',\n",
       " 'he was a',\n",
       " 'you have the',\n",
       " 'have the',\n",
       " 'he had been',\n",
       " 'there was a',\n",
       " 'i have the',\n",
       " 'he was a',\n",
       " 'all',\n",
       " 'he asked',\n",
       " 'and have the',\n",
       " 'and then asked',\n",
       " 'when have the',\n",
       " 'and have the',\n",
       " 'he had to have the',\n",
       " 'he',\n",
       " 'when i have the',\n",
       " 'he had made a',\n",
       " 'the',\n",
       " 'i have the',\n",
       " 'have the',\n",
       " 'the',\n",
       " 'have the',\n",
       " 'and then asked',\n",
       " 'he',\n",
       " 'i have the',\n",
       " 'i have been to have the',\n",
       " 'have the',\n",
       " 'he had to have the',\n",
       " 'have the',\n",
       " 'he s not have the',\n",
       " 'he',\n",
       " 'he was',\n",
       " 'he asked',\n",
       " 'have the',\n",
       " 'he is',\n",
       " 'he asked',\n",
       " 'he asked as if he asked as if he asked as if he asked as if he asked as if',\n",
       " 'it and he and he',\n",
       " 'i have your',\n",
       " 'and have the',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'harry could have the',\n",
       " 'he asked',\n",
       " 'i have the',\n",
       " 'and have the',\n",
       " 'she had not have the',\n",
       " 'this have the',\n",
       " 'they have the',\n",
       " 'you have been asked',\n",
       " 'he',\n",
       " 'he',\n",
       " 'he asked',\n",
       " 'i have the',\n",
       " 'not have not have not have not have not have not have not have not have not have not have',\n",
       " 'he asked',\n",
       " 'have the',\n",
       " 'she have the',\n",
       " 'they have the',\n",
       " 'if he was not have the',\n",
       " 'when asked him',\n",
       " 'when have the',\n",
       " 'i have the',\n",
       " 'i have the',\n",
       " 'have the',\n",
       " 'he was his',\n",
       " 'he asked',\n",
       " 'don t have to have to have to have to have to don t have to have to have to',\n",
       " 'as if he asked as if he had made',\n",
       " 'he asked',\n",
       " 'he asked as if he asked as if he asked as if he asked as if he asked as if',\n",
       " 'i have the',\n",
       " 'he',\n",
       " 'i have the',\n",
       " 'when he asked to have the',\n",
       " 'no one can t have been to have been to have been to have been to have been to have',\n",
       " 'he asked',\n",
       " 'if he d have the',\n",
       " 'then asked as if they have the',\n",
       " 'he',\n",
       " 'he',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'have been to have been to have been to have been to have the',\n",
       " 'i have the',\n",
       " 'i have the',\n",
       " 'he',\n",
       " 'i have the',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'he have he had made it have he had made',\n",
       " 'he asked',\n",
       " 'he was a',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'when i have the',\n",
       " 'as if the',\n",
       " 'he said',\n",
       " 'when i have the',\n",
       " 'she have the',\n",
       " 'he asked',\n",
       " 'i have not have not have not have not have not have not have not have not have not have',\n",
       " 'have the',\n",
       " 'you don t have the',\n",
       " 'he asked',\n",
       " 'not really in not really so not really in not really not really not really not really so not really',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'i have the',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'he',\n",
       " 'i have the',\n",
       " 'he asked',\n",
       " 'he',\n",
       " 'he asked as if he asked as if he asked as if he asked as if he asked as if',\n",
       " 'he asked',\n",
       " 'i have it',\n",
       " 'have the',\n",
       " 'they have the',\n",
       " 'when i have the',\n",
       " 'had to have been to have been to have had to have been to have been to have been to',\n",
       " 'harry could not have the',\n",
       " 'his',\n",
       " 'but it have not have not have the',\n",
       " 'he s not have the',\n",
       " 'i have the',\n",
       " 'he',\n",
       " 'one another',\n",
       " 'i have the',\n",
       " 'he had to have the',\n",
       " 'i have the',\n",
       " 'he asked',\n",
       " 'he asked',\n",
       " 'he asked']"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_train = train.sample(1000)\n",
    "result = []\n",
    "i=0\n",
    "for i, j , k in tqdm(sample_train.values):\n",
    "    # print(i , j , k)\n",
    "    pred = predict(i)\n",
    "    result.append(pred)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 941
    },
    "id": "vpMHogB_XyUc",
    "outputId": "1f9c104c-e00e-4790-b450-eea13359c4f4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ERRONEOUS_SENTENCE</th>\n",
       "      <th>english_inp</th>\n",
       "      <th>english_out</th>\n",
       "      <th>PREDICTED_SENTENCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>197645</th>\n",
       "      <td>That makes no no difference to me said Jehan tis</td>\n",
       "      <td>&lt;start&gt; That makes no difference to me said Je...</td>\n",
       "      <td>That makes no difference to me said Jehan tis ...</td>\n",
       "      <td>i have been to have been to have been to have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99162</th>\n",
       "      <td>Couldn t get past the gargoyle</td>\n",
       "      <td>&lt;start&gt; Couldn t get past the gargoyle</td>\n",
       "      <td>Couldn t get past the gargoyle &lt;end&gt;</td>\n",
       "      <td>asked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107285</th>\n",
       "      <td>That s me outsmarted nnnnt All rngh come nn then</td>\n",
       "      <td>&lt;start&gt; That s me outsmarted innit All righ co...</td>\n",
       "      <td>That s me outsmarted innit All righ come in th...</td>\n",
       "      <td>i have the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28930</th>\n",
       "      <td>LADY ANNE Would they were basilisks to strike ...</td>\n",
       "      <td>&lt;start&gt; LADY ANNE Would they were basilisks to...</td>\n",
       "      <td>LADY ANNE Would they were basilisks to strike ...</td>\n",
       "      <td>they have the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67247</th>\n",
       "      <td>The most they could do however was to lock away</td>\n",
       "      <td>&lt;start&gt; The most they could do however was to ...</td>\n",
       "      <td>The most they could do however was to lock awa...</td>\n",
       "      <td>you don t have the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134317</th>\n",
       "      <td>The curse of God on thee for a blockhead sakd</td>\n",
       "      <td>&lt;start&gt; The curse of God on thee for a blockhe...</td>\n",
       "      <td>The curse of God on thee for a blockhead said ...</td>\n",
       "      <td>and have the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87438</th>\n",
       "      <td>Then he was gone</td>\n",
       "      <td>&lt;start&gt; Then he was gone</td>\n",
       "      <td>Then he was gone &lt;end&gt;</td>\n",
       "      <td>i have the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26381</th>\n",
       "      <td>Resofrnng ohe essenonal cfndnonfn ff relaonfn ...</td>\n",
       "      <td>&lt;start&gt; Restoring the essential condition of r...</td>\n",
       "      <td>Restoring the essential condition of relation ...</td>\n",
       "      <td>he asked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202068</th>\n",
       "      <td>He clung to it with desjerate hands and at the</td>\n",
       "      <td>&lt;start&gt; He clung to it with desperate hands an...</td>\n",
       "      <td>He clung to it with desperate hands and at the...</td>\n",
       "      <td>i have to have to have to have to have to have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157665</th>\n",
       "      <td>But But this period was important for another ...</td>\n",
       "      <td>&lt;start&gt; But this period was important for anot...</td>\n",
       "      <td>But this period was important for another reas...</td>\n",
       "      <td>harry could have the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136199</th>\n",
       "      <td>Be off and God speed you and bad luck to</td>\n",
       "      <td>&lt;start&gt; Be off and God speed you and bad luck to</td>\n",
       "      <td>Be off and God speed you and bad luck to &lt;end&gt;</td>\n",
       "      <td>he asked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102699</th>\n",
       "      <td>And this ability of yours to detect Voldemort ...</td>\n",
       "      <td>&lt;start&gt; And this ability of yours to detect Vo...</td>\n",
       "      <td>And this ability of yours to detect Voldemort ...</td>\n",
       "      <td>he asked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61704</th>\n",
       "      <td>and if you tzy and magic youzself out they ll</td>\n",
       "      <td>&lt;start&gt; and if you try and magic yourself out ...</td>\n",
       "      <td>and if you try and magic yourself out they ll ...</td>\n",
       "      <td>he asked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147979</th>\n",
       "      <td>Mr</td>\n",
       "      <td>&lt;start&gt; Mr</td>\n",
       "      <td>Mr &lt;end&gt;</td>\n",
       "      <td>he asked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72657</th>\n",
       "      <td>Nn cans There s nnnhing here nn hprn ynp Hagrid</td>\n",
       "      <td>&lt;start&gt; No cats There s nothing here to hurt y...</td>\n",
       "      <td>No cats There s nothing here to hurt you Hagri...</td>\n",
       "      <td>i have the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53138</th>\n",
       "      <td>Are there piles ff jewels bnd gfld lying bbfut...</td>\n",
       "      <td>&lt;start&gt; Are there piles of jewels and gold lyi...</td>\n",
       "      <td>Are there piles of jewels and gold lying about...</td>\n",
       "      <td>i have the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42719</th>\n",
       "      <td>There are turns and casts of fancy there are s...</td>\n",
       "      <td>&lt;start&gt; There are turns and casts of fancy the...</td>\n",
       "      <td>There are turns and casts of fancy there are s...</td>\n",
       "      <td>i have the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54874</th>\n",
       "      <td>Who now shall refill the cup for me For now</td>\n",
       "      <td>&lt;start&gt; Who now shall refill the cup for me Fo...</td>\n",
       "      <td>Who now shall refill the cup for me For now &lt;end&gt;</td>\n",
       "      <td>when they have been to be asked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27315</th>\n",
       "      <td>Come on Everybody says come on here thought Al...</td>\n",
       "      <td>&lt;start&gt; Come on Everybody says come on here th...</td>\n",
       "      <td>Come on Everybody says come on here thought Al...</td>\n",
       "      <td>he s going to an</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151496</th>\n",
       "      <td>Charley Charley said I come away And I I hurried</td>\n",
       "      <td>&lt;start&gt; Charley Charley said I come away And I...</td>\n",
       "      <td>Charley Charley said I come away And I hurried...</td>\n",
       "      <td>he was a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       ERRONEOUS_SENTENCE  ...                                 PREDICTED_SENTENCE\n",
       "197645   That makes no no difference to me said Jehan tis  ...  i have been to have been to have been to have ...\n",
       "99162                      Couldn t get past the gargoyle  ...                                              asked\n",
       "107285   That s me outsmarted nnnnt All rngh come nn then  ...                                         i have the\n",
       "28930   LADY ANNE Would they were basilisks to strike ...  ...                                      they have the\n",
       "67247     The most they could do however was to lock away  ...                                 you don t have the\n",
       "134317      The curse of God on thee for a blockhead sakd  ...                                       and have the\n",
       "87438                                    Then he was gone  ...                                         i have the\n",
       "26381   Resofrnng ohe essenonal cfndnonfn ff relaonfn ...  ...                                           he asked\n",
       "202068     He clung to it with desjerate hands and at the  ...  i have to have to have to have to have to have...\n",
       "157665  But But this period was important for another ...  ...                               harry could have the\n",
       "136199           Be off and God speed you and bad luck to  ...                                           he asked\n",
       "102699  And this ability of yours to detect Voldemort ...  ...                                           he asked\n",
       "61704       and if you tzy and magic youzself out they ll  ...                                           he asked\n",
       "147979                                                 Mr  ...                                           he asked\n",
       "72657     Nn cans There s nnnhing here nn hprn ynp Hagrid  ...                                         i have the\n",
       "53138   Are there piles ff jewels bnd gfld lying bbfut...  ...                                         i have the\n",
       "42719   There are turns and casts of fancy there are s...  ...                                         i have the\n",
       "54874         Who now shall refill the cup for me For now  ...                    when they have been to be asked\n",
       "27315   Come on Everybody says come on here thought Al...  ...                                   he s going to an\n",
       "151496   Charley Charley said I come away And I I hurried  ...                                           he was a\n",
       "\n",
       "[20 rows x 4 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_train['PREDICTED_SENTENCE']= result\n",
    "sample_train.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmxIVOOQPWMu"
   },
   "source": [
    "<font color='blue'>**Calculate BLEU score**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0iHiLdROM23l",
    "outputId": "eca5a2c2-4a93-4477-c1ea-a61fdeb2ff3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0\n"
     ]
    }
   ],
   "source": [
    "#Create an object of your custom model.\n",
    "#Compile and train your model on dot scoring function.\n",
    "# Visualize few sentences randomly in Test data\n",
    "# Predict on 1000 random sentences on test data and calculate the average BLEU score of these sentences.\n",
    "# https://www.nltk.org/_modules/nltk/translate/bleu_score.html\n",
    "\n",
    "#Sample example\n",
    "import nltk.translate.bleu_score as bleu\n",
    "reference = ['i am groot'.split(),] # the original\n",
    "translation = 'it is ship'.split() # trasilated using model\n",
    "print('BLEU score: {}'.format(bleu.sentence_bleu(reference, translation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "872NLcHMJjIx",
    "outputId": "6a6effa2-2185-4c57-8cb3-bbae09aa818f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "100%|██████████| 1000/1000 [00:01<00:00, 826.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average BLEU score of these sentences. are : 0.822307632753043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk.translate.bleu_score as bleu\n",
    "\n",
    "score = 0\n",
    "for i in tqdm(range(1000)):\n",
    "    inp = sample_train.ERRONEOUS_SENTENCE.values[i]\n",
    "\n",
    "    # print(inp)\n",
    "    translate = sample_train.PREDICTED_SENTENCE.values[i]\n",
    "    # print(translate)\n",
    "    bleu = nltk.translate.bleu_score.sentence_bleu(inp, translate)\n",
    "    score = score + bleu\n",
    "average_bleu = score / 1000\n",
    "print('the average BLEU score of these sentences. are :' , average_bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "E5Y6JrIxIao5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "GRAMMAR_ERROR_HANDLING_updated",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
