# GRAMMAR_ERROR_HANDLING
INTRODUCTION

In simple words , Grammatical error checking is a process of detecting and sometimes correcting erroneous words in a text.There have been various approaches for detecting and correcting texts in various languages. NLP enables computers to perform a wide range of natural language related tasks at all levels, ranging from parsing and partof-speech (POS) tagging, to machine translation and dialogue systems. 
In the last few years, neural networks based on dense vector representations have been producing superior results on various NLP tasks. This trend became popular with the rise of word embeddings and deep learning methods.Deep Learning models and its method applied to Natural Language tasks such as convolutional neural networks (CNNs) , recurrent neural networks (RNNs) , and recursive neural networks . Grammar Error Handling (GEH) is a general term that covers both Grammar Error Detection (GED) and Grammar Error Correction (GEC). The parts of the sentences with errors are identified in GED, while GEC deals with applying specific edits to remedy errors and generate the corrected sentences. It has been used in word processors as old as Microsoft Word 96 .
As a daily life example , we can take email which uses Grammar error Handling to detect incorrect words following with the corrections. In recent times, Grammatical Error Correction (GEC) has gained popularity in the field of NLP and has its uses in order to build a GEC system. This system can be useful in various ways , say in word processor to check sentences for grammatical errors. Also utilizing it as a post editor for various applications such as machine translation .Most of the work in GEC is concentrated on the English language, particularly on errors made by learners
Performance metric
Various metrics for GEC is used , such as the F-score, precision, recall, MaxMatch, and GLUE. In NLP,well-known metrics for performance evaluation are the F-score, precision, and recall. The F-score was the most widely used metric in the initial GEC research, and it uses the harmonic mean of precision and recall as the final performance value.The F-score, also commonly known as F1, is the mean of precision (how many predictions were correct) and recall (how many of the possible correct predictions were made). However, these metrics exhibit the weakness whereby they cannot evaluate sentences that exceed the phrase level. Moreover, the F-score cannot capture the difference between"no change" and "wrong edits" of the GEC model. To alleviate the limitations of traditional methods, Dahlmeier andNg [9] suggested a metric known as the MaxMatch scorer,that could consider edits up to the phrase level. However, MaxMatch requires annotations for individual errors.
 MaxMatch (M2), is an algorithm for efficiently computing the sequence of phrase-level edits between a source sentence and a system hypothesis that achieves the highest overlap with the gold-standard annotation. This optimal edit sequence is subsequently scored using F1 measure. We test our M2 scorer on the Helping Our Own (HOO) shared task data and show that our method results in more accurate evaluation for grammatical error correction. 
Bleu score is one of the metrics which can be used for the Grammar Error correction. However , Bleu score results aren't meaningful as it's results are based on the overlapping of the input and the output sentences . A new evaluation metric is suggested that can comprehensively include the correction performance and over correction model performance. The specific steps for the performance measurement are as follows: Firstly, the existing performance evaluation metric GLUE is used, by means of which the
correction performance can be identified. Secondly, we measure the ratio of how perfectly the correction result matches the reference when the input is a correct sentence.

Business constraints

As the grammar error Handling is an Important task either in sending messages , searching something or even sending the mail.So the user expects the results to be presented within a few seconds and not more . If the predictions takes a lot of time it would be frustrating for the user and would make a bad impression . Also accuracy is a concern , as the users did not want his correct sentences to be predicted incorrectly .

Training the Deep Learning models , Encoder - Decoder Model and Attention Model .
